The Rules of Probability
sum rule
                                  p(X) = \sum_{Y}p(X,Y)
product rule
                                  p(X,Y) = p(Y|X)p(X)
Bayes' theorem
                                  p(Y|X) = \frac{p(X|Y)p(Y)}{p(X)}
Denominator in Bayes' theorem
                                  p(X) = \sum_Y{p(X|Y)p(Y)}
probability densities
                                  p(x\in (a,b)) = \int_{a}^{b}p(x)dx
The probability density function p(x) must satisfy the two conditions
                                  p(x) \geq 0
                                  \int_{-\infty}^{\infty}p(x)dx = 1
Combinations of discrete and continuous varables.
                                  p(x) = \int_p(x,y)dy
                                  p(x,y) = p(y|x)p(x)
                                  
Expectations and covariances
The average value of some function f(x) under a probability distribution p(x) is called the expectation of f(x) and
will be denoted by 
                                  \mathbb{E}[f] = \sum_{x}p(x)f(x)
                                  \mathbb{E}[f] = \int p(x)f(x)dx
                                  
approximation
                                  \mathbb{E}[f] \simeq \frac{1}{N}\sum_{n=1}^{N}{f(x_n)}
conditional expectation with respect to a conditional distribution                                
                                  \mathbb{E}_x[f|y] = \sum_{x}p(x|y)f(x)

variance of f(x) is defined by
                                  var[f] = \mathbb{E}[(f(x) - \mathbb{E}[f(x)])^2
                                  var[f] = \mathbb{E}[f(x)^2]-\mathbb{E}[f(x)]^2
convariance 
                                  cov[x,y] = \mathbb{E}_{x,y}[\{x- \mathbb{E}[x]\}\{y-\mathbb{E}[y]\}]
In the case of two vectors of random variables \textbf{x} and \textbf{y}
cov[\textbf{x},\textbf{y}] = \mathbb{E}_{x,y}[\{\textbf{x}-\mathbb{E}[\textbf{x}]\} \{ \textbf{y}^T - \mathbb{E}[\textbf{y}^T ]\}]
cov[\textbf{x},\textbf{y}] = \mathbb{E}_{x,y}[\textbf{x}\textbf{y}^T] - \mathbb{E}[\textbf{x}] \mathbb{E}[\textbf{y}^T]

Bayesian probabilities
So far, we have viewed probabilities in terms of the frequencies of random,repeatable events,which we shall refer to
as the classical or frequentist interpretation of probability.Now we turn to the more general Bayesian view,in which 
probabilities provide a quantification of uncertainty.
We can adopt a similar approach when making inferences about quantities such as the paramaters \textbf{w} in the 
polynomial curve fitting.We capture our assumptions about \textbf{w},before ovserving the data,in the form of a 
prior probability distribution p(\textbf{w}).The effect of the observed data D = {t_1,...,t_N} is expressed through
the conditional probability p(D|w)
                    p(\textbf{w} |D) = \frac{p(D|\textbf{w})p(\textbf{w})}{p(D)}
The quantity p(D|w) on the right-hand side of Bayes' theorem is evaluated for the observed data set D and 
can be viewed as a function of the parameter vector \textbf{w},in which case it is called the likelihood function.
                    posteroir \propto likelihood \times prior
 Integrating both side with respect to \textbf{w}
                    p(D) = \int p(D|\textbf{w}) p(\textbf{w})d\textbf{w}
A widely used frequentise estimator is maximum likelihood,in which \textbf{w} is set to the value that 
maximizes the likehood function p(D|w).







Linear Regression:
Data representation:
N observations of x,wiritten 
                          "X \equiv (x_1,x_2,...,x_n)^T"
together with corresponding observations of the values of t,denoted 
                          "t \equiv (t_1,t_2,...t_N)^T".

Hypothesis function(model representation):
                  "y(x,\bold w)= w_0+w_1x+w_2x^2+...+w_Mx^M = \sum_{j=0}^{M}w_jx^j"
M is the order of the polynomial,and x^j denotes x raised to the power of j.The polynomial coefficients w_0,...w_M are
collectively denoted by the vector \bold w.

Error Function(Sum of squares of errors between predictions y(x_n,w) for each data point x_n and the corresponding
target values t_n,so that we minimize:
                          E(\bold w)=\frac{1}{2}\sum_{n=1}^{N}\{y(x_n,\bold w)-t_n \}^2
root-mean-squre(RMS) error defined by 
                          E_{RMS} = \sqrt[2]{2E(\bold w^*)/N}
Penalized(regularized) error function
\widetilde{E}(\textbf{w}) = \frac{1}{2}\sum_{n=1}^{N}\{y(x_n,\textbf{w}-t_n\}^2 + \frac{1}{2} \parallel \textbf{w} \parallel^2
where \parallel \textbf{w} \parallel^2 \equiv \textbf{w}^T\textbf{w}=w_0^2+w_1^2+...+w_M^2

Loss functions for regression
\mathbb{E}[\mathit{L}] = \int\int\mathit{L}(t,y(\textbf{x}))p(\textbf{x},t)d\textbf{x}dt
A common choice of loss function in squared loss given by 
                                  \mathit(L)(t,y(\textbf{x})) = \{y(\textbf{x}) - t\}^2
                        \mathbb{E}[\mathit{L}] = \int\int\{y(\textbf{x}) - t\}^2 p(\textbf{x},t)d\textbf{x}dt.
Minimize \mathbb{E}[\mathit{L}] by using the calculus of variations to give 
\frac{\delta\mathbb{E}[\mathit{L}]}{\delta y\(\textbf{x}))} = 2 \int\{ y(\textbf{x} -t) \}p(\textbf{x},t)dt = 0
Solving for y(\textbf{x}) and using the sum and product rules of probability,we obtain
y(\textbf{x}) = \frac{\int tp(\textbf{x},t)dt}{p(\textbf{x})} = \int tp(t|\textbf{x})dt = \mathbb{E}[t|\textbf{x}]
Let's derive this result in a slightly different way.Armed with knowledge that the optimal solution is the 
conditional expectation,we can expand the square term as follows
  \{y(\textbf{x} -t)\}^2
= \{y(\textbf{x} - \mathbb{E}[t|\textbf{x}] + \mathbb{E}[t|\textbf{x}] - t )\}^2 
= \{ y(\textbf{x}) - \mathbb{E}[t|\textbf{x}] \}^2 + 2\{ y(\textbf{x}) - \mathbb{E}[t|\textbf{x}] \}\{ \mathbb{E}[t|\textbf{x}]-t \} + \{ \mathbb{E}[t|\textbf{x}] -t \}^2
where,\mathbb{E}[t|\textbf{x}] denote \mathbb{E}_{t}[t|\textbf{x}].Substitute into the loss function and perform the
integral over t,we see the cross-term vanishes
\mathbb{E}[\mathit{L}]                                                            \\
= \int\int\{y(\textbf{x}) - t\}^2 p(\textbf{x},t)d\textbf{x}dt                    \\
= \int \{ y(\textbf{x}) -\mathbb{E}[t|\textbf{x}] \}^2 p(\textbf{x})d\textbf{x} + 
  \int\{ \mathbb{E}[t|\textbf{x}] - t \}^2 p(\textbf{x})d\textbf{x}               \\
= \int \{ y(\textbf{x}) -h(\textbf{x}) \}^2 p(\textbf{x})d\textbf{x} +            
  \int\{ h(\textbf{x}) - t \}^2 p(\textbf{x})d\textbf{x}                          \\
= \int \{ y(\textbf{x}) -h(\textbf{x}) \}^2 p(\textbf{x})d\textbf{x} +            
  \int\{ h(\textbf{x}) - t \}^2 p(\textbf{x},t)d\textbf{x}dt                      \\
  
For a popular choice,the squared loss function,for which the optimal prediction is given by the conditional
expectation,which we denote by h(\textbf{x}) and which is given by 
   h(\textbf{x}) = \mathbb{E}[t|\textbf{x}] = \int tp(t|\textbf{x})dt
  
Consider the integrand of the first term,which for particular data set D takes the form
  \{ y(\textbf{x};D) - h(\textbf{x}) \} ^2
This quantity will be dependent on the particular data set D,so we take its average over the ensemble of data sets.
If we add and substract the quantity \mathbb{E_D}[y(\textbf{x};D)] inside the braces,and then expand,we obtain
\{y(\textbf{x};D) - \mathbb{E_D}[y(\textbf{x};D)] + \mathbb{E_D}[y(\textbf{x};D)] -h(\textbf{x})  \}^2         \\
= \{ y(\textbf{x};D) -\mathbb{E}[y(\textbf{x};D)]    \}^2 + \{ \mathbb{E_D}[y(\textbf{x};D)] - h(\textbf{x})\}^2 +
  2\{ y(\textbf{x};D) - \mathbb{E_D}[y(\textbf{x};D)]\}\{ \mathbb{E_D}[y(\textbf{x};D)] -h(\textbf{x})\} \\
= \underbrace{\{ y(\textbf{x};D) -\mathbb{E}[y(\textbf{x};D)]    \}^2} + \underbrace{\{ \mathbb{E_D}[y(\textbf{x};D)] - h(\textbf{x})\}^2}   \\
=                    \color{red}{variance}                 +   \color{blue}{(bias)^2}  + (irreducible error)
The decomposition of the expected squared loss
                                  expected loss = (bias)^2 + variance + noise
where
                                  (bias)^2 = ...
                                  variance = ...
                                  noise = ...
  
The function y(\textbf{x}) we seek to determine enters only the first term,which will be minized when y(\textbf{x}) 
is equal to \mathbb{E}[t|\textbf{x}],in which case this term will vanish.The second term is the variance of 
distribution of t,averaged over \textbf{x},representing the intrinsic variablility of the target data and can be 
regarded as noise.It's the irreducible minimum value of the loss function.
More sophisticated loss function,Minkowski loss
\mathbb{E}[\mathit{L_q}] = \int\int| y(\textbf{x}) - t |^q p(\textbf{x},t)d\textbf{x}dt




Bias-variance decomposition of squared error
                                  y_i = f(x_i) + \epsilon 
where the noise \epsilon has zero mean and variance \sigma^2.
Find a function \hat{f}(x) that approximates the true function y = f(x) as well as possible.Make "as well as possible"

precise  by measuring the mean squared error between y and \hat{f}(x),we want (y - \hat{f}(x))^2 to be minimal.










Support Vector Machine:
Lagrange Multiplier of SVM loss function:
"\L(w,b,\alpha) = \frac{1}{2}\parallel \mathbf w \parallel + \sum_{i=1}^{m}[\alpha_iy_i(\mathbf w^Tx+b) -1]"
