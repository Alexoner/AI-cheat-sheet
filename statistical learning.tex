The Rules of Probability
sum rule
                                  p(X) = \sum_{Y}p(X,Y)
product rule
                                  p(X,Y) = p(Y|X)p(X)
Bayes' theorem
                                  p(Y|X) = \frac{p(X|Y)p(Y)}{p(X)}
Denominator in Bayes' theorem
                                  p(X) = \sum_Y{p(X|Y)p(Y)}
probability densities
                                  p(x\in (a,b)) = \int_{a}^{b}p(x)dx
The probability density function p(x) must satisfy the two conditions
                                  p(x) \geq 0
                                  \int_{-\infty}^{\infty}p(x)dx = 1
Combinations of discrete and continuous varables.
                                  p(x) = \int_p(x,y)dy
                                  p(x,y) = p(y|x)p(x)
                                  
Expectations and covariances
The average value of some function f(x) under a probability distribution p(x) is called the expectation of f(x) and
will be denoted by 
                                  \mathbb{E}[f] = \sum_{x}p(x)f(x)
                                  \mathbb{E}[f] = \int p(x)f(x)dx
                                  
approximation
                                  \mathbb{E}[f] \simeq \frac{1}{N}\sum_{n=1}^{N}{f(x_n)}
conditional expectation with respect to a conditional distribution                                
                                  \mathbb{E}_x[f|y] = \sum_{x}p(x|y)f(x)

variance of f(x) is defined by
                                  var[f] = \mathbb{E}[(f(x) - \mathbb{E}[f(x)])^2
                                  var[f] = \mathbb{E}[f(x)^2]-\mathbb{E}[f(x)]^2
convariance 
                                  cov[x,y] = \mathbb{E}_{x,y}[\{x- \mathbb{E}[x]\}\{y-\mathbb{E}[y]\}]
In the case of two vectors of random variables \textbf{x} and \textbf{y}
cov[\textbf{x},\textbf{y}] = \mathbb{E}_{x,y}[\{\textbf{x}-\mathbb{E}[\textbf{x}]\} \{ \textbf{y}^T - \mathbb{E}[\textbf{y}^T ]\}]
cov[\textbf{x},\textbf{y}] = \mathbb{E}_{x,y}[\textbf{x}\textbf{y}^T] - \mathbb{E}[\textbf{x}] \mathbb{E}[\textbf{y}^T]

Bayesian probabilities
So far, we have viewed probabilities in terms of the frequencies of random,repeatable events,which we shall refer to
as the classical or frequentist interpretation of probability.Now we turn to the more general Bayesian view,in which 
probabilities provide a quantification of uncertainty.
We can adopt a similar approach when making inferences about quantities such as the paramaters \textbf{w} in the 
polynomial curve fitting.We capture our assumptions about \textbf{w},before ovserving the data,in the form of a 
prior probability distribution p(\textbf{w}).The effect of the observed data D = {t_1,...,t_N} is expressed through
the conditional probability p(D|w)
                    p(\textbf{w} |D) = \frac{p(D|\textbf{w})p(\textbf{w})}{p(D)}
The quantity p(D|w) on the right-hand side of Bayes' theorem is evaluated for the observed data set D and 
can be viewed as a function of the parameter vector \textbf{w},in which case it is called the likelihood function.
                    posteroir \propto likelihood \times prior
 Integrating both side with respect to \textbf{w}
                    p(D) = \int p(D|\textbf{w}) p(\textbf{w})d\textbf{w}
A widely used frequentise estimator is maximum likelihood,in which \textbf{w} is set to the value that 
maximizes the likehood function p(D|w).







Linear Regression:
Data representation:
N observations of x,wiritten 
                          "X \equiv (x_1,x_2,...,x_n)^T"
together with corresponding observations of the values of t,denoted 
                          "t \equiv (t_1,t_2,...t_N)^T".

Hypothesis function(model representation):
                  "y(x,\bold w)= w_0+w_1x+w_2x^2+...+w_Mx^M = \sum_{j=0}^{M}w_jx^j"
M is the order of the polynomial,and x^j denotes x raised to the power of j.The polynomial coefficients w_0,...w_M are
collectively denoted by the vector \bold w.

Error Function(Sum of squares of errors between predictions y(x_n,w) for each data point x_n and the corresponding
target values t_n,so that we minimize:
                          E(\bold w)=\frac{1}{2}\sum_{n=1}^{N}\{y(x_n,\bold w)-t_n \}^2
root-mean-squre(RMS) error defined by 
                          E_{RMS} = \sqrt[2]{2E(\bold w^*)/N}
Penalized(regularized) error function
\widetilde{E}(\textbf{w}) = \frac{1}{2}\sum_{n=1}^{N}\{y(x_n,\textbf{w}-t_n\}^2 + \frac{1}{2} \parallel \textbf{w} \parallel^2
where \parallel \textbf{w} \parallel^2 \equiv \textbf{w}^T\textbf{w}=w_0^2+w_1^2+...+w_M^2


Support Vector Machine:
Lagrange Multiplier of SVM loss function:
"\L(w,b,\alpha) = \frac{1}{2}\parallel \mathbf w \parallel + \sum_{i=1}^{m}[\alpha_iy_i(\mathbf w^Tx+b) -1]"
