\chapter{Linear Models for Classification}
\label{Linear Models for Classification}
The goal in classification is to take an input vector $\vec{x}$ and to assign it to one of $K$ discrete classes $\mathcal{C}_k$ where $K = 1,...,K$.The input space is thereby divided into $decision regions$ whose boundaries are called $decision boundaries$ or $decision surfaces$.Data sets whose classes can be separated exactly by linear decision surfaces are said to be $linearly separable$.

In Chapter 1,we identified three distinct approaches to the classification problem.The simplest involves constructing a discriminant function that directly assigns
each vector x to a specific class. A more powerful approach, however, models the conditional probability distribution p(Ck|x) in an inference stage, and then subsequently uses this distribution to make optimal decisions. By separating inference and decision, we gain numerous benefits, as discussed in Section 1.5.4. There are two different approaches to determining the conditional probabilities $p(C_k|x)$. One technique is to model them directly, for example by representing them as parametric models and then optimizing the parameters using a training set. Alternatively, we can adopt a generative approach in which we model the class-conditional densities
given by p(x|Ck), together with the prior probabilities p(Ck) for the classes, and then we compute the required posterior probabilities using Bayes’ theorem
\begin{equation}
p(\mathcal{C}_k|\vec{x}) = \dfrac{p(\vec{x}\mathcal{C}_k)p(\mathcal{C}_k)}{p(\vec{x})}
\end{equation}

In the linear regression models considered in Chapter 3, the model prediction
y(x,w) was given by a linear function of the parameters w. In the simplest case,
the model is also linear in the input variables and therefore takes the form y(x) =
wTx+w0, so that y is a real number. For classification problems, however, we wish
to predict discrete class labels, or more generally posterior probabilities that lie in
the range (0, 1). To achieve this, we consider a generalization of this model in which
we transform the linear function of w using a nonlinear function f( · ) so that
\begin{equation}
y(\vec{x}) = f(\vec{w}^T\vec{x} + w_0)
\end{equation}
In the machine learning literature $f(\cdot)$ is known as an \textbf{activation function},whereas its inverse is called a \textbf{link function} in the statistics literature.The decision surfaces correspond to y(x) = constant, so that $\vec{w}^T\vec{x} + w_0 = constant$ and hence the decision surfaces are linear functions of $\vec{x}$, even if the function $f(·)$ is nonlinear. For this reason, the class of models described by (4.3) are called \textbf{generalized linear models} (McCullagh and Nelder,1996).However,in contrast to the models used for regression,they are no longer linear in the parameters due to the presence of the nonlinear  function $f(\cdot)$.
\section{Discriminant Functions}
A discriminant is a function that takes an input vector $\vec{x}$ and assigns it to be one of $K$ classes,denoted  $\mathcal{C}_k$.
\subsection{Two classes}
\subsection{Multiple classes}
Consider the extension of linear discriminants to $ K >2$ classes,building a $K$-class discriminants by combining a number of two-class discriminant functions on which leads to some serious difficulties(Duda and Hart,1973).

Consider the use of $K-1$ classifier each of which solves a two-class problem of separating points int particular class $\mathcal{C}_k$ from points not in that class.This is known as a \textbf{one-versus-the-rest} classifier.An alternative is to introduce $K(K-1)/2$ binary discriminant functions,one for every possible pair classes.This is known as a \textbf{one-versus-one} classifier.Both of these two approaches run into the problem of ambiguous regions.We can avoid these difficulties by considering a single $K$-class discriminant comprising $k$ linear functions of the form
\begin{equation}
y_k(\vec{x}) = \vec{w_k}^T\vec{x}+w_{k0}
\end{equation}
and then assigning a point $\vec{x}$ to class $\mathcal{C}_k$ if $y_k(\vec{x}) > y_j(\vec{x})$ for all $j \neq k$.The decision boundary between class $\mathcal{C}_k$ and $\mathcal{C}_j$ is therefore given by $y_k(\vec{x}) = y_j(\vec{x})$ and hence corresponds to a $(D-1)$-dimensional hyperplane defined by
\begin{equation}
(\vec{w_k}-\vec{w_j})^T + (w_{k0}-w_{j0}) = 0
\end{equation}
The decision regions of such a discriminant are always singly connected and convex.

\subsection{Least squares for classification}
Even as a discriminant function(where we use it to make decisions directly and dispense with any probabilistic interpretation)it suffers from some severe problems.Least-squares solutions lack robustness to outliers,and this applies equally to the classification application.Additional points(outliers) produce a significant change in the location of the decision boundary.The sum-of-squares error function penalizes predictions that are 'too correct' in that they lie a long way on the correct side of the decision boundary.More than lack of robustness,least-squares solutions may gives poor results on classification problems.

The failure of least squares lies in the fact that it corresponds to maximum likelihood under the assumption of a Gaussian conditional distribution,whereas binary target vectors clearly have a distribution that is far from Gaussian(a Bernoulli).



\subsection{Fisher's linear discriminant}
\subsubsection{representation}
One way to view a linear classification model is in terms of dimensionality reduction.Consider first the case of two classes,and suppose we take the $D-dimensional$ input vector $\vec{x}$ and project it down to one dimension using
\begin{equation}\label{eqn:Fisher LDA projection}
y = \vec{w}^T\vec{x}.
\end{equation}
If we place a threshold on $y$ and classify $y \geq -w_0$ as class $\mathcal{C_1}$,and otherwise class $\mathcal{C_2}$,then we obtain our standard linear classifier discussed in the previous section.
\subsubsection{evaluation}
By adjusting the components of the weight vector $\vec{w}$,we can select a projection that maximizes the class separation.To begin with,consider a two-class problem in which there are $N_1$ points of class $\mathcal{C_1}$ and $N_2$ points of class $\mathcal{C_2}$,so that the mean vectors of the two classes are given by
\begin{equation}
\vec{m_1} = \dfrac{1}{N_1} \sum_{n\in \mathcal{C_1}}{\vec{x_n}},\vec{m_1} = \dfrac{1}{N_2}\sum_{n\in \mathcal{C_2}}\vec{x_n}
\end{equation}
The simplest measure of the separation of the classes,when projected onto $\vec{w}$,is the separation of the projected  class means.This suggests that we might choose $\vec{w}$ so as to maximize
\begin{equation}
m_2 - m_1 = \vec{x}^T(\vec{m_2}-\vec{m_1})
\end{equation}
where\begin{equation}
m_k = \vec{w}^T\vec{m_k}
\end{equation}
is the mean of the projected data from class $\mathcal{C_k}$.However this expression can be makde arbitrarily large simply by increasing the magnitude of $\vec{w}$.To solve this,we could constrain $\vec{2}$ to have unit length,so that $\sum_{i}{w_i^2}=1$.Using a Lagrange multiplier to perform the constrained maximization,we then find that $\vec{w} \propto (\vec{m_2} - \vec{m_1})$.There is still a problem with this approach that the projected data have considerable overlap for strongly nondiagonal convariances of the class distributions.

\subsubsection{optimization}
The idea proposed by Fisher is to \textbf{maximize a function that will give a large separation between the projected class means while also giving a small variance within each class,thereby minimizing the class overlap}.The projection formula \ref{eqn:Fisher LDA projection} transforms the set of labelled data points in $\vec{x}$ into a labelled set in the one-dimensional space $y$.The within-class variance of the transformed data from class $\mathcal{C_k}$ is therefore given by
\begin{equation}
s_k^2 = \sum_{n\in \mathcal{C_k}}(y_n-m_k)^2 
\end{equation}
where\begin{equation}
y_n = \vec{w}^T\vec{x_n}
\end{equation}
We can define the total within-class variance for the whole data set to be simply $s_1^2+s_2^2$.The Fisher criterion is defined to be the ratio of the between-class variance to the within-class variance and is given by
\begin{equation}
J(\vec{w}) = \dfrac{(m_2-m_1)^2}{s_1^2+s_2^2}
\end{equation}

We can make the dependence on \vec{w} explicit by rewrite the Fisher criterion in the form
\begin{equation}\label{eqn:Fisher criterion}
J(\vec{w}) = \dfrac{\vec{w}^T\vec{S_B}\vec{w}}{\vec{w}^T\vec{S_W}\vec{w}}
\end{equation}
where $\vec{S_B}$ is the $between-class$ covariance matrix,given by 
\begin{equation}\label{eqn:Fisher between-class covariance matrix}
\vec{S_B} = (\vec{m_2}-\vec{m_1})(\vec{m_2}-\vec{m_1})^T
\end{equation}
so
\begin{equation}
\vec{w}^T\vec{S_B}\vec{w} = \vec{w} ^T(\vec{m_2} - \vec{m_1})(\vec{m_2}-\vec{m_1})^T\vec{w} = (m_2-m_1)(m_2-m_1)
\end{equation}
and $\vec{S_W}$ is the total $within-class$ covariance matrix,given by
\begin{equation}
\vec{S_W} = \sum_{\mathcal{C}}\sum_{n\in \mathcal{C}_1}(\vec{x_n} - \vec{m_1})(\vec{x_n}-\vec{m_1})^T
\end{equation}
so
\begin{equation}
\vec{w}^T\vec{S_W}\vec{w} = \sum_{\mathcal{C}_k}\sum_{n\in \mathcal{C}_k}{\vec{w}^T(\vec{x_n}-\vec{m_k})(\vec{x_n}-\vec{m_k})^T} = \sum_{\mathcal{C}_i}\sum_{n \in \mathcal{C}_k}(y_n-m_k)^2
\end{equation}
Differentiating \ref{eqn:Fisher criterion} with respect to $\vec{w}$,we find that $J(\vec{w})$ is maximized when
\begin{equation}
{(\vec{w}^T\vec{S_B}\vec{w})\vec{S_W}\vec{w}} = (\vec{w}^T\vec{S_W}\vec{w})\vec{S_B}\vec{w}
\end{equation}
Rewriting this as
\begin{equation}\label{eqn:Fisher generalized eigenvalue}
\vec{S_B}\vec{w} = \lambda\vec{S_W}\vec{w}
\end{equation}
where
\begin{equation}
\lambda = \dfrac{(\vec{w}^T\vec{S_B}\vec{w})}{(\vec{w}^T\vec{S_W}\vec{w})}
\end{equation}
which is called a \textbf{generalized eigenvalue} problem solution.

\begin{proof}
\begin{eqnarray}\because\begin{cases}
	\dfrac{\partial}{\partial x}\dfrac{f(x)}{g(x)} = \dfrac{f'g-fg'}{g^2}, \text{where } f^\prime = \dfrac{\partial f(x)}{\partial x} \text{ and } g' = \dfrac{\partial g(x)}{\partial x}
	\\ \dfrac{\partial}{\partial \vec{x}} \vec{x}^T\vec{A}\vec{x} = (\vec{A}+\vec{A}^T)\vec{x}
\end{cases}	\\
\therefore\end{eqnarray}
\begin{eqnarray}
\dfrac{\partial}{\partial \vec{w}}J(\vec{w}) & =  \dfrac{2\vec{S_B}\vec{w}(\vec{w}^T\vec{S_W}\vec{w})-(\vec{w}^T\vec{S_B}\vec{w})\vec{S_W}\vec{w}}{(\vec{w}^T\vec{S_W}\vec{w})^2}
\end{eqnarray}
Setting the derivative to zero,we can get the result.
\end{proof}
In particular,since 
\begin{equation}
\lambda\vec{w} = \vec{S_W}(\vec{m_2}-\vec{m_1})(m_2-m_1)
\end{equation}
Multiply both sides of \ref{eqn:Fisher generalized eigenvalue} by $\vec{S_W}^{-1}$ we then obtain
\begin{equation}\label{eqn:Fisher's linear discriminant}
\vec{w} \propto \vec{S_W}^{-1}(\vec{m_2}-\vec{m_1})
\end{equation}
Note that if the within-class covariance is isotropic,so that $\vec{S_W}$ is proportional to the unit matrix($\vec{S_W} \propto \vec{I}$),we find that $\vec{w}$ is proportional to the difference vector of the class means.

The result \ref{eqn:Fisher's linear discriminant} is known as $Fisher's linear discriminant$,although strictly it is not a discriminant but rather a specific choice of direction for projection of the data down to one dimension.However,the projected data can subsequently be used to construct a discriminant,by choosing a threshold $y_0$ so that we classify a new point as belonging to $\mathcal{C}_1$ if $y(\vec{x}) \geq y_0$ and can classify it as belonging to $\mathcal{C}_2$ otherwise.

\subsection{Extensions to higher dimensions and multiple classes}
We can extend the above idea to multiple classes,and to higher dimensional subspaces,by finding a projection $matrix \vec{W}$ which maps from $D$ to $L$ so as to maximize
\begin{equation}
J(\vec{W}) = \dfrac{|\vec{W}\vec{\Sigma_B}\vec{W}^T|}{|\vec{W}\Sigma_W\vec{W}^T|}
\end{equation}
where
\begin{eqnarray}
\Sigma_B \triangleq \sum_{c}{\dfrac{N_c}{N}(\vec{m_c}-\vec{m})(\vec{m_c}-\vec{m})^T} \\
\Sigma_W \triangleq \sum_{c}\dfrac{N_c}{N}\Sigma_c \\
\Sigma_c \triangleq \sum_{i:y_i=c}(\vec{x_i}-\vec{m_c})(\vec{x_i}-\vec{m_c})^T
\end{eqnarray}
The solution can be shown to be
\begin{equation}
\vec{W} = \Sigma_W^{-\frac{1}{2}}\vec{U}
\end{equation}
where $\vec{U}$ are the $\vec{L}$ leading eigenvectors of $\Sigma_W^{-\frac{1}{2}}\Sigma_B\Sigma_W^{-1\frac{1}{2}}$,assuming $\Sigma_W$ is non-singular.(If it is singular,we can first perform PCA on all the data).
\subsection{Probabilistic interpretation of FLDA *}
\subsection{Relation to least squares}
\subsection{Fisher's discriminant for multiple classes}
\subsection{The perceptron algorithm}
Another example of a linear discriminant model is the perceptron of Rosenblatt(1962),which corresponds to a two-class transformation to give a feature vector $\phi(\vec{x})$,and this is then used to construct a generalized linear model of the form
\begin{equation}
y(\vec{x}) = f(\vec{w}^T\vec{\phi}(\vec{x}))
\end{equation}
where the nonlinear activation function $f(\cdot)$ is given by a step function of the form
\begin{equation}
f(a) = \begin{cases}
+1,a \geq 0 \\
-1,a \leq 0
\end{cases}
\end{equation}

\subsubsection{error function}
A natural choice of error function would be the total number of misclassified patterns,which does not lead to a simple learning algorithm because the error is a piecewise constant function of $\vec{w}$,with discontinuities wherever a change in $\vec{w}$ causes the decision boundary to move across one of the data points.Methods based on gradient can't be applied.An alternative error function is known as the \textbf{perceptron criterion},given by
\begin{equation}
E_P(\vec{w}) = -\sum_{n\in \mathcal{M}}{\vec{w}^T\vec{\phi_n}(\vec{x})t_n}
\end{equation}
where we use $t\in \{-1,+1\}$ coding scheme, $\mathcal{M}$ denotes the set of all misclassified patterns.The perceptron criterion associates zero error with any pattern that is correctly classified,whereas for a misclassified pattern $\vec{x}$ it tries to minimize the quantity $-\vec{w}^T\vec{\phi_n}t_n$.The perceptron learning rule is not guaranteed to reduce the total error function at each stage.

However,the \textbf{perceptron convergence theorem} states that if there exists an exact solution(linearly separable),then the perceptron learning algorithm is guaranteed to find an exact solution if a finite number of steps.

Aside from difficulties with the learning algorithm,the perceptron does not provide probabilistic outputs,nor does it generalize readily to $K>2$ classes.The most important limitation,however,arises from the fact that it is based on linear combinations of fixed basis functions.

\subsubsection{optimization}
We now apply the stochastic gradient descent algorithm to this error function.The change in the weight vector $\vec{w}$ is then given by
\begin{equation}
\vec{w}^{\tau+1} = \vec{w}^{\tau} - \eta\nabla E_P(\vec{w}) = \vec{w}^{\tau} + \eta\phi_n t_n
\end{equation}
where $\eta$ is the learning rate parameter and $\tau$ is an integer that indexes the steps of the algorithm.Because the perceptron function $y(\vec{x},\vec{w})$ is unchanged if we multiply $\vec{w}$ by a constant,so we can set the learning rate $\eta$ equal to 1 without of generality.

\section{Probabilistic Generative Models}
We turn next to a probabilistic view of classification and show how models with linear decision boundaries arise from simple assumptions about the distribution of the data,adopting a generative approach in which we model the class-conditional densities $p(\vec{x}|\mathcal{C}_k)$,as well as the class priors $p(\mathcal{c}_k)$,and then use these to compute posterior probabilities $p(\mathcal{C}_k|\vec{x})$ through Bayes' theorem.

Consider first of all the case two classes.The posterior probability for class $\mathcal{C}_1$ can be written as 
\begin{align}
p(\mathcal{C}_1|\vec{x}) &= \dfrac{p(\vec{x}|\mathcal{C}_1)p(\mathcal{C}_1)}{p(\vec{x}|\mathcal{C}_1)p(\mathcal{C}_1)+p(\vec{x}|\mathcal{C}_2)p(\mathcal{C}_2)} \\
&= \dfrac{1}{1+exp(-a)} = \sigma(a)
\end{align}
where we have defined
\begin{equation}
a = \ln \dfrac{p(\vec{x}|\mathcal{C}_1)p(\mathcal{C}_1)}{p(\vec{x}|\mathcal{C}_2)p(\mathcal{C}_2)}
\end{equation}
and $\sigma(a)$ is the \textbf{logistic sigmoid} function defined by
\begin{equation}
\sigma(a) = \dfrac{1}{1+exp(-a)}
\end{equation}
The term 'sigmoid' means S-shaped,sometimes called a 'squashing function' because it maps the whole real axis into a finite interval.It satisfies symmetry property
\begin{equation}
\sigma(-a) = 1 - \sigma(a)
\end{equation}
The inverse of the logistic sigmoid is given by 
\begin{equation}
a = \ln(\dfrac{\sigma}{1-\sigma})
\end{equation}
and is known as the \textbf{logit} function.It represents the log of the ratio of probabilities $ln[p(\mathcal{C}_1|vec{x})/p(\mathcal{C}_2|vec{x})]$ for the two classes,also known as the \textbf{log odds}.

\subsection{Continuous inputs}
\subsection{Maximum likelihood solution}
\subsection{Discrete features}
\subsection{Exponential family}


\section{Probabilistic Discriminative Models}
\subsection{Fixed basis functions}
\subsection{Logistic regression}
\subsection{Iterative reweighted least squares}
\subsection{Multiclass logistic regression}
\subsection{Probit regression}
\subsection{Canonical link functions}

\subsection{The Laplace Approximation}
\subsection{Model comparison and BIC}

\section{Bayesian Logistic Regression}
\subsection{Laplace approximation}
\subsection{Predictive distribution}