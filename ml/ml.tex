Linear Regression:
Data representation:
N observations of x,wiritten 
                          "X \equiv (x_1,x_2,...,x_n)^T"
together with corresponding observations of the values of t,denoted 
                          "t \equiv (t_1,t_2,...t_N)^T".

Hypothesis function(model representation):
                  "y(x,\bold w)= w_0+w_1x+w_2x^2+...+w_Mx^M = \sum_{j=0}^{M}w_jx^j"
M is the order of the polynomial,and x^j denotes x raised to the power of j.The polynomial coefficients w_0,...w_M are
collectively denoted by the vector \bold w.

Error Function(Sum of squares of errors between predictions y(x_n,w) for each data point x_n and the corresponding
target values t_n,so that we minimize:
                          E(\bold w)=\frac{1}{2}\sum_{n=1}^{N}\{y(x_n,\bold w)-t_n \}^2
root-mean-squre(RMS) error defined by 
                          E_{RMS} = \sqrt[2]{2E(\bold w^*)/N}
Penalized(regularized) error function
\widetilde{E}(\textbf{w}) = \frac{1}{2}\sum_{n=1}^{N}\{y(x_n,\textbf{w}-t_n\}^2 + \frac{1}{2} \parallel \textbf{w} \parallel^2
where \parallel \textbf{w} \parallel^2 \equiv \textbf{w}^T\textbf{w}=w_0^2+w_1^2+...+w_M^2

Loss functions for regression
\mathbb{E}[\mathit{L}] = \int\int\mathit{L}(t,y(\textbf{x}))p(\textbf{x},t)d\textbf{x}dt
A common choice of loss function in squared loss given by 
                                  \mathit(L)(t,y(\textbf{x})) = \{y(\textbf{x}) - t\}^2
                        \mathbb{E}[\mathit{L}] = \int\int\{y(\textbf{x}) - t\}^2 p(\textbf{x},t)d\textbf{x}dt.
Minimize \mathbb{E}[\mathit{L}] by using the calculus of variations to give 
\frac{\delta\mathbb{E}[\mathit{L}]}{\delta y\(\textbf{x}))} = 2 \int\{ y(\textbf{x} -t) \}p(\textbf{x},t)dt = 0
Solving for y(\textbf{x}) and using the sum and product rules of probability,we obtain
y(\textbf{x}) = \frac{\int tp(\textbf{x},t)dt}{p(\textbf{x})} = \int tp(t|\textbf{x})dt = \mathbb{E}[t|\textbf{x}]
Let's derive this result in a slightly different way.Armed with knowledge that the optimal solution is the 
conditional expectation,we can expand the square term as follows
  \{y(\textbf{x} -t)\}^2
= \{y(\textbf{x} - \mathbb{E}[t|\textbf{x}] + \mathbb{E}[t|\textbf{x}] - t )\}^2 
= \{ y(\textbf{x}) - \mathbb{E}[t|\textbf{x}] \}^2 + 2\{ y(\textbf{x}) - \mathbb{E}[t|\textbf{x}] \}\{ \mathbb{E}[t|\textbf{x}]-t \} + \{ \mathbb{E}[t|\textbf{x}] -t \}^2
where,\mathbb{E}[t|\textbf{x}] denote \mathbb{E}_{t}[t|\textbf{x}].Substitute into the loss function and perform the
integral over t,we see the cross-term vanishes
\mathbb{E}[\mathit{L}]                                                            \\
= \int\int\{y(\textbf{x}) - t\}^2 p(\textbf{x},t)d\textbf{x}dt                    \\
= \int \{ y(\textbf{x}) -\mathbb{E}[t|\textbf{x}] \}^2 p(\textbf{x})d\textbf{x} + 
  \int\{ \mathbb{E}[t|\textbf{x}] - t \}^2 p(\textbf{x})d\textbf{x}               \\
= \int \{ y(\textbf{x}) -h(\textbf{x}) \}^2 p(\textbf{x})d\textbf{x} +            
  \int\{ h(\textbf{x}) - t \}^2 p(\textbf{x})d\textbf{x}                          \\
= \int \{ y(\textbf{x}) -h(\textbf{x}) \}^2 p(\textbf{x})d\textbf{x} +            
  \int\{ h(\textbf{x}) - t \}^2 p(\textbf{x},t)d\textbf{x}dt                      \\
  
For a popular choice,the squared loss function,for which the optimal prediction is given by the conditional
expectation,which we denote by h(\textbf{x}) and which is given by 
   h(\textbf{x}) = \mathbb{E}[t|\textbf{x}] = \int tp(t|\textbf{x})dt
  
Consider the integrand of the first term,which for particular data set D takes the form
  \{ y(\textbf{x};D) - h(\textbf{x}) \} ^2
This quantity will be dependent on the particular data set D,so we take its average over the ensemble of data sets.
If we add and substract the quantity \mathbb{E_D}[y(\textbf{x};D)] inside the braces,and then expand,we obtain
\{y(\textbf{x};D) - \mathbb{E_D}[y(\textbf{x};D)] + \mathbb{E_D}[y(\textbf{x};D)] -h(\textbf{x})  \}^2         \\
= \{ y(\textbf{x};D) -\mathbb{E}[y(\textbf{x};D)]    \}^2 + \{ \mathbb{E_D}[y(\textbf{x};D)] - h(\textbf{x})\}^2 +
  2\{ y(\textbf{x};D) - \mathbb{E_D}[y(\textbf{x};D)]\}\{ \mathbb{E_D}[y(\textbf{x};D)] -h(\textbf{x})\} \\
= \underbrace{\{ y(\textbf{x};D) -\mathbb{E}[y(\textbf{x};D)]    \}^2} + \underbrace{\{ \mathbb{E_D}[y(\textbf{x};D)] - h(\textbf{x})\}^2}   \\
=                    \color{red}{variance}                 +   \color{blue}{(bias)^2}  + (irreducible error)
The decomposition of the expected squared loss
                                  expected loss = (bias)^2 + variance + noise
where
                                  (bias)^2 = ...
                                  variance = ...
                                  noise = ...
  
The function y(\textbf{x}) we seek to determine enters only the first term,which will be minized when y(\textbf{x}) 
is equal to \mathbb{E}[t|\textbf{x}],in which case this term will vanish.The second term is the variance of 
distribution of t,averaged over \textbf{x},representing the intrinsic variablility of the target data and can be 
regarded as noise.It's the irreducible minimum value of the loss function.
More sophisticated loss function,Minkowski loss
\mathbb{E}[\mathit{L_q}] = \int\int| y(\textbf{x}) - t |^q p(\textbf{x},t)d\textbf{x}dt




Bias-variance decomposition of squared error
                                  y_i = f(x_i) + \epsilon 
where the noise \epsilon has zero mean and variance \sigma^2.
Find a function \hat{f}(x) that approximates the true function y = f(x) as well as possible.Make "as well as possible"

precise  by measuring the mean squared error between y and \hat{f}(x),we want (y - \hat{f}(x))^2 to be minimal.










Support Vector Machine:
Lagrange Multiplier of SVM loss function:
"\L(w,b,\alpha) = \frac{1}{2}\parallel \mathbf w \parallel + \sum_{i=1}^{m}[\alpha_iy_i(\mathbf w^Tx+b) -1]"





Adaptive basis function models
Kernel methods prediction takes the form 
                                  f(\textbf{x}) = \textbf{w}^T\phi (\textbf{x})
where we define
                                  \mathbf{\phi}(x) = [ k(\mathbf{x,\mu_1}),...,k(\mathbf{x,\mu_N})].
and where \mu_k are either all the training data or some subset.Models of this form essentially perform a form of
template matching,whereby they compare the input x to the stored prototypes \mu_k.
Adaptive basis-functional model(ABM),which is a model of the form
                                  f(\mathbf{x}) = w_0 + \sum_{m=1}^{M}{w_m\phi_m(\mathbf{x})}
where \phi_m(\mathbf{x}) is the m'th basis function,which is learned from data.Typically the basis functions are 
parametric,so we can write \phi_m(\mathbf{x}) = \phi(\mathbf{x;v_m}),whiere \mathbf{v_m} are the parameters of the 
basis function itself.

Classification and regression trees(CART)
Also called decision trees.The result of these axis parallel splits  is to partition feature space into several 
regions.We can write the model in the following form
f(\mathbf{x}) = \mathbb{E}[y|\mathbf{x}] = \sum_{m=1}^{M}\mathbb{I}(\mathbf{x}\in R_m)= \sum_{m=1}^{M}\phi (\mathbf{x;v_m})
where R_m is the m'th region,w_m is the mean response in this region,and \mathbf{v_m} encodes the choice of variable
to split on,and the threshold value,on the path from the root to the m'th leaf.
                                  Growing a tree
Finding the optimal partitioning of the data is NP-complete,so it is common to use the greedy prodecure to compute 
a locally optimal MLE.The split function chooses the best feature,and the best value for that feature,as follows:
(j^*,t^*) = arg\min_{j\in {1,...,D}}\min_{t \in \mathit{T_j}}cost(\{ \mathbf{x_i},y_i:x_{ij} \leq t\}) + cost(\{\mathbf{x_i},y_i:x_{ij} > t  \})
where x_{ij} means a feature,the t is the threshold.
  Grow a classification/regression tree in a recursive procedure.
Regression cost:
  cost(D) = \sum_{i\in D}{(y_i-\bar{y})^2}
Classification cost:
  \hat\pi= \frac{1}{|D|}\sum_{i\in D}{\mathbb{I}(y_i = c)}.
Given this,there several common error measurs for evaluating proposed partition:
\mathbf{Misclassification rate} = \frac{1}{|D|} = \sum_{i\in D}{\mathbb{I}(y_i\neq \hat{y})} = 1 - \hat\pi_\hat{y}
\mathbf{entropy}=-\sum_{c=1}^{C}\hat\pi_clog\hat\pi_c,equivalent to maximizing the information gain.
\mathbf{Gini index}=\sum_{c=1}^{C}{\hat\pi_c}(1-\hat\pi_c) = \sum_{c}{\hat\pi_c} - \sum_{c}{\hat\pi_c}^2 = 1- \sum_{c}{\hat\pi_c}^2
                                  Pruning a tree
                                  Random forests
One way to reduce the variance of an estimate is to average together many estimates.Train M differenct trees on 
different subsets of the data,chosen randomly with replacement,and then compute the ensemble.
                                  f(\mathbf{x}) = \sum_{m=1}^{M}\frac{1}{M}f_m(\mathbf{x})

                                  Generalized additive models
                                  f(\mathbf{x}) = \alpha + f_1(x_1) + ... + f_D(x_D)                                  


Boosting
  Boosting is a greedy algorithm for fitting adaptive basis-function models of the form 
  
  
  
  
  
  
 \chapter{Principal Component Analysis}
 \section{Introduction}
    Principal Component Analysis is widely used for applications such as dimensionality reduction,lossy data 
  compression,feature extraction,and data visualization.Also known as the Karhunen-Loeve transfrom.There are 
  two definitions giving rise to the same algorithm.PCA can be defined as the orthogonal projection of the data 
  onto a lower dimensional linear space,known as the principal subspace,such that the variance of the projected data
  is maximized.Equivalently,it can be defined as the linear projection the minimizes the average projection cost,
  defined as  the linear projection taht minimizes the average projection cost,defined as the mean squared distance
  between the data points and their projections.
\section{Maximum variance formulation}
    Consider a data set of observations \{x_n\} where n = 1,...,N,and x_n is a Euclidean variable wih dimensionality D.
Our goal is to project the data onto a space having dimensionality M < D while maxmizing the variance of the projected
data.We define the direction of this space using a D-dimensional unit vector \mathbf{u_1^T}\mathbf{u_1} = 1.Each 
data point \mathbf{x_n} is then projected onto a scalar value \mathbf{u_1^T}\mathbf{x_n}.The mean of the projected 
data is \mathbf{u_1^T}\bar{\mathbf{x}} where the \bar{\mathbf{x}} is the sample set mean given by
                                  \bar{\mathbf{x}} = \frac{1}{N}\sum_{n=1}^{N}{\mathbf{x_n}}
and the variance of the projected data is given by
\frac{1}{N}\sum_{n=1}^{N}\{\mathbf{u_1^T}\mathbf{x_n} - \mathbf{u_1^T}\bar{\mathbf{x}}\}^2 \\
= \frac{1}{N}\sum_{n=1}^{N}{\{\mathbf{u_1^T}(\mathbf{x_n} - \bar{\mathbf{x}})\}^2} \\
= \frac{1}{N}\sum_{n=1}^{N}{\{\mathbf{u_1^T(\mathbf{x_n - \bar{\mathbf{x}}})(\mathbf{x_n -\bar{x}})^T\mathbf{u_1^T} }  \}} \\
= \mathbf{u_1^T}\mathbf{S}\mathbf{u_1}
where \mathbf{S} is the data covariance matrix defined by
\mathbf{S} = \frac{1}{N}\sum_{n=1}^{N}(\mathbf{x_n}-\bar{\mathbf{x}})(\mathbf{x_n}-\mathbf{\bar{x}})^T
We now maximize the projected variance \mathbf{u_1^T}\mathbf{S}\mathbf{u_1} with respect to \mathbf{u_1},which is a
constrained maximization to prevent \parallel\mathbf{u_1}\parallel\rightarrow \infty .The appropriate constraint 
comes from the normalization condition \mathbf{u_1^T}\mathbf{u_1}=1.To enforce this constraint,we introduce a 
Lagrange multiplier that we shall denote by \lambda_1,and then make an unconstrained maximization of
\begin{equation}
\mathbf{u_1^T}\mathbf{S}\mathbf{u_1} + \lambda_1(1-\mathbf{u_1^T}\mathbf{u_1})
\end{equation}
By setting the derivative with respect to \mathbf{u_1} equal to zero,we see that this quantity will have a stationayr
point when
\begin{equation}
\mathbf{S}\mathbf{u_1} = \lambda_1\mathbf{u_1}
\end{equation
which says that \mathbf{u_1} must be an eigenvector of \mathbf{S}.If we left-mutiply by \mathbf{u_1^T} and make use
of \mathbf{u_1^T}{u_1} = 1,we see that the variance is given by
\begin{equation}
\mathbf{u_1^TSu_1} = \lambda_1
\end{equation}
and so the variance will be a maximum when we set \mathbf{u_1} equal to the eigenvactor having the largest 
eigenvalue \lambda_1.This eigenvector is known as the first principal component.

\section{Minimum-error formulation}


\section{Applications of PCA}

