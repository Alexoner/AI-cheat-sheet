<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>
<h1 id="linear-regression">Linear Regression</h1>
<h2 id="introduction">Introduction</h2>
<p>Linear regression is the “work horse” of statistics and (supervised) machine learning. When augmented with kernels or other forms of basis function expansion, it can model also nonlinear relationships. And when the Gaussian output is replaced with a Bernoulli or multinoulli distribution, it can be used for classification, as we will see below. So it pays to study this model in detail.</p>
<p>In the simplest approach,we can directly construct an appropriate function <span class="math inline">\(y(\vec{x})\)</span> whose values for new inputs <span class="math inline">\(\vec{x}\)</span> constitute the predictions for the corresponding values of <span class="math inline">\(t\)</span>.More generally,from a probabilistic perspective,we aim to model the predictive distribution <span class="math inline">\(p(t|x)\)</span> because this expresses the uncertainty about the value of <span class="math inline">\(t\)</span> for each value of <span class="math inline">\(x\)</span>.</p>
<h2 id="representation">Representation</h2>
<p>The simplest linear model for regression is <strong>linear regression</strong> that involves a linear combination of the input variables <span class="math display">\[y(\vec{x},\vec{w}) = w_0+w_1x_1+...+w_Dx_D\]</span> where <span class="math inline">\(\vec{x}=(x_1,...,x_D)^T\)</span>.</p>
<p>Linear regression can be made to model non-linear relationships by replacing <span class="math inline">\(\vec{x}\)</span> with some non-linear function of the inputs, <span class="math inline">\(\phi(\vec{x})\)</span>.Consider the linear combinations of fixed nonlinear functions of the input variables,of the form <span class="math display">\[y(\vec{x},\vec{w}) = w_0+\sum_{j=1}^{M-1}w_j\phi_j(x) 
=\sum_{j=0}^{M-1}w_j\phi_j(x) = \vec{w}^T\phi(x)\]</span> where <span class="math inline">\(\phi_j(x)\)</span> are known as <strong>basis functions</strong>, <span class="math inline">\(\vec{w}=(w_0,...,w_{M-1})^T\)</span> and <span class="math inline">\(\phi=(\phi_0,...,\phi_{M-1})^T\)</span>. This is known as <strong>basis function expansion</strong>. (Note that the model is still linear in the parameters <span class="math inline">\(\vec{w}\)</span>, so it is still called linear regression; the importance of this will become clear below.) A simple example are polynomial basis functions, where the model has the form <span class="math display">\[\phi(x)=(1, x, \cdots, x^d)\]</span></p>
<p>As before,we assume that the target variable is given by a deterministic function <span class="math inline">\(y(\vec{x},\vec{w})\)</span> with additive Gaussian noise so that <span class="math display">\[t=y(\vec{x},\vec{w})+\epsilon\]</span> where <span class="math inline">\(\epsilon\)</span> is a zero mean Gaussian random variable with precision(inverse variance) <span class="math inline">\(\beta\)</span>.Thus <span class="math display">\[\label{eqn:linear regression representation}
p(t|\vec{x},\vec{w},\beta)=\mathcal{N}(t|h(\vec{x},\vec{w}),\beta^{-1})\]</span> or <span class="math display">\[\begin{aligned}
&amp; y(\vec{x}) = \vec{w}^T\vec{x}+\epsilon \\
&amp; p(y|\vec{x},\vec{\theta})=\mathcal{N}(y|\vec{w}^T\vec{x}, \sigma^2) \\\end{aligned}\]</span> where <span class="math inline">\(\vec{w}\)</span> (<strong>weight vector</strong>) and <span class="math inline">\(\vec{x}\)</span> are extended vectors, <span class="math inline">\(\vec{x}=(1,x)\)</span>, <span class="math inline">\(\vec{w}=(b,w)\)</span> and <span class="math inline">\(\epsilon\)</span> has a <strong>Gaussian</strong> or <strong>normal</strong> distribution.<span class="math inline">\(w_0\)</span> is the intercept or <strong>text</strong></p>
<h2 id="maximum-likelihood-estimationsleast-squares">Maximum likelihood estimations(least squares)</h2>
<p>Assume the training examples are <strong>independently and identically distributed(IID)</strong>,we obtain the <strong>likelihood function</strong>,which is a function of adjustable parameters <span class="math inline">\(\vec{w}\)</span> and <span class="math inline">\(\beta\)</span>,in the form <span class="math display">\[\begin{aligned}
\label{eqn:linear regression likelihood}
p(\vec{t}|\vec{X},\vec{w},\beta) =
\prod_{n=1}^{N}\mathcal{N}(t_n|\vec{w}^T\phi(\vec{x_n}),\beta^{-1})\end{aligned}\]</span> we can write the <strong>log-likelihood</strong> (logarithm of the likelihood) function as follows: <span class="math display">\[\ell(\vec{\theta}) \triangleq \log p(\mathcal{D}|\vec{\theta})\]</span> A common way to estimate the parameters of a statistical model is to compute the MLE(Maximum likelihood estimations),which is defined as <span class="math display">\[\vec{\hat{\theta}}=\arg\max_\theta{\log p(\mathcal{D}|\vec{\theta})}\]</span></p>
<p>Instead of maximizing the log-likelihood, we can equivalently minimize the <strong>negative log likelihood</strong> or <strong>NLL</strong>: <span class="math display">\[\text{NLL}(\vec{\theta}) \triangleq -\ell(\vec{\theta})=-\log p(\mathcal{D}|\vec{\theta})\]</span></p>
<p>The NLL formulation is sometimes more convenient, since many optimization software packages are designed to find the minima of functions, rather than maxima.</p>
<p>Now let us apply the method of MLE to the linear regression setting. Inserting the definition of the Gaussian into the above, we find that the log likelihood is given by <span class="math display">\[\begin{aligned}
\ell(\vec{\theta}) &amp;= \log p(\vec{T}|\vec{w},\beta) \\
&amp;=\sum_{n=1}^{N}\log \mathcal{N}(t_n|\vec{w}^T\phi(\vec{x_n}),\beta^{-1}) \\
&amp;=\sum\limits_{i=1}^N \log\left[\dfrac{1}{\sqrt{2\pi}\sigma}\exp\left(-\dfrac{1}{2\sigma^2}(y_i-\vec{w}^T\vec{x}_i)^2\right)\right] \\
&amp;=\dfrac{N}{2}\log \beta-\dfrac{N}{2}\log (2\pi)-\beta E_D(\vec{w}) \\
&amp;=-\dfrac{1}{2\sigma^2}\text{RSS}(\vec{w})-\dfrac{\vec{w}}{2}\log(2\pi\sigma^2)\end{aligned}\]</span> where the sum-of-squares error function is defined by <span class="math display">\[E_D(\vec{w}) =
\dfrac{1}{2}\sum_{n=1}^{N}\{t_n-\vec{w}^T\phi(\vec{x_n}) \}^2\]</span> RSS stands for <strong>residual sum of squares</strong> and is defined by <span class="math display">\[\text{RSS}(\vec{w}) \triangleq \sum\limits_{i=1}^N (t_i-\vec{w}^T\vec{x}_i)^2\]</span> The RSS can also be written as the square of the <span class="math inline">\(\ell_2\)</span> <strong>norm</strong> of the vector of residual errors: <span class="math display">\[RSS(\vec{w}) = \parallel\epsilon\parallel_2^2 = \sum_{i=1}^{N}\epsilon_i^2\]</span> where <span class="math inline">\(\epsilon_i = (t_i - \vec{w}^T\vec{x_i})\)</span>.</p>
<p>There two ways to compute the optimal solution of likelihood function.</p>
<h3 id="derivations-of-the-mle">Derivations of the MLE</h3>
<p>We see that maximizing the likelihood function under a conditional Gaussian noise distribution for a linear model is equivalent to <span class="math inline">\(\vec{w}\)</span> minimizing the RSS(or <span class="math inline">\(E_D\)</span>), so this method is known as <strong>least squares</strong>.The gradient of the log likelihood function takes the form <span class="math display">\[\nabla\log p(\vec{t}|\vec{w},\beta)
=\sum_{n=1}^{N}\{t_n-\vec{w}^T\phi(\vec{x_n}) \}\phi(\vec{x_n})^T\]</span> Setting the gradient respect to <span class="math inline">\(\vec{w}\)</span> to zero gives <span class="math display">\[\begin{aligned}
0=\sum_{n=1}^{N}t_n\phi(\vec{x}_n)^T-\vec{w}^T(\sum_{n=1}^{N}\vec{\phi}(\vec{x}_n)\vec{\phi}(\vec{x}_n)^T)\end{aligned}\]</span> Solving for <span class="math inline">\(\vec{w}\)</span> we obtain <span class="math display">\[\begin{aligned}
\vec{w}_{ML}=(\vec{\Phi}^T\vec{\Phi})^{-1}\vec{\Phi}^T\vec{t}\end{aligned}\]</span> which are known as the <strong>normal equations</strong>.</p>
<p>Make the bias parameter explicit,the error function <span class="math display">\[\begin{aligned}
E_D(\vec{w})=\dfrac{1}{2}\sum_{n=1}^{N}\{t_n-w_0-\sum_{j=1}^{M-1}w_j\phi_j(\vec{x}_n)\}^2\end{aligned}\]</span> Setting the derivative with respect to <span class="math inline">\(w_0\)</span> equal to zero,we obtain <span class="math display">\[\begin{aligned}
w_0 = \bar{t}-\sum_{j=1}^{M-1}w_j\bar{\phi_j}\end{aligned}\]</span> Then we have <span class="math display">\[\begin{aligned}
y&amp;=\vec{w}^T\vec{\phi} \\
y&amp;=\sum_{j=1}^{M}w_j \phi_j+w_0 \\
y&amp;=\sum_{j=1}^{M}w_j \phi_j+\bar{t}-\sum_{j=1}^{M-1}w_j\bar{\phi_j} \\
y-\bar{y}&amp;=\sum_{j=1}^{M}w_j(\phi_j-\bar{\phi}_j)\end{aligned}\]</span> which indicates that we can normalize the features by subtracting the mean of <span class="math inline">\(\vec{\Phi}\)</span> while training to reduce computation complexity but obtain the same weight parameter.</p>
<p>Note that <span class="math inline">\(\vec{t}=(t_1,t_2,\cdots,t_N)^T\)</span>, <span class="math inline">\(\vec{\Phi}=\left(\begin{array}{c}\vec{\phi}_1^T \\ \vec{\phi}_2^T \\ \vdots \\ \vec{\phi}_N^T\end{array}\right)\)</span></p>
<p><span class="math inline">\(\vec{\Phi}\)</span> is an <span class="math inline">\(N\times M\)</span> matrix,called the <strong>design matrix</strong>,whose elements are given by <span class="math inline">\(\Phi_{nj}=\phi_j(\vec{x_n})\)</span> so that <span class="math display">\[\vec{\Phi}=
\begin{bmatrix}
\phi_0(\vec{x_1}) &amp; \phi_1(\vec{x_1}) &amp;...&amp;\phi_{M-1}(\vec{x_1}) \\
\phi_0(\vec{x_2}) &amp; \phi_1(\vec{x_2}) &amp;...&amp;\phi_{M-1}(\vec{x_2}) \\
\vdots &amp;\vdots &amp;\vdots&amp;\vdots \\
\phi_0(\vec{x_N})&amp; \phi_1(\vec{x_{N}})&amp; \cdot &amp;\phi_{M-1}(\vec{x_N})\\
\end{bmatrix}\]</span></p>
<p><span class="math display">\[\begin{aligned}
\because \sum_{n=1}^{N}t_n\phi(\vec{x}_n)^T &amp;=\vec{t}^T\vec{\Phi}\end{aligned}\]</span></p>
<p><span class="math display">\[\begin{aligned}
\begin{cases}
\vec{\Phi} = \begin{bmatrix}
&amp;\vec{\Phi_1^T} \\
&amp;\vec{\Phi_2^T} \\
&amp; ... \\
&amp;\vec{\Phi_n^T}
\end{bmatrix} \\    
\vec{\Phi^T}\vec{\Phi} =
\begin{bmatrix}
\vec{\Phi_1} &amp; \vec{\Phi_2} &amp; ...&amp;\vec{\Phi_n}
\end{bmatrix} 
\cdot
\begin{bmatrix}
&amp;\vec{\Phi_1^T} \\
&amp;\vec{\Phi_2^T} \\
&amp; ... \\
&amp;\vec{\Phi_N^T}
\end{bmatrix}  = \sum\limits_{i=1}^{N}\vec{\phi_i}\vec{\phi_i^T} = \textbf{sum of squares matrix} \\
\end{cases}\end{aligned}\]</span></p>
<p><span class="math display">\[\begin{aligned}
\therefore 
\vec{w}^T = \vec{t}^T\vec{\Phi}(\vec{\Phi}^T\vec{\Phi})^{-1}
\therefore
\vec{w}_{ML}=(\vec{\Phi}^T\vec{\Phi})^{-1}\vec{\Phi}^T\vec{t}\end{aligned}\]</span></p>
<p>Let’s drop constants with respect to <span class="math inline">\(\vec{w}\)</span> and NLL can be written as <span class="math display">\[\text{NLL}(\vec{w}) = \dfrac{1}{2}\sum\limits_{i=1}^N (y_i-\vec{w}^T\vec{\phi}_i)^2\]</span></p>
<p>Then we can rewritten the objective in a form that is more amendable to differentiation: <span class="math display">\[\begin{aligned}
\text{NLL}(\vec{w}) &amp;= \dfrac{1}{2}(\vec{t}-\vec{\Phi}\vec{w})^T(\vec{t}-\vec{\Phi}\vec{w})\\
&amp;= \dfrac{1}{2} [\vec{w^T} (\vec{\Phi}^T\vec{\Phi})\vec{w} - \vec{t}^T\vec{\Phi}\vec{w}- \vec{w}^T\vec{x}^T\vec{t}  +\vec{t}^T\vec{t}]          \\
\Rightarrow
\dfrac{\partial}{\partial \vec{w}}\text{NLL} &amp;= \dfrac{1}{2} \vec{w^T} (\vec{\Phi}^T\vec{\Phi})\vec{w} -\vec{w}^T(\vec{\Phi}^T\vec{t}) \end{aligned}\]</span></p>
<p>where</p>
<p>When <span class="math inline">\(\mathcal{D}\)</span> is small(for example, <span class="math inline">\(N &lt; 1000\)</span>), we can use the following equation to compute <span class="math inline">\(\vec{w}\)</span> directly <span class="math display">\[\vec{w}_{ML}=\hat{\vec{w}}_{\mathrm{OLS}}=(\vec{\Phi}^T\vec{\Phi})^{-1}\vec{\Phi}^T\vec{t}\]</span></p>
<p>The corresponding solution <span class="math inline">\(\hat{\vec{w}}_{\mathrm{OLS}}\)</span> to this linear system of equations is called the <strong>ordinary least squares</strong> or <strong>OLS</strong> or <strong>normal equation</strong> solution.</p>
<p>We now state without proof some facts of matrix derivatives (we won’t need all of these at this section). <span class="math display">\[\begin{aligned}
trA &amp;\triangleq&amp; \sum\limits_{i=1}^n A_{ii} \nonumber \\
\frac{\partial}{\partial A}AB &amp;=&amp; B^T \\
\frac{\partial}{\partial A^T}f(A) &amp;=&amp; \left[\frac{\partial}{\partial A}f(A)\right]^T \label{eqn:matrix-1} \\
\frac{\partial}{\partial A}ABA^TC &amp;=&amp; CAB+C^TAB^T \label{eqn:matrix-2} \\
\frac{\partial}{\partial A}|A| &amp;=&amp; |A|(A^{-1})^T\end{aligned}\]</span></p>
<p>Then, <span class="math display">\[\begin{aligned}
\text{NLL}(\vec{w}) &amp;=&amp; \frac{1}{2N}(\vec{X}\vec{w}-\vec{y})^T(\vec{X}\vec{w}-\vec{y}) \\
\frac{\partial \text{NLL}}{\partial\vec{w}} 
&amp;=&amp; \frac{1}{2} \frac{\partial}{\partial\vec{w}} (\vec{w}^T\vec{X}^T\vec{X}\vec{w}-\vec{w}^T\vec{X}^T\vec{y}-\vec{y}^T\vec{X}\vec{w}+\vec{y}^T\vec{y}) \\
&amp;=&amp; \frac{1}{2} \frac{\partial}{\partial\vec{w}} (\vec{w}^T\vec{X}^T\vec{X}\vec{w}-\vec{w}^T\vec{X}^T\vec{y}-\vec{y}^T\vec{X}\vec{w}) \\
&amp;=&amp; \frac{1}{2} \frac{\partial}{\partial\vec{w}}  \\
&amp;=&amp; \frac{1}{2} \frac{\partial}{\partial\vec{w}} (tr\vec{w}^T\vec{X}^T\vec{X}\vec{w}-2tr\vec{y}^T\vec{X}\vec{w})\end{aligned}\]</span></p>
<p>Combining Equations [eqn:matrix-1] and [eqn:matrix-2], we find that <span class="math display">\[\frac{\partial}{\partial A^T}ABA^TC = B^TA^TC^T+BA^TC\]</span></p>
<p>Let <span class="math inline">\(A^T=\vec{w}, B=B^T=\vec{X}^T\vec{X}\)</span>, and <span class="math inline">\(C=I\)</span>, Hence, <span class="math display">\[\begin{aligned}
\frac{\partial \text{NLL}}{\partial\vec{w}} &amp;=&amp; \frac{1}{2} (\vec{X}^T\vec{X}\vec{w}+\vec{X}^T\vec{X}\vec{w} -2\vec{X}^T\vec{y}) \nonumber \\
                           &amp;=&amp; \frac{1}{2} (\vec{X}^T\vec{X}\vec{w} - \vec{X}^T\vec{y}) \nonumber \\
\frac{\partial \text{NLL}}{\partial\vec{w}} &amp;=&amp; 0 \Rightarrow \vec{X}^T\vec{X}\vec{w} - \vec{X}^T\vec{y} =0 \nonumber \\
\vec{X}^T\vec{X}\vec{w} &amp;=&amp; \vec{X}^T\vec{y} \label{eqn:normal-equation} \\
\hat{\vec{w}}_{\mathrm{OLS}} &amp;=&amp; (\vec{X}^T\vec{X})^{-1}\vec{X}^T\vec{y} \nonumber\end{aligned}\]</span></p>
<p>Equation [eqn:normal-equation] is known as the <strong>normal equation</strong>.</p>
<p>Consider <span class="math inline">\(\mathbf{vector-vector}\)</span> differentiation,if <span class="math inline">\(\mathbf{A}\)</span> is not a function of <span class="math inline">\(\vec{x}\)</span> then <span class="math display">\[\dfrac{\partial\vec{X}^TA\vec{X}}{\partial\vec{X}} = \vec{X}^T(\mathbf{A}+\mathbf{A}^T) = (\mathbf{A}+\mathbf{A}^T)\vec{X}\]</span></p>
<p>Then,we can derivate <span class="math inline">\(\dfrac{\partial NLL}{\partial \vec{w}}\)</span> <span class="math display">\[\begin{aligned}
g(\vec{w}) &amp;= \dfrac{\partial NLL}{\partial\vec{w}} \\
&amp;=\vec{\Phi}^T\vec{\Phi}\vec{w} - \vec{\Phi}^T\vec{y}           \end{aligned}\]</span> Setting the derivative to zero,we get <span class="math display">\[\vec{w} = (\vec{X}^T\vec{X})^{-1}\vec{x}^T\vec{y}\]</span></p>
<h3 id="geometric-interpretation">Geometric interpretation</h3>
<p>See Figure [fig:graphical-interpretation-of-OLS].</p>
<div class="figure">
<img src="graphical-interpretation-of-OLS.png" alt="Graphical interpretation of least squares for N=3 examples and D=2 features. \tilde{\vec{x}}_1 and \tilde{\vec{x}}_2˜ are vectors in \mathbb{R}^3; together they define a 2D plane. \vec{y} is also a vector in \mathbb{R}^3 but does not lie on this 2D plane. The orthogonal projection of \vec{y} onto this plane is denoted \hat{\vec{y}}. The red line from \vec{y} to \hat{\vec{y}} is the residual, whose norm we want to minimize. For visual clarity, all vectors have been converted to unit norm." />
<p class="caption">Graphical interpretation of least squares for <span class="math inline">\(N=3\)</span> examples and <span class="math inline">\(D=2\)</span> features. <span class="math inline">\(\tilde{\vec{x}}_1\)</span> and <span class="math inline">\(\tilde{\vec{x}}_2\)</span>˜ are vectors in <span class="math inline">\(\mathbb{R}^3\)</span>; together they define a 2D plane. <span class="math inline">\(\vec{y}\)</span> is also a vector in <span class="math inline">\(\mathbb{R}^3\)</span> but does not lie on this 2D plane. The orthogonal projection of <span class="math inline">\(\vec{y}\)</span> onto this plane is denoted <span class="math inline">\(\hat{\vec{y}}\)</span>. The red line from <span class="math inline">\(\vec{y}\)</span> to <span class="math inline">\(\hat{\vec{y}}\)</span> is the residual, whose norm we want to minimize. For visual clarity, all vectors have been converted to unit norm.<span data-label="fig:graphical-interpretation-of-OLS"></span></p>
</div>
<p>Given that <span class="math display">\[\begin{aligned}
\vec{X} = \begin{bmatrix}
\vec{x_1}^T &amp;\\
\vec{x_2}^T &amp;\\
 ...        &amp;\\
\vec{x_N}^T
\end{bmatrix} 
= \begin{bmatrix}
\vec{\tilde{x_1}} &amp; \vec{\tilde{x_1}} &amp; ... &amp;\vec{\tilde{x_D}}
\end{bmatrix}\\
\vec{y} = \begin{bmatrix}
t_1 \\
t_2 \\
... \\
t_n
\end{bmatrix} \end{aligned}\]</span> We seek a vector <span class="math inline">\(\hat{\vec{t}} \in \mathbb{R}^N\)</span> that lies in the column linear space of <span class="math inline">\(\vec{X}\)</span> and is as close as possible to <span class="math inline">\(\vec{t}\)</span>,i.e.,we want to find <span class="math display">\[\begin{aligned}
\hat{\vec{t}} \in span(\vec{X}) \\
\Rightarrow \hat{\vec{t}} = \vec{X}\vec{w} = w_1\vec{\tilde{x_1}}+\cdot\cdot\cdot+w_D\vec{\tilde{x_D}} \\
\vec{\hat{t}}=\arg\min\limits_{\hat{\vec{t}} \in \text{span} (\{\vec{\tilde{x_1}},...,\vec{\tilde{x_D}}\})}\end{aligned}\]</span></p>
<p>To minimize the norm of the residual, <span class="math inline">\(\vec{t}-\hat{\vec{t}}\)</span>, we want the residual vector to be orthogonal to every column of <span class="math inline">\(\vec{X}\)</span>,so˜ <span class="math inline">\(\tilde{\vec{x}}_j(\vec{t}-\hat{\vec{t}})=0\)</span> for <span class="math inline">\(j=1:D\)</span>. Hence <span class="math display">\[\begin{split}
\tilde{\vec{x}}_j(\vec{t}-\hat{\vec{t}})=0 &amp; \Rightarrow \vec{X}^T(\vec{t}-\vec{X}\vec{w})=0 \\
                                           &amp; \Rightarrow \vec{w}=(\vec{X}^T\vec{X})^{-1}\vec{X}^T\vec{t}
\end{split}\]</span></p>
<h3 id="sequential-learning">Sequential learning</h3>
<p>Batch techniques,such as the maximum likelihood solution,which involves processing the entire training set in one go(pass),can be computationally costly for large data sets.If the data set is sufficiently large,it may be worthwhile to use <strong>sequential</strong> algorithms,known as <strong>on-line</strong> algorithms. When <span class="math inline">\(\mathcal{D}\)</span> is large, use <strong>stochastic gradient descent(SGD)</strong>,also known as sequential gradient descent. <span class="math display">\[\begin{aligned}
E &amp;=\sum_{n}E_n \\
\vec{w}^{\tau+1} &amp;= \vec{w}^{\tau}-\eta\nabla E_n\end{aligned}\]</span> where <span class="math inline">\(\tau\)</span> denotes the iteration number,the <span class="math inline">\(\eta\)</span> is a learning rate parameter.For the case of sum-of-squares error function,this gives <span class="math display">\[\vec{w}^{\tau+1} = \vec{w}^{\tau}-\eta(t_n-\vec{w}^{(\tau)T}\phi_n)\phi_n\]</span> where <span class="math inline">\(\phi_n=\phi(\vec{x_n})\)</span>.This is known as the <strong>least-mean-squares</strong> or the LMS algorithm.</p>
<p><span class="math display">\[\begin{aligned}
\because \dfrac{\partial}{\partial w_i}\text{NLL}(\vec{w})=&amp; \sum\limits_{i=1}^N (\vec{w}^T\vec{x}_i-y_i)x_{ij} \\
\therefore w_j=&amp; w_j - \alpha\dfrac{\partial}{\partial w_j}\text{NLL}(\vec{w}) \nonumber \\
                  =&amp; w_j - \sum\limits_{i=1}^N \alpha(\vec{w}^T\vec{x}_i-y_i)x_{ij} \\
\therefore \vec{w}=&amp; \vec{w}-\alpha(\vec{w}^T\vec{x}_i-y_i)\vec{x}\end{aligned}\]</span></p>
<h2 id="ridge-regressionmap">Ridge regression(MAP)</h2>
<p>One problem with ML estimation is that it can result in over-fitting. In this section, we discuss a way to ameliorate this problem by using MAP estimation with a Gaussian prior.</p>
<h3 id="basic-idea">Basic idea</h3>
<p>We can encourage the parameters to be small, thus resulting in a smoother curve, by using a zero-mean Gaussian prior: <span class="math display">\[p(\vec{w})=\prod\limits_j \mathcal{N}(w_j|0,\tau^2)\]</span> where <span class="math inline">\(1/\tau^2\)</span> controls the strength of the prior. The corresponding MAP estimation problem becomes <span class="math display">\[\arg\max_{\vec{w}} \sum\limits_{i=1}^N \log{\mathcal{N}(t_i|w_0+\vec{w}^T\vec{\phi}_i,\sigma^2)}+\sum\limits_{j=1}^D \log{\mathcal{N}(w_j|0,\tau^2)}\]</span></p>
<p>This is equivalent to minimizing the following <span class="math display">\[\begin{aligned}
\label{eqn:Ridge-regression-J}
J(\vec{w})&amp;=E_D(\vec{w})+\lambda E_W(\vec{w}) \\
&amp;=\dfrac{1}{N}\sum\limits_{i=1}^N (y_i-(w_0+\vec{w}^T\vec{x}_i))^2+\lambda\lVert\vec{w}\rVert^2 , \lambda \triangleq \dfrac{\sigma^2}{\tau^2}\end{aligned}\]</span> where the first term is the MSE/ NLL as usual, and the second term, <span class="math inline">\(\lambda \geq 0\)</span>, is the regularization coefficient that controls the complexity penalty. Here <span class="math inline">\(E_W(\vec{w})\)</span> is one of the simplest forms of regularizer given by the sum-of-squares of the weight vector elements. <span class="math display">\[E_W(\vec{w})=\dfrac{1}{2}\vec{w}^T\vec{w}\]</span> This particular choice of regularizer is known as <strong>weight decay</strong>.</p>
<p>The corresponding solution is given by <span class="math display">\[\label{eqn:Ridge-regression-solution}
\hat{\vec{w}}_{\mathrm{ridge}}=(\lambda\vec{I}_D+\vec{\Phi}^T\vec{\Phi})^{-1}\vec{\Phi}^T\vec{t}\]</span></p>
<p>This technique is known as <strong>ridge regression</strong>,or <strong>penalized least squares</strong>. In general, adding a Gaussian prior to the parameters of a model to encourage them to be small is called <span class="math inline">\(\ell_2\)</span> <strong>regularization</strong> or <strong>weight decay</strong>. Note that the offset term <span class="math inline">\(w_0\)</span> is not regularized, since this just affects the height of the function, not its complexity.</p>
<p>We will consider a variety of different priors in this book. Each of these corresponds to a different form of <strong>regularization</strong>. This technique is very widely used to prevent overfitting.</p>
<h3 id="multiple-outputs">Multiple outputs</h3>
<p>Now consider the case where we wish to predict <span class="math inline">\(K&gt;1\)</span> target variables,which we denote collectively by the target vector <span class="math inline">\(\vec{t}\)</span>.To use the same set of basis functions to model all the components of the target vector so that <span class="math display">\[\vec{y}(\vec{x},\vec{w}) = \vec{W}^T\phi(x)\]</span> where <span class="math inline">\(\vec{y}\)</span> is a <span class="math inline">\(K\)</span>-dimensional column vector,<span class="math inline">\(\vec{W}\)</span> is an <span class="math inline">\(M\times K\)</span> matrix of parameters.</p>
<p>The conditional distribution of the target vector is an isotropic Gaussian <span class="math display">\[p(\vec{t}|\vec{x},\vec{W},\beta) =
\mathcal{N}(\vec{t}|\vec{W}^T\phi(x,\beta^{-1}\vec{I}))\]</span></p>
<p>The log likelihood function is then given by <span class="math display">\[\begin{aligned}
\ln p(\vec{T}|\vec{X},\vec{W},\beta) 
&amp;=\sum_{n=1}^{N}\ln \mathcal{N}(\vec{t_n}|\vec{W}^T\phi(x_n),\beta^{-1}\vec{I}) \\
&amp;=\dfrac{NK}{2}\ln(\dfrac{\beta}{2\pi}) 
 -\dfrac{\beta}{2}\sum_{n=1}^{N}\parallel \vec{t_n}-\vec{W}^T\phi(x_n)\parallel^2\end{aligned}\]</span> We maximize this function with respect to <span class="math inline">\(\vec{W}\)</span>,giving <span class="math display">\[\vec{W}_{ML} = (\vec{\Phi}^T\vec{\Phi})^{-1}\vec{\Phi}^T\vec{T}\]</span></p>
<h3 id="numerically-stable-computation">Numerically stable computation *</h3>
<p><span class="math display">\[\label{eqn:Ridge-regression-SVD}
\hat{\vec{w}}_{\mathrm{ridge}}=\vec{V}(\vec{Z}^T\vec{Z}+\lambda\vec{I}_N)^{-1}\vec{Z}^T\vec{y}\]</span></p>
<h3 id="connection-with-pca">Connection with PCA *</h3>
<h3 id="regularization-effects-of-big-data">Regularization effects of big data</h3>
<p>Regularization is the most common way to avoid overfitting. However, another effective approach — which is not always available — is to use lots of data. It should be intuitively obvious that the more training data we have, the better we will be able to learn.</p>
<p>In domains with lots of data, simple methods can work surprisingly well (Halevy et al. 2009). However, there are still reasons to study more sophisticated learning methods, because there will always be problems for which we have little data. For example, even in such a data-rich domain as web search, as soon as we want to start personalizing the results, the amount of data available for any given user starts to look small again (relative to the complexity of the problem).</p>
<h2 id="the-bias-variance-decomposition">The Bias-Variance Decomposition</h2>
<h4 id="representation-1">representation</h4>
<p>Data representation: N observations of x,wiritten <span class="math inline">\(X \equiv (x_1,x_2,...,x_n)^T\)</span> together with corresponding observations of the values of t,denoted <span class="math inline">\(t \equiv (t_1,t_2,...t_N)^T\)</span>. <span class="math display">\[t_i = f(\vec{x_i}) + \epsilon\]</span> where the noise <span class="math inline">\( \epsilon \)</span> has zero mean and variance <span class="math inline">\( \sigma^2 \)</span>. Find a function <span class="math display">\[\hat{f}(x)\]</span> that approximates the true function <span class="math display">\[t = f(\vec{x})\]</span> as well as possible.Make “as well as possible” precise by measuring the mean squared error between y and <span class="math inline">\( \hat{f}(x) \)</span>,we want <span class="math inline">\( (t - \hat{f}(x))^2 \)</span> to be minimal. Hypothesis function(model representation): <span class="math inline">\(y(x,\bold w)= w_0+w_1x+w_2x^2+...+w_Mx^M = \sum_{j=0}^{M}w_jx^j\)</span> M is the order of the polynomial,and <span class="math inline">\(x^j\)</span> denotes <span class="math inline">\(x\)</span> raised to the power of <span class="math inline">\(j\)</span>.The polynomial coefficients <span class="math inline">\(w_0,...w_M\)</span> are collectively denoted by the vector <span class="math inline">\(\mathbf{w}\)</span>.</p>
<p>Error Function(Sum of squares of errors between predictions <span class="math inline">\(y(x_n,w)\)</span> for each data point <span class="math inline">\(x_n\)</span> and the corresponding target values <span class="math inline">\(t_n\)</span>,so that we minimize: <span class="math display">\[E(\bold w)=\frac{1}{2}\sum_{n=1}^{N}\{y(x_n,\bold w)-t_n \}^2\]</span> root-mean-squre(RMS) error defined by <span class="math display">\[E_{RMS} = \sqrt[2]{2E(\bold w^*)/N}\]</span> Penalized(regularized) error function <span class="math display">\[\widetilde{E}(\textbf{w}) = \frac{1}{2}\sum_{n=1}^{N}\{y(x_n,\textbf{w}-t_n\}^2 + \frac{1}{2} \parallel \textbf{w} \parallel^2\]</span> where <span class="math inline">\(\parallel \textbf{w} \parallel^2 \equiv \textbf{w}^T\textbf{w}=w_0^2+w_1^2+...+w_M^2\)</span></p>
<h4 id="loss-function-for-regression">Loss function for regression</h4>
<p><span class="math display">\[\mathbb{E}[\mathit{L}] = \iint\mathit{L}(t,y(\textbf{x}))p(\textbf{x},t)d\textbf{x}dt\]</span></p>
<p>A common choice of loss function in squared loss given by <span class="math display">\[\begin{aligned}
\mathit{L}(t,y(\textbf{x})) = \{y(\textbf{x}) - t\}^2 \\
\mathbb{E}[\mathit{L}] = \iint\{y(\textbf{x}) - t\}^2 p(\textbf{x},t)d\textbf{x}dt.\end{aligned}\]</span> Minimize <span class="math inline">\( \mathbb{E}[\mathit{L}] \)</span> by using the calculus of variations to give <span class="math display">\[\frac{\delta\mathbb{E}[\mathit{L}]}{\delta y(\mathbf{x}))} = 2 \int\{ y(\mathbf{x} -t) \}p(\mathbf{x},t)dt = 0\]</span> Solving for <span class="math inline">\( y(\textbf{x}) \)</span> and using the sum and product rules of probability,we obtain <span class="math display">\[y(\textbf{x}) = \frac{\int tp(\textbf{x},t)dt}{p(\textbf{x})} = \int tp(t|\textbf{x})dt = \mathbb{E}[t|\textbf{x}]\]</span></p>
<p>Let’s derive this result in a slightly different way.Armed with knowledge that the optimal solution is the conditional expectation,we can expand the square term as follows <span class="math display">\[\{y(\textbf{x} -t)\}^2
= \{y(\textbf{x}) - \mathbb{E}[t|\textbf{x}] + \mathbb{E}[t|\textbf{x}] - t )\}^2 
= \{ y(\textbf{x}) - \mathbb{E}[t|\textbf{x}] \}^2 + 2\{ y(\textbf{x}) - \mathbb{E}[t|\textbf{x}] \}\{ \mathbb{E}[t|\textbf{x}]-t \} + \{ \mathbb{E}[t|\textbf{x}] -t \}^2\]</span> where,<span class="math inline">\( \mathbb{E}[t|\textbf{x}] \)</span> denote <span class="math inline">\( \mathbb{E}_{t}[t|\textbf{x}] \)</span>.Substitute into the loss function and perform the integral over t,we see the cross-term vanishes <span class="math display">\[\begin{aligned}
\label{eqn:squared loss function}
\mathbb{E}[\mathit{L}]                                                           
&amp;= \iint\{y(\textbf{x}) - t\}^2 p(\textbf{x},t)d\textbf{x}dt                    \\
&amp;= \int \{ y(\textbf{x}) -\mathbb{E}[t|\textbf{x}] \}^2 p(\textbf{x})d\textbf{x} + 
\int\{ \mathbb{E}[t|\textbf{x}] - t \}^2 p(\textbf{x})d\textbf{x}               \\
&amp;= \int \{ y(\textbf{x}) -h(\textbf{x}) \}^2 p(\textbf{x})d\textbf{x} +            
\int\{ h(\textbf{x}) - t \}^2 p(\textbf{x})d\textbf{x}                          \\
&amp;= \int \{ y(\textbf{x}) -h(\textbf{x}) \}^2 p(\textbf{x})d\textbf{x} +            
\int\{ h(\textbf{x}) - t \}^2 p(\textbf{x},t)d\textbf{x}dt                      \\\end{aligned}\]</span></p>
<h4 id="decomposition">Decomposition</h4>
<p>For a popular choice,we use squared loss function,for which the optimal prediction is given by the conditional expectation,which we denote by h(<strong>x</strong>) and which is given by <span class="math display">\[h(\textbf{x}) = \mathbb{E}[t|\textbf{x}] = \int tp(t|\textbf{x})dt\]</span></p>
<p>Consider the integrand of the first term of [eqn:squared loss function],which for particular data set D takes the form <span class="math display">\[\{ y(\textbf{x};D) - h(\textbf{x}) \} ^2\]</span> This quantity will be dependent on the particular data set D,so we take its average over the ensemble of data sets. If we add and subtract the quantity <span class="math inline">\( \mathbb{E_D}[y(\textbf{x};D)] \)</span> inside the braces,and then expand,we obtain <span class="math display">\[\begin{aligned}
\{ y(\textbf{x};D) - h(\textbf{x}) \} ^2 \\
=&amp;\{y(\textbf{x};D) - \mathbb{E_D}[y(\textbf{x};D)] 
    + \mathbb{E_D}[y(\textbf{x};D)] -h(\textbf{x})  \}^2         \\
=&amp; \{ y(\textbf{x};D) -\mathbb{E}_{\mathbb{D}}[y(\textbf{x};D)]    \}^2 
    + \{ \mathbb{E_D}[y(\textbf{x};D)] - h(\textbf{x})\}^2
    + 2\{ y(\textbf{x};D) - \mathbb{E_D}[y(\textbf{x};D)]\}\{ \mathbb{E_D}[y(\textbf{x};D)] -h(\textbf{x})\} \\
=&amp; \underbrace{\{ y(\textbf{x};D) -\mathbb{E}_{\mathbb{D}}[y(\textbf{x};D)]    \}^2}_\text{\color{red}{variance}} 
    + \underbrace{\{ \mathbb{E_D}[y(\textbf{x};D)] - h(\textbf{x})\}^2}_\text{\color{blue}$(bias)^2$} 
    +0\\\end{aligned}\]</span> The decomposition of the expected squared loss <span class="math display">\[\text{expected loss} = (bias)^2 + variance + noise\]</span> where <span class="math display">\[\begin{aligned}
(bias)^2 = ... \\
variance = ... \\
noise = ...\end{aligned}\]</span></p>
<p>The function <span class="math inline">\( y(\textbf{x}) \)</span> we seek to determine enters only the first term,which will be minimized when <span class="math inline">\( y(\textbf{x}) \)</span> is equal to <span class="math inline">\( \mathbb{E}[t|\textbf{x}] \)</span>,in which case this term will vanish.The second term is the variance of distribution of t,averaged over <span class="math inline">\( \textbf{x} \)</span>,representing the intrinsic variabilility of the target data and can be regarded as noise.It’s the irreducible minimum value of the loss function.</p>
<p>More sophisticated loss function,Minkowski loss <span class="math display">\[\mathbb{E}[\mathit{L_q}] = \iint| y(\textbf{x}) - t |^q p(\textbf{x},t)d\textbf{x}dt\]</span></p>
<h2 id="bayesian-linear-regression">Bayesian linear regression</h2>
<p>Hold-out data can be used to determine model complexity but it will be computationally expensive and wasteful of valuable data.We therefore turn to a Bayesian treatment of linear regression,which will avoid the over-fitting problem of maximum likelihood,and which will also lead to automatic methods of determining model complexity using the training data.</p>
<h3 id="parameter-distribution">Parameter distribution</h3>
<p>First introduce a prior probability distribution over the model parameters.The likelihood function <span class="math inline">\(p(\vec{t}|\vec{w})\)</span> defined by [eqn:linear regression likelihood] is the exponential of a quadratic function of <span class="math inline">\(\vec{w}\)</span>,so the corresponding conjugate prior is Gaussian <span class="math display">\[p(\vec{w}) = \mathcal{N}(\vec{w}|\vec{m_0},\vec{S}_0)\]</span></p>
<p>The posterior distribution is proportional to the product of the likelihood and the prior.And the posterior will also be Gaussian due to the choice of conjugate Gaussian prior,which is derived in [sec:Gaussian distribution]. <span class="math display">\[\label{eqn:Bayes linear regression posterior}
p(\vec{w}|\vec{t}) = \mathcal{N}(\vec{w}|\vec{m}_N,\vec{S}_N)\]</span> where <span class="math display">\[\begin{aligned}
\vec{m}_N &amp;=\vec{S}_N(\vec{S}_0^{-1}\vec{m}_0+\beta\vec{\Phi}^T\vec{t})\\
\vec{S}_N^{-1} &amp;= \vec{S}_0^{-1}+\beta\vec{\Phi}^T\vec{\Phi}\end{aligned}\]</span> Thus the maximum posterior weigh vector is simply given by <span class="math inline">\(\vec{w}_{MAP}=\vec{m}_{N}\)</span>.If <span class="math inline">\(N=0\)</span> then the posterior distribution reverts to the prior.Furthermore,if data points arrive sequentially,then the posterior distribution at any stage acts as the prior distribution,such that the new posterior is again given.</p>
<p>A zero-mean isotropic Gaussian governed by a single precision parameter <span class="math inline">\(\alpha\)</span> so that <span class="math display">\[p(\vec{w}|\alpha) = \mathcal{N}(\vec{w}|\vec{0},\alpha^{-1}\vec{I})\]</span> for which the posterior is given by <span class="math display">\[\begin{aligned}
\vec{m}_N &amp;=\beta\vec{S}_N\vec{\Phi}^T\vec{t} \\
\vec{S}_N^{-1}&amp;=\alpha\vec{I}+\beta\vec{\Phi}^T\vec{\Phi}\end{aligned}\]</span></p>
<p>The log of posterior distribution is given by the sum of the log likelihood and the log of prior and,as a function of <span class="math inline">\(\vec{w}\)</span>,takes the form <span class="math display">\[\log p(\vec{w}|\vec{t}) = 
-\dfrac{\beta}{2}\sum_{n=1}^{N}\{t_n-\vec{w}^T\phi(\vec{x}_n) \}^2-\dfrac{\alpha}{2}\vec{w}^T\vec{w}+const\]</span> Maximization of this posterior w.r.t <span class="math inline">\(\vec{w}\)</span> is equivalent to minimization of the sum-of-squares error function with the addition of a quadratic regularization term,corresponding with <span class="math inline">\(\lambda=\alpha/\beta\)</span>.</p>
<h3 id="predictive-distribution">Predictive distribution</h3>
<p>Evaluate the <strong>predictive distribution</strong> defined by <span class="math display">\[p(t|\vec{t},\alpha,\beta) = \int p(t|\vec{w},\beta)p(\vec{w}|\vec{t},\alpha,\beta)d\vec{w}\]</span> The right hand quantity can be interpreted as the marginalization by integration of the joint distribution of <span class="math inline">\(p(t,\vec{w}|\vec{t},\alpha,\beta)\)</span> over <span class="math inline">\(\vec{w}\)</span>.And the joint distribution can be solved by the Bayesian theorem for Gaussian. The conditional distribution <span class="math inline">\(p(t|\vec{x},\vec{w},\beta)\)</span> of the target variable is given by [eqn:linear regression representation],and the posterior weight distribution is given by [eqn:Bayes linear regression posterior].This involves the <strong>convolution</strong> of two Gaussian distributions.The predictive distribution take the form <span class="math display">\[p(t|x,\vec{t},\alpha,\beta) = \mathcal{N}(t|\vec{m}_N^T\phi(x),\sigma_N^2(\vec{x}))\]</span> where the variance <span class="math inline">\(\sigma_N^2(\vec{x})\)</span> is given by <span class="math display">\[\sigma_N^2(\vec{x})=\dfrac{1}{\beta}+\phi(\vec{x})^T\vec{S}_N\phi(\vec{x})\]</span> The first term represents the noise on the data whereas the second term reflects the uncertainty associated with the parameters <span class="math inline">\(\vec{w}\)</span>. <span class="math inline">\(\sigma_{N+1}^2(\vec{x})\leq \sigma_N(\vec{x})\)</span>.In the limit <span class="math inline">\(N\rightarrow \infty\)</span>,the second term goes to zero.</p>
<p>Note that if both <span class="math inline">\(\vec{w}\)</span> and <span class="math inline">\(\beta\)</span> are treated as unknown,then we can introduce a conjugate prior distribution <span class="math inline">\(p(\vec{w},\beta)\)</span> given by Gaussian-gamma distribution,leading to a Student’s t-distribution predictive distribution.</p>
<h3 id="equivalent-kernel">Equivalent kernel</h3>
<p>The posterior mean solution [] has an interpretation that will set stage for kernel methods,including Gaussian processes.The predictive mean: <span class="math display">\[y(\vec{x},\vec{w})=\vec{m}_N^T\vec{\phi}(\vec{x})
=\beta\vec{\phi}(\vec{x})^T\vec{S}_N\vec{\Phi}^T\vec{t}
=\sum_{n=1}^{N}\beta\vec{\phi}(\vec{x})^T\vec{S}_N\vec{\phi}(\vec{x}_n)t_n
=\sum_{n=1}^{N}\mathit{k}(\vec{x},\vec{x}_n)t_n\]</span> where <span class="math display">\[k(\vec{x},\vec{x&#39;})
=\beta\vec{\phi}(\vec{x})^T\vec{S}_N\vec{\phi}(\vec{x&#39;})\]</span> is known as the <strong>smoother matrix</strong> or <strong>equivalent kernel</strong>.Regression functions,such as this,which make predictions by taking linear combinations of the training set target values are known <strong>linear smoothers</strong>.The kernel functions are localized around <span class="math inline">\(x\)</span>(local evidence weight more than distant evidence).</p>
<p>The covariance between <span class="math inline">\(y(\vec{x})\)</span> and <span class="math inline">\(y(\vec{x&#39;})\)</span> is given by <span class="math display">\[\begin{aligned}
cov[y(\vec{x}),y(\vec{x&#39;})] &amp;=cov[\vec{\phi}(\vec{x})^T\vec{w},\vec{w}^T\vec{\phi}(\vec{x&#39;})] \\
&amp;=\phi(\vec{x})^T\vec{S}_N\phi(\vec{x&#39;})=\beta^{-1}\mathit{k}(\vec{x},\vec{x&#39;})\end{aligned}\]</span> We see that the predictive mean at nearby points will be highly correlated.</p>
<p>The formulation of linear regression in terms of kernel functions suggests an alternative approach,called <strong>Gaussian process</strong>:Instead of introducing a set of basis functions,which implicitly determines an equivalent kernel,we can instead define a localized kernel directly and use this to make predictions for new input vectors <span class="math inline">\(\vec{x}\)</span>,given the observed training set.</p>
<p>The effective kernel defines the weights by which the training set target values are combined in order to make a prediction at a new value of <span class="math inline">\(x\)</span>,and it can be shown that these weights sum to one, <span class="math display">\[\begin{aligned}
\sum\limits_{n=1}^{N}\mathcal{k}(\vec{x},\vec{x}_n) = 1\end{aligned}\]</span> for all values of <span class="math inline">\(\vec{x}\)</span>.</p>
<p>General kernel functions share an important property,namely that it can be expressed in the form an inner product with respect to a vector <span class="math inline">\(\varPsi(\vec{x}) \)</span> of nonlinear functions,so that <span class="math display">\[\begin{aligned}
\mathcal{k}(\vec{x},\vec{z}) = \varPsi(\vec{x})^T\varPsi(\vec{z})\end{aligned}\]</span> where <span class="math inline">\(\varPsi(\vec{x})=\beta^{1/2}\vec{S}_n^{1/2}\phi(\vec{x})\)</span></p>
<h2 id="bayesian-model-comparison">Bayesian Model Comparison</h2>
<p>The over-fitting associated with maximum likelihood can be avoided by <strong>marginalizing(summing or integrating)</strong> over the model parameters instead of making point estimates of their values,no need for validation.</p>
<p>Suppose we wish to compare a set of <span class="math inline">\(L\)</span> models <span class="math inline">\(\{\mathcal{M}_i\},i=1,...L\)</span> over observed data <span class="math inline">\(\mathcal{D}\)</span>.Evaluate the posterior distribution <span class="math display">\[\begin{aligned}
p(\mathcal{M}_i|\mathcal{D})\propto p(\mathcal{M}_i)p(\mathcal{D}|\mathcal{M}_i)\end{aligned}\]</span> The prior allows us to express a preference for different models.<span class="math inline">\(p(\mathcal{D}|\mathcal{M}_i)\)</span> is the <strong>model evidence</strong>,which expresses the preference shown by the data for different models.It is also called the <strong>marginal likelihood</strong>.The ratio of model evidences <span class="math inline">\(p(\mathcal{D}|\mathcal{M}_i)/p(\mathcal{D}|\mathcal{M}_j)\)</span> for two models is <strong>Bayes factor</strong>. The predictive distribution <span class="math display">\[\begin{aligned}
&amp; p(t|\vec{x},\mathcal{D})p(\mathcal{M}_i|\mathcal{D})&amp;=p(t|\vec{x},\mathcal{M}_i,\mathcal{D})p(\mathcal{M}_i|\mathcal{D}) \\
&amp;\Rightarrow \sum\limits_{i=1}^{L}p(t|\vec{x},\mathcal{D})p(\mathcal{M}_i|\mathcal{D})&amp;=\sum\limits_{i=1}^{L}p(t|\vec{x},\mathcal{M}_i,\mathcal{D})p(\mathcal{M}_i|\mathcal{D}) \\
&amp;\Rightarrow p(t|\vec{x},\mathcal{D})&amp;=\sum\limits_{i=1}^{L}p(t|\vec{x},\mathcal{M}_i,\mathcal{D})p(\mathcal{M}_i|\mathcal{D}) \\\end{aligned}\]</span> This is an example of a <strong>mixture distribution</strong> in which the overall predictive distribution is obtained by averaging the predictive distributions of individual models weighted by the posterior probabilities <span class="math inline">\(p(\mathcal{M}_i|\mathcal{D})\)</span> of those models.</p>
<p>To use the single most probable model alone to make predictions is known as <strong>model selection</strong>.Model evidence <span class="math display">\[\begin{aligned}
p(\mathcal{D}|\mathcal{M}_i)=\int p(\mathcal{D}|\mathcal{\vec{w}})p(\vec{w}|\mathcal{M}_i)d\vec{w}\end{aligned}\]</span> because <span class="math display">\[\begin{aligned}
p(\vec{w}|\mathcal{D},\mathcal{M}_i)=\dfrac{p(\mathcal{D}|\vec{w},\mathcal{M}_i)p(\vec{w}|\mathcal{M}_i)}{p(\mathcal{D}|\mathcal{M}_i)}\end{aligned}\]</span></p>
<p>Assume the posterior distribution over parameters is sharply peaked around the most probable value <span class="math inline">\(w_{MAP}\)</span>,with width <span class="math inline">\(\Delta w_{posterior}\)</span>,then we can approximate the integral by the value of integrand at its maximum times the width of the peak.And further assume that the prior is flat with width <span class="math inline">\(\Delta w_{prior}\)</span> so that <span class="math inline">\(p(w)=1/\Delta w_{prior}\)</span>,then we have <span class="math display">\[\begin{aligned}
p(\mathcal{D})=\int p(\mathcal{D}|w)p(w)dw \simeq p(\mathcal{D}|w_{MAP})p(w_{MAP})\Delta w_{posterior} \simeq p(\mathcal{D}|w_{MAP})\dfrac{\Delta w_{posterior}}{\Delta w_{prior}}\end{aligned}\]</span></p>
<p><embed src="prml/Figure3.12" /></p>
<p>and so taking logs <span class="math display">\[\begin{aligned}
\ln p(\mathcal{D}) \simeq \ln p(\mathcal{D}|w_{MAP}) +\ln(\dfrac{\Delta w_{posterior}}{\Delta w_{prior}})\end{aligned}\]</span> If we have a set of <span class="math inline">\(M\)</span> parameters,assuming that all parameters have the same ratio of <span class="math inline">\(\Delta w_{posterior}/\Delta w_{prior}\)</span>,we obtain <span class="math display">\[\begin{aligned}
\ln p(\mathcal{D}) \simeq \ln p(\mathcal{D}|w_{MAP}) +M\ln(\dfrac{\Delta w_{posterior}}{\Delta w_{prior}})\end{aligned}\]</span> As we increase the complexity of the model,the first term representing the fit to the data,increases(?),whereas the second term will decrease due to the dependence on <span class="math inline">\(M\)</span>.The optimal model is given by a trade-off between these two competing terms.</p>
<p><embed src="prml/Figure3.13" /></p>
<p>Average the Bayes factor over the distribution of data sets <span class="math display">\[\begin{aligned}
\int p(\mathcal{D}|\mathcal{M}_1)\ln\dfrac{p(\mathcal{D}|\mathcal{M}_1)}{p(\mathcal{D}|\mathcal{M}_2)}\end{aligned}\]</span> This quantity is <span class="math inline">\(Kullback-Leibler\)</span> divergence,and will be bigger than or equal to <span class="math inline">\(0\)</span> if <span class="math inline">\(\mathcal{M}_1\)</span> is the correct model.</p>
<h2 id="model-evidence">Model Evidence</h2>
</body>
</html>
