<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>
<h1 id="chapter:Introduction">Introduction</h1>
<h2 id="types-of-machine-learning">Types of machine learning</h2>
<p><span class="math display">\[\nonumber
\begin{cases}
\text{Supervised Learning} \begin{cases} \text{Classification} \\ \text{Regression} \end{cases}\\
\text{Unsupervised Learning} \begin{cases} \text{Discovering clusters} \\ \text{Discovering latent factors} \\ \text{Discovering graph structure} \\ \text{Matrix completion} \end{cases}\\
\text{Reinforcement Learning}
\end{cases}\]</span></p>
<p>In the <strong>predictive</strong> or <strong>supervised learning</strong> approach,the goal is to learn a <strong>mapping</strong> from <strong>inputs</strong> <span class="math inline">\(\mathbf{x}\)</span> to <strong>outputs</strong> <span class="math inline">\(y\)</span>,given a labeled set of input-output paris <span class="math inline">\(D=\{(\mathbf{x_i},y_i)\}_{i=1}{N}\)</span>.Here <span class="math inline">\(D\)</span> is called the <strong>training set</strong>,and N is the number of training examples. In the simplest setting,each training input <span class="math inline">\(\mathbf{x_i}\)</span> is a D-dimensional vector of numbers,representing,say,the height and weight of a person,which are called <strong>features,attributes,or covariates</strong>.</p>
<h2 id="supervised-learning">Supervised learning</h2>
<p>Similarly the form of the output or <strong>response variable</strong> can in principle be anything,but most methods assume that <span class="math inline">\(y_i\)</span> is <strong>categorical or nominal</strong> variable from some finite set,<span class="math inline">\(y_i \in \{1,...,C\}\)</span>.When <span class="math inline">\(y_i\)</span> is categorical,the problem is known as <strong>classification or pattern recognition</strong>,and when real-valued,known as <strong>regression</strong>.</p>
<h3 id="classification">Classification</h3>
<p>Here the goal is to learn a mapping from inputs <span class="math inline">\(x\)</span> to outputs <span class="math inline">\(y\)</span>,where <span class="math inline">\(y \in \{1,...,C\}\)</span>,with <span class="math inline">\(C\)</span> being the number of classes.If <span class="math inline">\(C=2\)</span>,this is called <strong>binary classification</strong>;if <span class="math inline">\(C&gt;2\)</span>,this is called <strong>multiclass classification</strong>.if the class labels are not mutually exclusive,we call it <strong>multi-label classification</strong>,but this is best viewed as predicting multiple related binary class labels(a so-called <strong>multiple output model</strong>). One way to formalize the problem is as <strong>function approximation</strong>:assume <span class="math inline">\(y=f(\mathbf{x})\)</span> for some unknown function <span class="math inline">\(f\)</span>,and the goal of learning is to estimate the function <span class="math inline">\(f\)</span> given a labeled training set,and then to make predictions(estimate) using <span class="math inline">\(\hat{y} =\hat{f}(\mathbf{x})\)</span>.Our main goal is to make predictions on novel inputs,meaning ones that we have not seen before(<strong>generalization</strong>.</p>
<h4 id="probabilistic-predictions">Probabilistic predictions</h4>
<p>We denote the probability distribution over possible labels,given the input vector <span class="math inline">\(\vec{x}\)</span> and training set <span class="math inline">\(\mathcal{D}\)</span> by <span class="math inline">\(p(y|\vec{x},\mathcal{D})\)</span>. Given a probabilistic output,we can always compute out “best guess” as to the “true label” using <span class="math display">\[\hat{y} = \hat{f}(\mathbf{x}) = \arg\max\limits_{c=1}^{C} p(y=c|\mathbf{x},D)\]</span> This corresponds to the most probable class label,and is called the <strong>mode</strong> of distribution <span class="math inline">\(p(y|\vec{x},\mathcal{D})\)</span>;it is also known as a <strong>MAP estimate</strong>(MAP stands for<strong>maximum a posteriori</strong>).</p>
<h4 id="applications">Applications</h4>
<h3 id="regression">Regression</h3>
<h2 id="unsupervised-learning">Unsupervised learning</h2>
<p><strong>Descriptive or unsupervised learning</strong> approach is sometimes called <strong>knowledge discovery</strong>.We will formalize out task as one of <strong>density estimation</strong>,that is we want to build models of the form <span class="math inline">\(p(\mathbf{x_i|\theta})\)</span>,instead of <span class="math inline">\(p(y_i|\mathbf{x_i,\theta})\)</span>.</p>
<h3 id="discovering-clusters">Discovering clusters</h3>
<p>Let <span class="math inline">\(z_i \in \{1,...,K\}\)</span> represent the cluster to which data point <span class="math inline">\(i\)</span> is assigned.(<span class="math inline">\(z_i\)</span> is an exmaple of <strong>hidden or latent</strong> variable).</p>
<h3 id="discovering-latent-factors">Discovering latent factors</h3>
<p>Although the data may appear high dimensional,there may only be a small number of degrees of variability,corresponding to <strong>latent factors</strong>.The most common approach to dimensionality reduction is called <strong>principal components analysis or PCA</strong>.</p>
<h3 id="discovering-graph-structure">Discovering graph structure</h3>
<h3 id="matrix-completion">Matrix completion</h3>
<h4 id="image-inpainting">Image inpainting</h4>
<h4 id="collaborative-filtering">Collaborative filtering</h4>
<h4 id="market-basket-analysis">Market basket analysis</h4>
<h2 id="three-elements-of-a-machine-learning-model">Three elements of a machine learning model</h2>
<p><strong>Model = Representation + Evaluation + Optimization</strong><a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a></p>
<h3 id="representation">Representation</h3>
<p>In supervised learning, a model must be represented as a conditional probability distribution <span class="math inline">\(P(y|\vec{x})\)</span>(usually we call it classifier) or a decision function <span class="math inline">\(f(x)\)</span>. The set of classifiers(or decision functions) is called the hypothesis space of the model. Choosing a representation for a model is tantamount to choosing the hypothesis space that it can possibly learn.</p>
<h3 id="evaluation">Evaluation</h3>
<p>In the hypothesis space, an evaluation function (also called objective function or risk function) is needed to distinguish good classifiers(or decision functions) from bad ones.</p>
<h4 id="sec:Loss-function-and-risk-function">Loss function and risk function</h4>
<p>In order to measure how well a function fits the training data, a <strong>loss function</strong> <span class="math inline">\(L:Y \times Y \rightarrow R \geq 0\)</span> is defined. For training example <span class="math inline">\((x_i,y_i)\)</span>, the loss of predicting the value <span class="math inline">\(\widehat{y}\)</span> is <span class="math inline">\(L(y_i,\widehat{y})\)</span>.</p>
<p>The following is some common loss functions:</p>
<ol>
<li><p>0-1 loss function<br />
<span class="math inline">\(L(Y,f(X))=\mathbb{I}(Y,f(X))=\begin{cases} 1, &amp; Y=f(X) \\ 0, &amp; Y \neq f(X) \end{cases}\)</span></p></li>
<li><p>Quadratic(squared) loss function <span class="math inline">\(L(Y,f(X))=\frac{1}{2}\left(Y-f(X)\right)^2\)</span></p></li>
<li><p>Absolute loss function <span class="math inline">\(L(Y,f(X))=\abs{Y-f(X)}\)</span></p></li>
<li><p>Exponential loss function <span class="math inline">\(L(Y,f(X)=exp(-\hat{y_i}f(\mathbf{x_i}))\)</span></p></li>
<li><p>Logarithmic loss function<br />
<span class="math inline">\(L(Y,P(Y|X))=-\log{P(Y|X)}\)</span></p></li>
</ol>
<p><span>l*<span>3</span><span>c</span>r</span> Name &amp; Loss &amp; Derivative &amp; <span class="math inline">\(f^*\)</span> &amp; Algorithm<br />
Squared error &amp; <span class="math inline">\(\frac{1}{2}(y_i-f(\mathbf{x_i}))^2\)</span> &amp; <span class="math inline">\(y_i - f(\mathbf{x_i})\)</span> &amp; <span class="math inline">\(\mathbb{E}[y|\mathbf{x_i}]\)</span> &amp; L2Boosting<br />
Absolute error &amp; <span class="math inline">\(|y_i-f(\mathbf{x_i})|\)</span> &amp; <span class="math inline">\(sgn(y_i-f(\mathbf{x_i}))\)</span> &amp; <span class="math inline">\(median(y|\mathbf{x_i})\)</span> &amp; Gradient boosting<br />
Exponential loss &amp; <span class="math inline">\(exp(-\hat{y_i}f(\mathbf{x_i}))\)</span> &amp; <span class="math inline">\(-\hat{y_i}exp(-\hat{y_i}f(\mathbf{x_i}))\)</span> &amp; <span class="math inline">\(\frac{1}{2}log\frac{\pi_i}{1-\pi_i}\)</span> &amp; AdaBoost<br />
Logloss &amp; <span class="math inline">\(log(1+e^{-\hat{y_i}f_i})\)</span> &amp; <span class="math inline">\(y_i-\pi_i\)</span> &amp; <span class="math inline">\(\frac{1}{2}log\frac{\pi_i}{1-\pi_i•}\)</span> &amp; LogitBoost<br />
</p>
<p>The risk of function <span class="math inline">\(f\)</span> is defined as the expected loss of <span class="math inline">\(f\)</span>: <span class="math display">\[\label{eqn:expected-loss}
R_{\mathrm{exp}}(f)=E\left[L\left(Y,f(X)\right)\right]=\int L\left(y,f(x)\right)P(x,y)\mathrm{d}x\mathrm{d}y\]</span> which is also called expected loss or <strong>risk function</strong>.</p>
<p>The risk function <span class="math inline">\(R_{\mathrm{exp}}(f)\)</span> can be estimated from the training data as <span class="math display">\[R_{\mathrm{emp}}(f)=\dfrac{1}{N}\sum\limits_{i=1}^{N} L\left(y_i,f(x_i)\right)\]</span> which is also called empirical loss or <strong>empirical risk</strong>.</p>
<p>You can define your own loss function, but if you’re a novice, you’re probably better off using one from the literature. There are conditions that loss functions should meet<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a>:</p>
<ol>
<li><p>They should approximate the actual loss you’re trying to minimize. As was said in the other answer, the standard loss functions for classification is zero-one-loss (misclassification rate) and the ones used for training classifiers are approximations of that loss.</p></li>
<li><p>The loss function should work with your intended optimization algorithm. That’s why zero-one-loss is not used directly: it doesn’t work with gradient-based optimization methods since it doesn’t have a well-defined gradient (or even a subgradient, like the hinge loss for SVMs has).</p>
<p>The main algorithm that optimizes the zero-one-loss directly is the old perceptron algorithm(chapter §[chap:Perceptron]).</p></li>
</ol>
<h4 id="erm-and-srm">ERM and SRM</h4>
<p>ERM(Empirical risk minimization) <span class="math display">\[\min\limits _{f \in \mathcal{F}} R_{\mathrm{emp}}(f)=\min\limits _{f \in \mathcal{F}} \dfrac{1}{N}\sum\limits_{i=1}^{N} L\left(y_i,f(x_i)\right)\]</span></p>
<p>Structural risk <span class="math display">\[R_{\mathrm{smp}}(f)=\dfrac{1}{N}\sum\limits_{i=1}^{N} L\left(y_i,f(x_i)\right) +\lambda J(f)\]</span></p>
<p>SRM(Structural risk minimization) <span class="math display">\[\min\limits _{f \in \mathcal{F}} R_{\mathrm{srm}}(f)=\min\limits _{f \in \mathcal{F}} \dfrac{1}{N}\sum\limits_{i=1}^{N} L\left(y_i,f(x_i)\right) +\lambda J(f)\]</span></p>
<h3 id="optimization">Optimization</h3>
<p>Finally, we need a <strong>training algorithm</strong>(also called <strong>learning algorithm</strong>) to search among the classifiers in the the hypothesis space for the highest-scoring one. The choice of optimization technique is key to the <strong>efficiency</strong> of the model.</p>
<h2 id="model-selection">Model Selection</h2>
<p>If data is plentiful,then on approach to avoid over-fitting is to use some of the some of the available data to train a range of models,or a given model with a range of values for its complexity parameters,and then to compare them on independent data,sometimes called a <strong>Validation set</strong>,and select the one having the best predictive performance.If the model design is iterated many times using a limited size data set,then some over-fitting to the validation data can occur and so it may be necessary to keep aside a third <strong>test set</strong> on which the performance of the selected model is finally evaluated.</p>
<h4 id="sec:Cross-validation">Cross validation</h4>
<p><strong>Cross validation</strong>, sometimes called <em>rotation estimation</em>, is a <em>model validation</em> technique for assessing how the results of a statistical analysis will generalize to an independent data set<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a>.</p>
<p>Common types of cross-validation:</p>
<ol>
<li><p>K-fold cross-validation. In k-fold cross-validation, the original sample is randomly partitioned into k equal size subsamples. Of the k subsamples, a single subsample is retained as the validation data for testing the model, and the remaining k − 1 subsamples are used as training data.</p></li>
<li><p>2-fold cross-validation. Also, called simple cross-validation or holdout method. This is the simplest variation of k-fold cross-validation, k=2.</p></li>
<li><p>Leave-one-out cross-validation(<em>LOOCV</em>). k=M, the number of original samples.</p></li>
</ol>
<p>In many applications,however,the supply of data for training and testing will be limited.We wish to use as much of available data as possible for both training and validation set.One solution to this dilemma is to use <strong>cross-validations</strong>.One round of cross-validation involves partitioning a sample of data into complementary subsets, performing the analysis on one subset (called the training set), and validating the analysis on the other subset (called the validation set or testing set). To reduce variability, multiple rounds of cross-validation are performed using different partitions, and the validation results are averaged over the rounds.</p>
<h2 id="the-curse-of-dimensionality">The Curse of Dimensionality</h2>
<p>The common theme that arise when analyzing and organizing data in <strong>high-dimensional spaces</strong> is that when the <strong>demensionality increases</strong>,the <strong>volume of the space</strong> increases so fast (<strong>grows exponentially with the dimensionality</strong>) that the available data become sparse.</p>
<p>The volume of a sphere of radius <span class="math inline">\(r\)</span> in <span class="math inline">\(D\)</span> dimensions must scale as <span class="math inline">\(r^D\)</span>,and so we write <span class="math display">\[\begin{aligned}
V_D(r) = K_D r^D\end{aligned}\]</span> The fraction of the volume of the sphere that lies between radius <span class="math inline">\(r=1-\epsilon\)</span> and <span class="math inline">\(r=1\)</span> <span class="math display">\[\begin{aligned}
\dfrac{V_D(1)-V_D(1-\epsilon)}{V_D(1)}=1-(1-\epsilon)^D\end{aligned}\]</span> for large <span class="math inline">\(D\)</span> this fraction tends to <span class="math inline">\(1\)</span> even for small values of <span class="math inline">\(\epsilon\)</span>.Thus most of the volume of a sphere is concentrated in a thin shell near the surface!</p>
<h2 id="decision-theory">Decision Theory</h2>
<p>Probability theory provides us with a consistent mathematical framework for quantifying and manipulating uncertainty.And when combined with probability theory,decision theory allows us to make optimal decisions in situations involving uncertainty such as those encountered in pattern recognition.</p>
<p>Suppose we have an input vector x together with a corresponding vector t of target variables, and our goal is to predict t given a new value for x. For regression problems, t will comprise continuous variables, whereas for classification problems t will represent class labels. The joint probability distribution p(x, t) provides a complete summary of the uncertainty associated with these variables. Determination of p(x, t) from a set of training data is an example of <strong>inference</strong> and is typically a very difficult problem.</p>
<p>The <strong>decision</strong> step, is the subject of decision theory to tell us how to make optimal decisions given the appropriate probabilities. We shall see that the decision stage is generally very simple, even trivial, once we have solved the inference problem.</p>
<p>Consider,for a classification problem,in which case,the input are vectors <span class="math inline">\(\vec{x}\)</span> and we denote the class <span class="math inline">\(i\)</span> by <span class="math inline">\(\mathcal{C}_k\)</span>.Using Bayes’ theorem,these probabilities can be expressed in the form <span class="math display">\[p(\mathcal{C}_k|\vec{x}) = \dfrac{p(\vec{x}|\mathcal{C}_k)p(\mathcal{C}_k)}{p(\vec{x})}\]</span> Note that any of the quantities appearing in Bayes’ theorem can be obtained from the joint distribution <span class="math inline">\(p(\vec{x},\mathcal{C}_k)\)</span> by either <strong>marginalizing</strong> or <strong>conditioning</strong> with respect to the appropriate variables.We can now interpret <span class="math inline">\(p(\mathcal{C}_k)\)</span> as the prior probability for the class <span class="math inline">\(\mathcal{C}_k\)</span>,and the <span class="math inline">\(p(\mathcal{C}_k|\vec{x})\)</span> as the corresponding posterior probability,<span class="math inline">\(p(\vec{x}|\mathcal{C}_k)\)</span> is the likelihood function.If our aim it to minimize the chance of assigning <span class="math inline">\(\vec{x}\)</span> to the wrong class,then intuitively we would choose the class having the higher posterior probability.</p>
<h3 id="minimizing-the-misclassification-rate">Minimizing the misclassification rate</h3>
<h3 id="minimizing-the-expected-loss">Minimizing the expected loss</h3>
<h3 id="the-reject-option">The reject option</h3>
<h3 id="inference-and-decision">Inference and decision</h3>
<p>We have broken the classification problem down into two separate stages,the <strong>inference stage</strong> in which we use training data to learn to <strong>model</strong> for <span class="math inline">\(p(\mathcal{C}_k|\vec{x})\)</span>,and the subsequent <strong>decision stage</strong> in which we use these posterior probabilities to make optimal class assignments.An alternative possibility would be to solve both problems together and simply learn a function that maps input <span class="math inline">\(\vec{x}\)</span> directly into decisions.Such a function is called a <strong>discriminant function</strong>.</p>
<p>In fact,we can identify three distinct approaches to solving decision problems,given, in decreasing order of complexity,by</p>
<ol>
<li><p>First solve the inference problem of determining the class-conditional densities <span class="math inline">\(p(x|\mathcal{C}_k)\)</span> for each class <span class="math inline">\(\mathcal{C}_k\)</span> individually. Also separately infer the prior class probabilities <span class="math inline">\(p(\mathcal{C}_k)\)</span>. Then use <span class="math inline">\(Bayes’\)</span> theorem in the form <span class="math display">\[p(\mathcal{C}_k|\vec{x}) = \dfrac{p(x|\mathcal{C}_k)p(\mathcal{C}_k)}{p(\vec{x})}\]</span> to find the posterior class probabilities <span class="math inline">\(p(\mathcal{C}_k|\vec{x})\)</span>. As usual, the denominator in <span class="math inline">\(Bayes’\)</span> theorem can be found in terms of the quantities appearing in the numerator, because <span class="math display">\[p(\vec{x}) = \sum_k{p(\vec{x}|\mathcal{C}_k)p(\mathcal{C}_k)}\]</span> Equivalently, we can model the joint distribution <span class="math inline">\(p(x, \mathcal{C}_k)\)</span> directly and then normalize to obtain the posterior probabilities. Having found the posterior probabilities, we use decision theory to determine class membership for each new input x.Approaches that explicitly or implicitly model the <strong>distribution of inputs as well as outputs</strong> are known as <strong>Generative models</strong>,because by sampling from them it is possible to generate synthetic data points in the input space.</p></li>
<li><p>First solve the inference problem of determining the posterior class probabilities <span class="math inline">\(p(\mathcal{C}_k|\vec{x})\)</span>,and then subsequently use decision theory to assign each new <span class="math inline">\(\vec{x}\)</span> to one of the classes.Approaches that model the posterior probabilities are called <strong>discriminative models</strong></p></li>
<li><p>Find a function <span class="math inline">\(f(\vec{x})\)</span>,called a discriminant function,which maps each input <span class="math inline">\(\vec{x}\)</span> directly into a class label.For instance,in the case of two-case problems,<span class="math inline">\(f(\cdot)\)</span> might be binary valued and such that <span class="math inline">\(f=0\)</span> represents class <span class="math inline">\(\mathcal{C}_1\)</span> and so on.</p></li>
</ol>
<p>Now consider the relative merits of there three alternatives.Approach (a) is the most demanding because it involves finding the joint distribution over both x and Ck. For many applications, x will have high dimensionality, and consequently we may need a large training set in order to be able to determine the class-conditional densities to reasonable accuracy. Note that the class <strong>priors</strong> <span class="math inline">\(p(\mathcal{C_k})\)</span> can often be estimated simply from the <strong>fractions of the training set data points in each of the classes</strong>. One advantage of approach (a), however, is that it also allows the marginal density of data p(x) to be determined from (1.83). This can be useful for detecting new data points that have low probability under the model and for which the predictions may be of low accuracy, which is known as <strong>outlier detection or novelty detection</strong> (Bishop, 1994; Tarassenko, 1995).</p>
<p>However, if we only wish to make classification decisions, then it can be wasteful of computational resources, and excessively demanding of data, to find the joint distribution p(x, Ck) when in fact we only really need the posterior probabilities p(Ck|x), which can be obtained directly through approach (b). Indeed, the class conditional densities may contain a lot of structure that has little effect on the posterior probabilities, as illustrated in Figure 1.27. There has been much interest in exploring the relative merits of generative and discriminative approaches to machine learning, and in finding ways to combine them (Jebara, 2004; Lasserre et al., 2006). An even simpler approach is (c) in which we use the training data to find a discriminant function f(x) that maps each x directly onto a class label, thereby combining the inference and decision stages into a single learning problem. In the example of Figure 1.27, this would correspond to finding the value of x shown by the vertical green line, because this is the decision boundary giving the minimum probability of misclassification.</p>
<p>Reasons for wanting to compute the posterior probabilities include:</p>
<ul>
<li><p><strong>Minimize risk.</strong></p></li>
<li><p><strong>Reject option.</strong></p></li>
<li><p><strong>Compensating for class priors.</strong></p></li>
<li><p><strong>Combining models.</strong> For complex applications,we may wish to break the problem into a number of smaller subproblems each of which can be tackled by a separate module.As long as each of the two models gives posterior probabilities for the classes, we can combine the outputs systematically using the rules of probability. One simple way to do this is to assume that, for each class separately, the distributions of inputs are independent,so that <span class="math display">\[p(\vec{x_I},\vec{x_B}|\mathcal{C}_k)  = p(\vec{x_I}|\mathcal{C}_k)p(\vec{x_B}|\mathcal{C}_k)\]</span> This is an example of <strong>conditional independence</strong> property.The posterior probability,given both data(from different modules) ,is then given by <span class="math display">\[\begin{aligned}
    p(\mathcal{C}_k|\vec{x_I},\vec{x_B}) &amp;\propto p(\vec{x_I},\vec{x_B}|\mathcal{C}_k)p(\mathcal{C}_k) \\
    &amp;\propto p(\vec{x_I}|\mathcal{C}_k)p(\vec{x_B}|\mathcal{C}_k)p(\mathcal{C}_k) \\
    &amp;\propto \dfrac{p(\mathcal{C}_k|\vec{x_I})p(\mathcal{C}_k|\vec{x_B})}{p(\mathcal{C}_k)}
    \end{aligned}\]</span></p></li>
</ul>
<h2 id="information-theory">Information theory</h2>
<p>Consider a discrete random variable x and we ask how much information is received when we observe a specific value for this variable.Our measure of information content will therefore depend on the probability distribution <span class="math inline">\(p(x)\)</span>,and we therefore look for a quantity <span class="math inline">\(h(x)\)</span> that is monotonic function of the probability <span class="math inline">\(p(x)\)</span> and that expresses the information content.The form of <span class="math inline">\(h(\cdot)\)</span> can be found by noting that if we have two events <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> that are unrelated,then the information gain from observing both of them should be the sum of the information gained from each of them separately,so that <span class="math inline">\(h(x,y) = h(x)+ h(y)\)</span>.Two unrelated events will be statistically independent and so <span class="math inline">\(p(x,y) = p(x)p(y)\)</span>.From these two relationships,it is easily shown that <span class="math inline">\(h(x)\)</span> must be given by the logarithm of <span class="math inline">\(p(x)\)</span> and so we have <span class="math display">\[\label{eqn:information content}
h(x) = -\log_2p(x)\]</span> where the negative sign ensures that information is positive or zero.Note that low probability events x correspond to high information content.The choice of basis for the logarithm is arbitrary.</p>
<p>Now suppose that a sender wishes to transmit the value of a random variable to a receiver.The average amount of information that they transmit in the process is obtained by taking the expectation of [eqn:information content] with respect to the distribution <span class="math inline">\(p(x)\)</span> and is given by <span class="math display">\[H[x] = -\sum\limits_{x}p(x)\log_2 p(x)\]</span> This important quantity is called the <strong>entropy</strong> of the random variable x.</p>
<p>The <strong>noiseless coding theorem</strong> (Shannon,1948) states that the entropy is a lower bound on the number of bits needed to transmit the state of a random variable.</p>
<p>For example ,we can represente the states <span class="math inline">\(x={a, b, c, d, e, f, g, h}\)</span> using, for instance, the following set of code strings: 0, 10, 110, 1110, 111100, 111101, 111110, 111111. The average length of the code that has to be transmitted is then <span class="math display">\[\text{average code length} = \dfrac{1}{2} \times 1 + \dfrac{1}{4} \times 2+\dfrac{1}{8} \times 3 + \dfrac{1}{16} \times 4 + 4\times \dfrac{1}{64}\times 6 = 2 \text{ bits}\]</span> Now,we switch to the use of natural logarithm in defining entropy.And entropy can be understood in an alternative view of <strong>multiplicity</strong>. The continuous form of the entropy is given by <span class="math display">\[\lim\limits_{\Delta\rightarrow 0}\{\sum\limits_{i}p(x_i)\Delta \ln p(x_i) \} = -\int p(x)\ln p(x)dx\]</span> where the quantity on the right-side is called the <strong>differential entropy</strong>.</p>
<p>For a density defined over multiple continuous variables,denoted collectively by the vector <span class="math inline">\(\vec{x}\)</span>,the differential entropy is given by <span class="math display">\[H[x] = -\int p(\vec{x})lnp(\vec{x})d\vec{x}\]</span></p>
<p>In the case of discrete distributions,we see that the maximum entropy configuration correspondes to an equal distribution of probabilities across the possible states of variable.Now consider the maximum entropy configuration for a continuous variable.It will be necessary to constrain the first and second moments of <span class="math inline">\(p(x)\)</span> as well as preserving the normalization constraint.Therefore we need to maximize the differential entropy <span class="math inline">\(H[x] = -\int p(\vec{x}) \ln p(\vec{x})d\vec{x}\)</span> with three constraints <span class="math display">\[\begin{aligned}
\int_{-\infty}^{\infty}p(x)dx &amp;= 1 \\
\int_{-\infty}^{\infty}xp(x)dx &amp;= \mu \\
\int_{-\infty}^{\infty}(x-\mu)^2p(x)dx &amp;= \sigma^2.\end{aligned}\]</span> The constrained maximization can be performed using Lagrange multipliers so that we maximize the following functional with respect to <span class="math inline">\(p(x)\)</span> <span class="math display">\[-\int_{-\infty}^{\infty}p(x)\ln p(x)dx + \lambda_1(\int_{-\infty}^{\infty}p(x)dx-1) + \lambda_2(\int_{-\infty}^{\infty}xp(x)dx-\mu) + \lambda_3(\int_{-\infty}^{\infty}(x-\mu)^2p(x)dx - \sigma^2)\]</span> Using the calculus of variations,we set the derivative of this functional to zero giving <span class="math display">\[p(x) = exp\{-1+\lambda_1+\lambda_2 x+\lambda_3 (x-\mu)^2\}\]</span> The Lagrange multipliers can be found by back substitution of this result into the three constraint equations,leading finally to the result <span class="math display">\[p(x) = \dfrac{1}{(2\pi \sigma^2)^(1/2)}exp\{-\dfrac{(x-\mu)^2}{2\sigma ^2}\}\]</span> and so the distribution that maximizes the differential entropy is the Gaussian.</p>
<p>If we evaluate the differential entropy of the Gaussian,we obtain <span class="math display">\[H[x] = \dfrac{1}{2}\{1+\ln (2\pi \sigma^2)\}\]</span> Thus we see again that the entropy increases as the distribution becomes broader,i.e.,as <span class="math inline">\(\sigma^2\)</span> increases.</p>
<p>Suppose we have joint distribution <span class="math inline">\(p(\vec{x},\vec{y})\)</span> from which we draw pairs of values of <span class="math inline">\(\vec{x}\)</span> and <span class="math inline">\(\vec{y}\)</span>.If a value of <span class="math inline">\(\vec{x}\)</span> is already known,then the additional information needed to specify the corresponding value of <span class="math inline">\(\vec{y}\)</span> is given by <span class="math inline">\(-\ln p(\vec{y}|\vec{x})\)</span>,thus the average additional information needed can be written as <span class="math display">\[H[\vec{y}|\vec{x}] = -\iint p(\vec{y},\vec{x})\ln p(\vec{y}|\vec{x})d\vec{y}d\vec{x}\]</span> which is called the <strong>conditional entropy</strong> of <span class="math inline">\(\vec{y}\)</span> given <span class="math inline">\(\vec{x}\)</span>.Using the product rule,the conditional entropy satisfies the relation <span class="math display">\[H[\vec{x},\vec{y}] = H[\vec{y}|\vec{x}] + H[\vec{x}]\]</span> where <span class="math inline">\(H[\vec{x},\vec{y}]\)</span> is the differential entropy of <span class="math inline">\(p(\vec{x},\vec{y})\)</span> and <span class="math inline">\(H[\vec{x}]\)</span> is the differential entropy of the marginal distribution <span class="math inline">\(p(\vec{x})\)</span>.</p>
<p><span class="math display">\[\begin{aligned}
    H[\vec{x},\vec{y}] - H[\vec{y}|\vec{x}] &amp;=  -\iint p(\vec{y},\vec{x}) \ln p(\vec{y},\vec{x})d\vec{y}d\vec{x} + \iint p(\vec{y},\vec{x})\ln p(\vec{y}|\vec{x})d\vec{y}d\vec{x} \\
    &amp; = \iint p(\vec{y},\vec{x}) \ln \dfrac{p(\vec{y}|\vec{x})}{p(\vec{y},\vec{x})} \\
    &amp; = \iint p(\vec{y},\vec{x}) \ln \dfrac{1}{p(\vec{x})}d\vec{y}d\vec{x} \\
    &amp; = -\iint p(\vec{y},\vec{x}) \ln {p(\vec{x})}d\vec{y}d\vec{x} \\
    &amp; = -\iint p(\vec{x}) \ln {p(\vec{x})}d\vec{x} \\
    &amp; = H[\vec{x}]
    \end{aligned}\]</span></p>
<h3 id="relative-entropy-and-mutual-information">Relative entropy and mutual information</h3>
<p>Consider some unknown distribution <span class="math inline">\(p(\vec{x})\)</span>,and suppose that we have modelled this using an approximating distribution <span class="math inline">\(q\vec{x})\)</span>.Then the average <strong>additional</strong> amount of information(in nats) required to specify the value of <span class="math inline">\(\vec{x}\)</span> as a result of using <span class="math inline">\(q(\vec{x})\)</span> instead of the true distribution is given by <span class="math display">\[\begin{aligned}
KL(q\parallel p) &amp;= -\int p(\vec{x})\ln q(\vec{x}) d\vec{x} - (- \int p(\vec{x}) \ln p(\vec{x})d\vec{x}) \\
&amp; = -\int p(\vec{x})\ln \{\dfrac{q\vec{x}}{p\vec{x}}\}d\vec{x}\end{aligned}\]</span> This is known as the <strong>relative entropy</strong> or <strong>Kullback-Leibler divergence</strong>,or <strong>KL divergence</strong> (Kullback and Leibler,1951),between the distributions <span class="math inline">\(p(\vec{x})\)</span> and <span class="math inline">\(q(\vec{x})\)</span>.Note that it is not a symmetrical quantity. Now introduce the concept of <strong>convex</strong> functions .A function <span class="math inline">\(f(x)\)</span> is said to be convex if it has the property that every chord lies on or above the function, as shown in Figure 1.31. Any value of x in the interval from x = a to x = b can be written in the form λa + (1 − λ)b where <span class="math inline">\(0 \leq λ \leq 1\)</span>. The corresponding point on the chord is given by λf(a) + (1 − λ)f(b),and the corresponding value of the function is f(λa + (1 − λ)b).Convexity then implies <span class="math display">\[f(\lambda a + (1-\lambda)b) \leq \lambda f(a) + (1-\lambda) f(b)\]</span> This is equivalent to the requirement that the second derivative of the function be everywhere positive. A function is called strictly convex if the equality is satisfied only for <span class="math inline">\(λ\lambda = 0\)</span> and <span class="math inline">\(λ = 1\)</span>. If a function has the opposite property, namely that every chord lies on or below the function, it is called concave, with a corresponding definition for strictly concave. If a function <span class="math inline">\(f(x)\)</span> is convex, then <span class="math inline">\(−f(x)\)</span> will be concave.</p>
<p>Using the technique of <strong>proof by induction</strong>,we can show that a convex function <span class="math inline">\(f(x)\)</span> satisfies <span class="math display">\[\label{ineqn:Jensen&#39;s inequality}
f(\sum_{i=1}^{M}\lambda_i x_i) \leq \sum_{i=1}^{M}\lambda_i f(x_i)\]</span> where <span class="math inline">\(\lambda_i \geq 0\)</span> and <span class="math inline">\(\sum_{i}\lambda_i = 1\)</span>,for any set of points <span class="math inline">\(\{x_i\}\)</span>,this is known as <strong>Jensen’s inequality</strong>.If we interpret the <span class="math inline">\(\lambda_i\)</span> as the probability distribution over a discrete variable <span class="math inline">\(x\)</span> taking the values <span class="math inline">\(\{x_i\}\)</span>,then [ineqn:Jensen’s inequality] can be written <span class="math display">\[f(\mathbb{E}[x]) \leq \mathbb{E}[f(x)]\]</span> where <span class="math inline">\(\mathbb{E}(\cdot)\)</span> denotes the expectation.For continuous variables,Jensen’s inequality takes the form <span class="math display">\[f(\int \vec{x}p(\vec{x})) \leq \int f(\vec{x})p(\vec{x})d\vec{x}\]</span> We can apply Jensen’s inequality to the KL-divergence to give <span class="math display">\[KL(p\parallel q) = -\int p(\vec{x}) \ln\{\dfrac{q(\vec{x})}{p(\vec{x})}\}d\vec{x} \geq -\ln \int q(\vec{x})d\vec{x} = 0\]</span> where we have used the fact that <span class="math inline">\(-\ln x\)</span> is a convex function,together with the normalization condition <span class="math inline">\(\int q(\vec{x})d\vec{x} = 1\)</span>.</p>
<p>[log sum inequality] Let <span class="math inline">\(a_1,...,a_n\)</span> and <span class="math inline">\(b_1,...,b_n\)</span> be nonnegative numbers.Denote the sum of all <span class="math inline">\(a_i\)</span> by <span class="math inline">\(a\)</span> and sum of all <span class="math inline">\(b_i\)</span> by <span class="math inline">\(b\)</span>.The log sum inequality states that <span class="math display">\[\begin{aligned}
    \sum_{i=1}^n a_i\log\frac{a_i}{b_i}\geq a\log\frac{a}{b},
    \end{aligned}\]</span></p>
<p>Setting <span class="math inline">\(f(x) = x\log x\)</span>,we have <span class="math display">\[\begin{aligned}
        \sum_{i=1}^n a_i\log\frac{a_i}{b_i} &amp; {} = \sum_{i=1}^n b_i f\left(\frac{a_i}{b_i}\right)
        = b\sum_{i=1}^n \frac{b_i}{b} f\left(\frac{a_i}{b_i}\right) \\
        &amp; {} \geq b f\left(\sum_{i=1}^n \frac{b_i}{b}\frac{a_i}{b_i}\right) = b f\left(\frac{1}{b}\sum_{i=1}^n a_i\right)
        = b f\left(\frac{a}{b}\right) \\
        &amp; {} = a\log\frac{a}{b},
        \end{aligned}\]</span> where the inequality follows Jensen’s inequality[ineqn:Jensen’s inequality] since <span class="math inline">\(f\)</span> is convex. So we have <span class="math display">\[\begin{aligned}
D_{\mathrm{KL}}(P\|Q) \equiv \sum_{i=1}^n p_i \log_2 \frac{p_i}{q_i} \geq 1\log\frac{1}{1} = 0.\end{aligned}\]</span></p>
<p>Suppose we try to approximate this distribution using some parametric distribution <span class="math inline">\(q(\vec{x}|\vec{\theta})\)</span>.Then the expectation of KL-divergence with respect to <span class="math inline">\(p(\vec{x})\)</span> can be approximated by a finite sum over these training points,so that <span class="math display">\[KL(p\parallel q) \simeq \sum_{n=1}^{N}\{-\ln q(\vec{x_n}|\vec{\theta}) + \ln p(\vec{x_n})\}\]</span> The second term on the right-hand side is independent of <span class="math inline">\(\theta\)</span>,and the first term is the negative log likelihood function for <span class="math inline">\(\theta\)</span> under the distribution <span class="math inline">\(q(\vec{x}|\vec{\theta})\)</span> evaluated using the training set.Thus <strong>minimizing this KL-divergence is equivalent to maximizing the likelihood function</strong>.</p>
<p>If ,for a joint distribution,the variables <span class="math inline">\(\vec{x}\)</span> and <span class="math inline">\(\vec{y}\)</span> are not independent,we can gain some idea of whether they are ’close’ to being independent by considering the KL-divergence between the joint distribution and the product of the marginals,given by <span class="math display">\[\begin{aligned}
I[\vec{x},\vec{y}] &amp;\equiv KL(p(\vec{x},\vec{y}\parallel p(\vec{x})p(\vec{y})) \\
&amp; = -\iint p(\vec{x},\vec{y})\ln (\dfrac{p(\vec{x})p(\vec{y})}{p(\vec{x},\vec{y})})d\vec{x}d\vec{y}\end{aligned}\]</span> which is called the <strong>mutual information</strong> between the variables <span class="math inline">\(\vec{x}\)</span> and <span class="math inline">\(\vec{y}\)</span>.From the property of the KL-divergence,we see that <span class="math inline">\(I(\vec{x},\vec{y}) \geq 0\)</span> with equality if,and only if <span class="math inline">\(\vec{x}\)</span> and <span class="math inline">\(\vec{y}\)</span> are independent.Using the sum and product rules of probability,we see the relationship to conditional entropy through <span class="math display">\[I[\vec{x},\vec{y}] = H[\vec{x}] - H[\vec{x}|\vec{y}] = H[\vec{y}] - H[\vec{y}|\vec{x}]\]</span> Thus we can view the mutual information as the reduction in the uncertainty about <span class="math inline">\(\vec{x}\)</span> by virtue of being told the value of <span class="math inline">\(\vec{y}\)</span> (or vice versa).</p>
<h2 id="some-basic-concepts">Some basic concepts</h2>
<h3 id="parametric-vs-non-parametric-models">Parametric vs non-parametric models</h3>
<h3 id="a-simple-non-parametric-classifier-k-nearest-neighbours">A simple non-parametric classifier: K-nearest neighbours</h3>
<h4 id="representation-1">Representation</h4>
<p><span class="math display">\[y=f(\vec{x})=\arg\min_{c}{\sum\limits_{\vec{x}_i \in N_k(\vec{x})} \mathbb{I}(y_i=c)}\]</span></p>
<p>where <span class="math inline">\(N_k(\vec{x})\)</span> is the set of k points that are closest to point <span class="math inline">\(\vec{x}\)</span>.</p>
<p>Usually use <strong>k-d tree</strong> to accelerate the process of finding k nearest points.</p>
<h4 id="evaluation-1">Evaluation</h4>
<p>No training is needed.</p>
<h4 id="optimization-1">Optimization</h4>
<p>No training is needed.</p>
<h3 id="overfitting">Overfitting</h3>
<h3 id="model-selection-1">Model selection</h3>
<p>When we have a variety of models of different complexity (e.g., linear or logistic regression models with different degree polynomials, or KNN classifiers with different values ofK), how should we pick the right one? A natural approach is to compute the <strong>misclassification rate</strong> on the training set for each method.</p>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Domingos, P. A few useful things to know about machine learning. Commun. ACM. 55(10):78–87 (2012).<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p><a href="http://t.cn/zTrDxLO" class="uri">http://t.cn/zTrDxLO</a><a href="#fnref2">↩</a></p></li>
<li id="fn3"><p><a href="http://en.wikipedia.org/wiki/Cross-validation_(statistics)" class="uri">http://en.wikipedia.org/wiki/Cross-validation_(statistics)</a><a href="#fnref3">↩</a></p></li>
</ol>
</div>
</body>
</html>
