<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>
<h1 id="chap:Continuous Latent Variables">Continuous Latent Variables</h1>
<h2 id="principal-component-analysis">Principal Component Analysis</h2>
<h3 id="introduction">Introduction</h3>
<p>Principal Component Analysis is widely used for applications such as dimensionality reduction,lossy data compression,feature extraction,and data visualization.Also known as the Karhunen-Loeve transform.There are two definitions giving rise to the same algorithm.PCA can be defined as the orthogonal projection of the data onto a lower dimensional linear space,known as the principal subspace,such that the variance of the projected data is maximized.Equivalently,it can be defined as the linear projection the minimizes the average projection cost, defined as the linear projection that minimizes the average projection cost,defined as the mean squared distance between the data points and their projections.</p>
<h3 id="maximum-variance-formulation">Maximum variance formulation</h3>
<p>Consider a data set of observations <span class="math inline">\(\{x_n\}\)</span> where <span class="math inline">\(n = 1,...,N\)</span>,and <span class="math inline">\(x_n\)</span> is a Euclidean variable with dimensionality D. Our goal is to project the data onto a space having dimensionality <span class="math inline">\(M &lt; D\)</span> while maximizing the variance of the projected data.We define the direction of this space using a D-dimensional unit vector <span class="math inline">\(\mathbf{u_1^T}\mathbf{u_1} = 1\)</span>.Each data point <span class="math inline">\(\mathbf{x_n}\)</span> is then projected onto a scalar value <span class="math inline">\(\mathbf{u_1^T}\mathbf{x_n}\)</span>.The mean of the projected data is <span class="math inline">\(\mathbf{u_1^T}\bar{\mathbf{x}}\)</span> where the <span class="math inline">\(\bar{\mathbf{x}}\)</span> is the sample set mean given by <span class="math display">\[\begin{aligned}
\bar{\mathbf{x}} = \frac{1}{N}\sum_{n=1}^{N}{\mathbf{x_n}}\end{aligned}\]</span> and the variance of the projected data is given by <span class="math display">\[\begin{aligned}
\frac{1}{N}\sum_{n=1}^{N}\{\mathbf{u_1^T}\mathbf{x_n} - \mathbf{u_1^T}\bar{\mathbf{x}}\}^2 
&amp;= \frac{1}{N}\sum_{n=1}^{N}{\{\mathbf{u_1^T}(\mathbf{x_n} - \bar{\mathbf{x}})\}^2} \\
&amp;= \frac{1}{N}\sum_{n=1}^{N}{\{\mathbf{u_1^T(\mathbf{x_n - \bar{\mathbf{x}}})(\mathbf{x_n -\bar{x}})^T\mathbf{u_1^T} }  \}} \\
&amp;= \mathbf{u_1^T}\mathbf{S}\mathbf{u_1}\end{aligned}\]</span> where <span class="math inline">\(\mathbf{S}\)</span> is the data covariance matrix defined by <span class="math display">\[\begin{aligned}
\mathbf{S} = \frac{1}{N}\sum_{n=1}^{N}(\mathbf{x_n}-\bar{\mathbf{x}})(\mathbf{x_n}-\mathbf{\bar{x}})^T\end{aligned}\]</span> We now maximize the projected variance <span class="math inline">\(\mathbf{u_1^T}\mathbf{S}\mathbf{u_1}\)</span> with respect to <span class="math inline">\(\mathbf{u_1}\)</span>,which is a constrained maximization to prevent <span class="math inline">\(\parallel\mathbf{u_1}\parallel\rightarrow \infty\)</span> .The appropriate constraint comes from the normalization condition <span class="math inline">\(\mathbf{u_1^T}\mathbf{u_1}=1\)</span>.To enforce this constraint,we introduce a Lagrange multiplier that we shall denote by <span class="math inline">\(\lambda_1\)</span>,and then make an unconstrained maximization of <span class="math display">\[\mathbf{u_1^T}\mathbf{S}\mathbf{u_1} + \lambda_1(1-\mathbf{u_1^T}\mathbf{u_1})\]</span> By setting the derivative with respect to <span class="math inline">\(\mathbf{u_1}\)</span> equal to zero,we see that this quantity will have a stationary point when <span class="math display">\[\mathbf{S}\mathbf{u_1} = \lambda_1\mathbf{u_1}\]</span> which says that <span class="math inline">\(\mathbf{u_1}\)</span> must be an eigenvector of <span class="math inline">\(\mathbf{S}\)</span>.If we left-multiply by <span class="math inline">\(\mathbf{u_1^T}\)</span> and make use of <span class="math inline">\(\mathbf{u_1^T}{u_1} = 1\)</span>,we see that the variance is given by <span class="math display">\[\mathbf{u_1^TSu_1} = \lambda_1\]</span> and so the variance will be a maximum when we set <span class="math inline">\(\mathbf{u_1}\)</span> equal to the eigenvector having the largest eigenvalue <span class="math inline">\(\lambda_1\)</span>.This eigenvector is known as the first principal component.</p>
<h3 id="minimum-error-formulation">Minimum-error formulation</h3>
<h3 id="applications-of-pca">Applications of PCA</h3>
<h3 id="pca-for-high-dimensional-data">PCA for high-dimensional data</h3>
<h2 id="probabilistic-pca">Probabilistic PCA</h2>
<h2 id="kernel-pca">Kernel PCA</h2>
<h2 id="nonlinear-latent-variable-models">Nonlinear Latent Variable Models</h2>
</body>
</html>
