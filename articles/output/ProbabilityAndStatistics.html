<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>
<h1 id="chap:Probability and Statistics">Probability and Statistics</h1>
<p>One role for the distributions is to model the probability distribution <span class="math inline">\(p(\vec{x})\)</span> of a random variable <span class="math inline">\(x\)</span>,given a finite set of observations.This problem is known as <strong>density estimation</strong>.We shall assume that the data points are independent and identically distributed.</p>
<p><strong>Parameter</strong> distributions are governed by a small number of adaptive parameters,such as the mean and variance in the case of a Gaussian for example.In a frequentist treatment, we choose specific values for the parameters by optimizing some criterion, such as the likelihood function. By contrast, in a Bayesian treatment we introduce prior distributions over the parameters and then use Bayes’ theorem to compute the corresponding posterior distribution given the observed data.</p>
<p>An important role is played by <strong>conjugate priors</strong>,that lead to posterior distributions having the same functional form as the prior,and that therefore lead to a greatly simplified Bayesian analytics.</p>
<p><strong>Nonparametric</strong> density estimation methods in which the form of the distribution typically depends on the size of the data set.Parameters in such models control the model complexity rather than the form of the distribution.We cover three nonparametric methods based respectively on histograms,nearest-neighbours,and kernels.</p>
<h2 id="frequentists-vs.-bayesians">Frequentists vs. Bayesians</h2>
<p>There are two different interpretations of probability. One is called the <strong>frequentist</strong> interpretation. In this view, probabilities represent long run <strong>frequencies</strong> of events. For example, the above statement means that, if we flip the coin many times, we expect it to land heads about half the time.</p>
<p>The other interpretation is called the <strong>Bayesian</strong> interpretation of probability. In this view, probability is used to quantify our <strong>uncertainty</strong> about something; hence it is fundamentally related to information rather than repeated trials (Jaynes 2003). In the Bayesian view, the above statement means we believe the coin is equally likely to land heads or tails on the next toss</p>
<p>One big advantage of the Bayesian interpretation is that it can be used to model our uncertainty about events that do not have long term frequencies. For example, we might want to compute the probability that the polar ice cap will melt by 2020 CE. This event will happen zero or one times, but cannot happen repeatedly. Nevertheless,we thought to be able to quantify our uncertainty about this event. To give another machine learning oriented example, we might have observed a “blip” on our radar screen, and want to compute the probability distribution over the location of the corresponding target (be it a bird, plane, or missile). In all these cases, the idea of repeated trials does not make sense, but the Bayesian interpretation is valid and indeed quite natural. We shall therefore adopt the Bayesian interpretation in this book. Fortunately, the basic rules of probability theory are the same, no matter which interpretation is adopted.</p>
<h2 id="probability-theory">probability theory</h2>
<h3 id="concepts">Concepts</h3>
<p>The expression <span class="math inline">\(p(A)\)</span> denotes the probability that event A is true.We require that <span class="math inline">\(0\leq p(A) \leq 1\)</span>,where 0 means the event definitely will not happen,and <span class="math inline">\(p(A)=1\)</span> means the event definitely will happen.<span class="math inline">\(p(\hat{A})\)</span> denotes the probability of the event not A;this is defined to be <span class="math inline">\(p(\hat{A})=1-p(A)\)</span>.</p>
<p>We denote a random event by defining a <strong>random variable</strong> <span class="math inline">\(X\)</span>.</p>
<p><strong>Descrete random variable</strong>: <span class="math inline">\(X\)</span> ,which can take on any value from a finite or countably infinite set .We denote the probability of the event that <span class="math inline">\(X=x\)</span> by <span class="math inline">\(p(X=x)\)</span>,or just <span class="math inline">\(p(x)\)</span> for short.Here <span class="math inline">\(p()\)</span> is called a <strong>probability mass function</strong> or <strong>pmf</strong>.The pmfs are defined one <strong>state space</strong>.<span class="math inline">\(\mathbb{I}\)</span> denotes the binary <strong>indicator funcition</strong>.</p>
<p><strong>Continuous random variable</strong>: the value of <span class="math inline">\(X\)</span> is real-valued.</p>
<dl>
<dt><strong>probability</strong></dt>
<dd><p>Probability is the measure of the likeliness that an event will occur.</p>
</dd>
<dt><strong>conditional probability</strong></dt>
<dd><p>A conditional probability measures the probability of an event given that (by assumption, presumption, assertion or evidence) another event has occurred.</p>
</dd>
<dt><strong>joint probability</strong></dt>
<dd><p>Joint probability is a measure of two events happening at the same time, and can only be applied to situations where more than one observation can be occurred at the same time.</p>
</dd>
<dt><strong>prior probability distribution</strong></dt>
<dd><p>In Bayesian statistical inference, a prior probability distribution, often called simply the prior, of an uncertain quantity p is the probability distribution that would express one’s uncertainty about p <strong>before</strong> some evidence is taken into account.</p>
</dd>
<dt><strong>posterior probability distribution</strong></dt>
<dd><p>In Bayesian statistics, the posterior probability of a random event or an uncertain proposition is the <strong>conditional probability</strong> that is assigned <strong>after</strong> the relevant evidence or background is taken into account.</p>
</dd>
<dt><strong>likelihood function</strong></dt>
<dd><p>In statistics, a likelihood function (often simply the likelihood) is a function of the parameters of a statistical model.The likelihood of a set of parameter values, θ, given outcomes x, is equal to the <strong>probability</strong> or <strong>probability desity</strong> of those observed outcomes given those parameter values.</p>
</dd>
</dl>
<h3 id="fundamental-rules">Fundamental rules</h3>
<p>In this section,we review the basic rule of probability.</p>
<h4 id="probability-of-a-union-of-two-events">Probability of a union of two events</h4>
<p>Given two events,<span class="math inline">\(A\)</span> and B,we define the probability of A or B as follows: <span class="math display">\[\begin{aligned}
p(A\cup B)&amp; = p(A) + p(B) - p(A\cap B)  \\
          &amp; = p(A) + p(B)\end{aligned}\]</span> if A and B are mutually independent</p>
<h4 id="joint-probabilities">Joint probabilities</h4>
<p>We define the probability of the joint event A and B as follows: <span class="math display">\[p(A,B) = p(A\cap B) = p(A|B)p(B)\]</span> This is sometimes called the <strong>product rule</strong></p>
<h4 id="conditional-probability">Conditional probability</h4>
<p>Define the <strong>conditional probability</strong> of event A,given that event B is true,as follows: <span class="math display">\[p(A|B)= \frac{p(A,B)}{p(B)},if p(B) &gt; 0\]</span></p>
<h4 id="sum-rule">sum rule</h4>
<p><span class="math display">\[p(X) = \sum_{Y}p(X,Y)\]</span></p>
<h4 id="product-rule">product rule</h4>
<p><span class="math display">\[\begin{aligned}
p(X,Y) &amp;= p(Y|X)p(X) \\
p(A,B|C) &amp;= p(A|C)p(B|AC) = p(B|C)p(A|BC)\end{aligned}\]</span></p>
<p><span class="math inline">\(p(X,Y) = p(Y|X)p(X)\)</span> is by definition. <span class="math display">\[\begin{aligned}
        p(A,B|C) &amp;= \dfrac{p(A,B,C)}{p(C)} \\
        &amp;=\dfrac{p(A,C)p(B|A,C)}{p(C)}\\
        &amp;=p(A|C)p(B|A,C)
    \end{aligned}\]</span> <span class="math display">\[\begin{aligned}
        p(C|A,B) &amp;= \dfrac{p(A,B,C)}{p(A,B)} \\
        &amp;=\dfrac{\dfrac{p(A,B,C)}{p(B)}}{\dfrac{p(A,B)}{p(B)}}\\
        &amp;=\dfrac{p(A,C|B)}{p(A|B)}
        \end{aligned}\]</span></p>
<h4 id="cdf">CDF</h4>
<p><span class="math display">\[F(x) \triangleq P(X \leq x)=\begin{cases}
\sum_{u \leq x}p(u) &amp; \text{, discrete}\\
\int_{-\infty}^{x} f(u)\mathrm{d}u &amp; \text{, continuous}\\
\end{cases}\]</span></p>
<h4 id="pmf-and-pdf">PMF and PDF</h4>
<p>For descrete random variable, We denote the probability of the event that <span class="math inline">\(X=x\)</span> by <span class="math inline">\(P(X=x)\)</span>, or just <span class="math inline">\(p(x)\)</span> for short. Here <span class="math inline">\(p(x)\)</span> is called a <strong>probability mass function</strong> or <strong>PMF</strong>.A probability mass function is a function that gives the probability that a discrete random variable is exactly equal to some value<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>. This satisfies the properties <span class="math inline">\(0 \leq p(x) \leq 1\)</span> and <span class="math inline">\(\sum_{x \in \mathcal{X}} p(x)=1\)</span>.</p>
<p>For continuous variable, in the equation <span class="math inline">\(F(x)=\int_{-\infty}^{x} f(u)\mathrm{d}u\)</span>, the function <span class="math inline">\(f(x)\)</span> is called a <strong>probability density function</strong> or <strong>PDF</strong>. A probability density function is a function that describes the relative likelihood for this random variable to take on a given value<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a>.This satisfies the properties <span class="math inline">\(f(x) \geq 0\)</span> and <span class="math inline">\(\int_{-\infty}^{\infty} f(x)\mathrm{d}x=1\)</span>.</p>
<h4 id="probability-densities">probability densities</h4>
<p>probability densities <span class="math display">\[p(x\in (a,b)) = \int_{a}^{b}p(x)dx\]</span> The probability density function p(x) must satisfy the two conditions <span class="math display">\[\begin{cases}
p(x) \geq 0               \\
\int_{-\infty}^{\infty}p(x)dx = 1
\end{cases}\]</span></p>
<p>Combinations of discrete and continuous variables. <span class="math display">\[p(x) = \int_p(x,y)dy\]</span> <span class="math display">\[p(x,y) = p(y|x)p(x)\]</span></p>
<h3 id="mutivariate-random-variables">Mutivariate random variables</h3>
<h4 id="joint-cdf">Joint CDF</h4>
<p>We denote joint CDF by <span class="math inline">\(F(x,y) \triangleq P(X \leq x \cap Y \leq y)=P(X \leq x , Y \leq y)\)</span>.</p>
<p><span class="math display">\[F(x,y) \triangleq P(X \leq x, Y \leq y)=\begin{cases}
\sum_{u \leq x, v \leq y}p(u,v) \\
\int_{-\infty}^{x}\int_{-\infty}^{y} f(u,v)\mathrm{d}u\mathrm{d}v \\
\end{cases}\]</span></p>
<p><strong>product rule</strong>: <span class="math display">\[\label{eqn:product-rule}
p(X,Y)=P(X|Y)P(Y)\]</span></p>
<p><strong>Chain rule</strong>: <span class="math display">\[p(X_{1:N})=p(X_1)p(X_3|X_2,X_1)...p(X_N|X_{1:N-1})\]</span></p>
<h4 id="marginal-distribution">Marginal distribution</h4>
<p><strong>Marginal CDF</strong>: <span class="math display">\[\begin{split}
F_X(x) \triangleq F(x,+\infty)= 
&amp; \begin{cases}
\sum\limits_{x_i \leq x}P(X=x_i)=\sum\limits_{x_i \leq x}\sum\limits_{j=1}^{+\infty}P(X=x_i,Y=y_j) \\
\int_{-\infty}^{x}f_X(u)du=\int_{-\infty}^{x}\int_{-\infty}^{+\infty} f(u,v)\mathrm{d}u\mathrm{d}v \\
\end{cases}
\end{split}\]</span></p>
<p><span class="math display">\[\begin{split}
F_Y(y) \triangleq F(+\infty,y)= 
&amp; \begin{cases}
\sum\limits_{y_j \leq y}p(Y=y_j)=\sum\limits_{i=1}^{+\infty}\sum_{y_j \leq y}P(X=x_i,Y=y_j) \\
\int_{-\infty}^{y}f_Y(v)dv=\int_{-\infty}^{+\infty}\int_{-\infty}^{y} f(u,v)\mathrm{d}u\mathrm{d}v \\
\end{cases}
\end{split}\]</span></p>
<p><strong>Marginal PMF and PDF</strong>: <span class="math display">\[\begin{cases}
P(X=x_i)=\sum_{j=1}^{+\infty}P(X=x_i,Y=y_j) &amp; \text{, descrete}\\
f_X(x)=\int_{-\infty}^{+\infty} f(x,y)\mathrm{d}y &amp; \text{, continuous}\\
\end{cases}\]</span></p>
<p><span class="math display">\[\begin{cases}
p(Y=y_j)=\sum_{i=1}^{+\infty}P(X=x_i,Y=y_j) &amp; \text{, descrete}\\
f_Y(y)=\int_{-\infty}^{+\infty} f(x,y)\mathrm{d}x &amp; \text{, continuous}\\
\end{cases}\]</span></p>
<h4 id="conditional-distribution">Conditional distribution</h4>
<p><strong>Conditional PMF</strong>: <span class="math display">\[p(X=x_i|Y=y_j)=\dfrac{p(X=x_i,Y=y_j)}{p(Y=y_j)} \text{ if } p(Y)&gt;0\]</span> The pmf <span class="math inline">\(p(X|Y)\)</span> is called <strong>conditional probability</strong>.</p>
<p><strong>Conditional PDF</strong>: <span class="math display">\[f_{X|Y}(x|y)=\dfrac{f(x,y)}{f_Y(y)}\]</span></p>
<h3 id="bayes-rule">Bayes rule</h3>
<p>Bayes’ theorem <span class="math display">\[p(Y|X) = \frac{p(X|Y)p(Y)}{p(X)}\]</span> Denominator in Bayes’ theorem <span class="math display">\[p(X) = \sum_Y{p(X|Y)p(Y)}\]</span></p>
<p>Bayesian probabilities So far, we have viewed probabilities in terms of the frequencies of random,repeatable events,which we shall refer to as the classical or frequentist interpretation of probability.Now we turn to the more general Bayesian view,in which probabilities provide a quantification of uncertainty.</p>
<p>We can adopt a similar approach when making inferences about quantities such as the parameters <span class="math inline">\(\mathbf{w}\)</span> in the polynomial curve fitting.We capture our assumptions about <strong>w</strong>,before observing the data,in the form of a <strong>prior</strong> probability distribution <span class="math inline">\(p(\mathbf{w})\)</span>.The effect of the observed data <span class="math inline">\(\mathcal{D} = {t_1,...,t_N}\)</span> is expressed through the conditional probability <span class="math inline">\(p(D|w)\)</span>.Bayes’ theorem,which takes the form <span class="math display">\[p(\textbf{w}|\mathcal{D}) = \frac{p(\mathcal{D}|\textbf{w})p(\textbf{w})}{p(\mathcal{D})}\]</span> then allows us to evaluate the uncertainty in <span class="math inline">\(\vec{w}\)</span> <strong>after</strong> we have observed <span class="math inline">\(\mathcal{D}\)</span> in the form of the <strong>posterior</strong> probability <span class="math inline">\(p(\mathbf{w}|\mathcal{D})\)</span>.</p>
<p>The quantity <span class="math inline">\(p(D|w)\)</span> on the right-hand side of Bayes’ theorem is evaluated for the observed data set D and can be viewed as a function of the parameter vector <span class="math inline">\(\mathbf{w}\)</span>,in which case it is called the <strong>likelihood function</strong>.It expresses how probable the observed data set is for different settings of the parameter vector <span class="math inline">\(\vec{w}\)</span>. <span class="math display">\[posteroir \propto likelihood \times prior\]</span> where all of these quantities are viewed as functions of <span class="math inline">\(\vec{w}\)</span>. Summing both side with respect to <span class="math inline">\(\mathbf{w}\)</span> <span class="math display">\[\begin{aligned}
&amp; p(\mathcal{D})p(\vec{w}|\mathcal{D}) &amp;= p(\mathcal{D}|\vec{w})p(\vec{w}) \\
&amp;\Rightarrow \sum_{\vec{w}}p(\mathcal{D})p(\vec{w}|\mathcal{D}) &amp;= \sum_{\vec{w}}p(\mathcal{D}|\vec{w})p(\vec{w}) \\
&amp;\Rightarrow p(\mathcal{D})&amp;= \sum_{\vec{w}}p(\mathcal{D}|\vec{w})p(\vec{w}) \\\end{aligned}\]</span> Integrating both side with respect to <span class="math inline">\(\mathbf{w}\)</span> <span class="math display">\[\begin{aligned}
\int p(\mathcal{D})p(\vec{w}|\mathcal{D})d{\vec{w}} &amp;= \int p(\mathcal{D}|\vec{w})p(\vec{w}) d{\vec{w}} \\ 
p(\mathcal{D}) &amp;= \int p(\mathcal{D}|\mathbf{w}) p(\mathbf{w})d\mathbf{w} 
 \end{aligned}\]</span></p>
<p>A widely used frequentist estimator is <strong>maximum likelihood</strong>,in which <span class="math inline">\(\mathbf{w}\)</span> is set to the value that maximizes the likelihood function <span class="math inline">\(p(\mathcal{D}|w)\)</span>.In the machine learning literature,the negative log of the likelihood function is called an <strong>error function</strong>.</p>
<p>Oner approach to determining the frequentist error bars is the <strong>bootstrap</strong>,in which multiple data sets are created as follows.Suppose out original data set consists of <span class="math inline">\(N\)</span> data points.We can create a new data set <span class="math inline">\(X_B\)</span> by drawing <span class="math inline">\(N\)</span> points at random from <span class="math inline">\(X\)</span>, with <strong>replacement</strong>, so that some points in X may be replicated in XB, whereas other points in X may be absent from <span class="math inline">\(X_B\)</span>. This process can be repeated L times to generate L data sets each of size N and each obtained by sampling from the original data set X. The statistical accuracy of parameter estimates can then be evaluated by looking at the variability of predictions between the different bootstrap data sets.</p>
<h3 id="independence-and-conditional-independence">Independence and conditional independence</h3>
<p>We say <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are unconditionally independent or marginally independent, denoted <span class="math inline">\(X \perp Y\)</span>, if we can represent the joint as the product of the two marginals, i.e., <span class="math display">\[X \perp Y=P(X,Y)=P(X)P(Y)\]</span></p>
<p>We say <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are conditionally independent(CI) given <span class="math inline">\(Z\)</span> if the conditional joint can be written as a product of conditional marginals: <span class="math display">\[X \perp Y|Z=P(X,Y|Z)=P(X|Z)P(Y|Z)\]</span></p>
<h3 id="quantiles">Quantiles</h3>
<p>Since the cdf <span class="math inline">\(F\)</span> is a monotonically increasing function, it has an inverse; let us denote this by <span class="math inline">\(F^{-1}\)</span>. If <span class="math inline">\(F\)</span> is the cdf of <span class="math inline">\(X\)</span> , then <span class="math inline">\(F^{-1}(\alpha)\)</span> is the value of <span class="math inline">\(x_{\alpha}\)</span> such that <span class="math inline">\(P(X \leq x_{\alpha})=\alpha\)</span>; this is called the <span class="math inline">\(\alpha\)</span> quantile of <span class="math inline">\(F\)</span>. The value <span class="math inline">\(F^{-1}(0.5)\)</span> is the <strong>median</strong> of the distribution, with half of the probability mass on the left, and half on the right. The values <span class="math inline">\(F^{-1}(0.25)\)</span> and <span class="math inline">\(F^{−1}(0.75)\)</span>are the lower and upper <strong>quartiles</strong>.</p>
<h3 id="mean-and-variance">Mean and variance</h3>
<p>The most familiar property of a distribution is its <strong>mean</strong>,or <strong>expected value</strong>, denoted by <span class="math inline">\(\mu\)</span>. For discrete rv’s, it is defined as <span class="math inline">\(\mathbb{E}[X] \triangleq \sum_{x \in \mathcal{X}}xp(x)\)</span>, and for continuous rv’s, it is defined as <span class="math inline">\(\mathbb{E}[X] \triangleq \int_{\mathcal{X}}xp(x)\mathrm{d}x\)</span>. If this integral is not finite, the mean is not defined (we will see some examples of this later).</p>
<p>The <strong>variance</strong> is a measure of the “spread” of a distribution, denoted by <span class="math inline">\(\sigma^2\)</span>. This is defined as follows: <span class="math display">\[\begin{aligned}
var[X]&amp; =\mathbb{E}[(X-\mu)^2] \\
      &amp; =\int{(x-\mu)^2p(x)\mathrm{d}x} \nonumber \\
      &amp; =\int{x^2p(x)\mathrm{d}x}+{\mu}^2\int{p(x)\mathrm{d}x}-2\mu\int{xp(x)\mathrm{d}x} \nonumber \\
      &amp; =\mathbb{E}[X^2]-{\mu}^2\end{aligned}\]</span></p>
<p>from which we derive the useful result <span class="math display">\[\mathbb{E}[X^2]=\sigma^2+{\mu}^2\]</span></p>
<p>The <strong>standard deviation</strong> is defined as <span class="math display">\[std[X] \triangleq \sqrt{var[X]}\]</span></p>
<p>This is useful since it has the same units as <span class="math inline">\(X\)</span> itself.</p>
<p>Expectations and covariances The average value of some function <span class="math inline">\(f(x)\)</span> under a probability distribution p(x) is called the expectation of f(x) and will be denoted by <span class="math display">\[\mathbb{E}[f] = \sum_{x}p(x)f(x)
                                  \mathbb{E}[f] = \int p(x)f(x)dx\]</span></p>
<p>approximation <span class="math display">\[\mathbb{E}[f] \simeq \frac{1}{N}\sum_{n=1}^{N}{f(x_n)}\]</span></p>
<p>conditional expectation with respect to a conditional distribution <span class="math display">\[\mathbb{E}_x[f|y] = \sum_{x}p(x|y)f(x)\]</span></p>
<p>variance of f(x) is defined by <span class="math display">\[var[f] = \mathbb{E}[(f(x) - \mathbb{E}[f(x)])^2
var[f] = \mathbb{E}[f(x)^2]-\mathbb{E}[f(x)]^2\]</span> convariance <span class="math display">\[cov[x,y] = \mathbb{E}_{x,y}[\{x- \mathbb{E}[x]\}\{y-\mathbb{E}[y]\}]\]</span></p>
<p>In the case of two vectors of random variables <strong>x</strong> and <strong>y</strong> <span class="math display">\[cov[\textbf{x},\textbf{y}] = \mathbb{E}_{x,y}[\{\textbf{x}-\mathbb{E}[\textbf{x}]\} \{ \textbf{y}^T - \mathbb{E}[\textbf{y}^T ]\}] \\
cov[\textbf{x},\textbf{y}] = \mathbb{E}_{x,y}[\textbf{x}\textbf{y}^T] - \mathbb{E}[\textbf{x}] \mathbb{E}[\textbf{y}^T]\]</span></p>
<h2 id="some-common-discrete-distributions">Some common discrete distributions</h2>
<p>In this section, we review some commonly used parametric distributions defined on discrete state spaces, both finite and countably infinite.</p>
<h3 id="the-bernoulli-and-binomial-distributions">The Bernoulli and binomial distributions</h3>
<p>Now suppose we toss a coin only once. Let <span class="math inline">\(X \in \{0,1\}\)</span> be a binary random variable, with probability of “success” or “heads” of <span class="math inline">\(\theta\)</span>. We say that <span class="math inline">\(X\)</span> has a <strong>Bernoulli distribution</strong>. This is written as <span class="math inline">\(X \sim \text{Ber}(\theta)\)</span>, where the pmf is defined as <span class="math display">\[\text{Ber}(x|\theta) \triangleq \theta^{\mathbb{I}(x=1)}(1-\theta)^{\mathbb{I}(x=0)}\]</span></p>
<p>Suppose we toss a coin <span class="math inline">\(n\)</span> times. Let <span class="math inline">\(X \in \{0,1,\cdots,n\}\)</span> be the number of heads. If the probability of heads is <span class="math inline">\(\theta\)</span>, then we say <span class="math inline">\(X\)</span> has a <strong>binomial distribution</strong>, written as <span class="math inline">\(X \sim \text{Bin}(n, \theta)\)</span>. The pmf is given by <span class="math display">\[\label{eqn:binomial-pmf}
\text{Bin}(k|n,\theta) \triangleq \dbinom{n}{k}\theta^k(1-\theta)^{n-k}\]</span></p>
<h3 id="the-multinoulli-and-multinomial-distributions">The multinoulli and multinomial distributions</h3>
<p>The Bernoulli distribution can be used to model the outcome of one coin tosses. To model the outcome of tossing a K-sided dice, let <span class="math inline">\(\vec{x} =(\mathbb{I}(x=1),\cdots,\mathbb{I}(x=K)) \in \{0,1\}^K\)</span> be a random vector(this is called <strong>dummy encoding</strong> or <strong>one-hot encoding</strong>), then we say <span class="math inline">\(X\)</span> has a <strong>multinoulli distribution</strong>(or <strong>categorical distribution</strong>), written as <span class="math inline">\(X \sim \text{Cat}(\theta)\)</span>. The pmf is given by: <span class="math display">\[p(\vec{x}) \triangleq \prod\limits_{k=1}^K\theta_k^{\mathbb{I}(x_k=1)}\]</span></p>
<p>Suppose we toss a K-sided dice <span class="math inline">\(n\)</span> times. Let <span class="math inline">\(\vec{x} =(x_1,x_2,\cdots,x_K) \in \{0,1,\cdots,n\}^K\)</span> be a random vector, where <span class="math inline">\(x_j\)</span> is the number of times side <span class="math inline">\(j\)</span> of the dice occurs, then we say <span class="math inline">\(X\)</span> has a <strong>multinomial distribution</strong>, written as <span class="math inline">\(X \sim \text{Mu}(n, \vec{\theta})\)</span>. The pmf is given by <span class="math display">\[\label{eqn:multinomial-pmf}
p(\vec{x}) \triangleq \dbinom{n}{x_1 \cdots x_k} \prod\limits_{k=1}^K\theta_k^{x_k}\]</span> where <span class="math inline">\(\dbinom{n}{x_1 \cdots x_k} \triangleq \dfrac{n!}{x_1!x_2! \cdots x_K!}\)</span></p>
<p>Bernoulli distribution is just a special case of a Binomial distribution with <span class="math inline">\(n=1\)</span>, and so is multinoulli distribution as to multinomial distribution. See Table [tab:multinomial-summary] for a summary.</p>
<p><span>llll</span> Name &amp; K &amp; n &amp; X<br />
Bernoulli &amp; 1 &amp; 1 &amp; <span class="math inline">\(x \in \{0,1\}\)</span><br />
Binomial &amp; 1 &amp; - &amp; <span class="math inline">\(\vec{x} \in \{0,1,\cdots,n\}\)</span><br />
Multinoulli &amp; - &amp; 1 &amp; <span class="math inline">\(\vec{x} \in \{0,1\}^K, \sum_{k=1}^K x_k=1\)</span><br />
Multinomial &amp; - &amp; - &amp; <span class="math inline">\(\vec{x} \in \{0,1,\cdots,n\}^K, \sum_{k=1}^K x_k=n\)</span><br />
</p>
<h3 id="the-poisson-distribution">The Poisson distribution</h3>
<p>Poisson distribution is a discrete probability distribution that expresses the probability of a given number of events occurring in a <strong>fixed interval</strong> of time and/or space if these events occur with a <strong>known average rate</strong> and <strong>independently</strong> of the time since the last event.</p>
<p>We say that <span class="math inline">\(X \in \{0,1,2,\cdots\}\)</span> has a <strong>Poisson distribution</strong> with parameter <span class="math inline">\(\lambda&gt;0\)</span>, written as <span class="math inline">\(X \sim \text{Poi}(\lambda)\)</span>, if its pmf is <span class="math display">\[p(x|\lambda)=e^{-\lambda}\dfrac{\lambda^x}{x!}\]</span></p>
<p>The first term is just the normalization constant, required to ensure the distribution sums to 1.</p>
<p>The Poisson distribution is often used as a model for counts of rare events like radioactive decay and traffic accidents.</p>
<p><span>llllll</span> Name &amp; Written as &amp; X &amp; <span class="math inline">\(p(x)\)</span>(or <span class="math inline">\(p(\vec{x})\)</span>) &amp; <span class="math inline">\(\mathbb{E}[X]\)</span> &amp; <span class="math inline">\(\text{var}[X]\)</span><br />
Bernoulli &amp; <span class="math inline">\(X \sim \text{Ber}(\theta)\)</span> &amp; <span class="math inline">\(x \in \{0,1\}\)</span> &amp; <span class="math inline">\(\theta^{\mathbb{I}(x=1)}(1-\theta)^{\mathbb{I}(x=0)}\)</span> &amp; <span class="math inline">\(\theta\)</span> &amp; <span class="math inline">\(\theta(1-\theta)\)</span><br />
Binomial &amp; <span class="math inline">\(X \sim \text{Bin}(n,\theta)\)</span> &amp; <span class="math inline">\(x \in \{0,1,\cdots,n\}\)</span> &amp; <span class="math inline">\(\dbinom{n}{k}\theta^k(1-\theta)^{n-k}\)</span> &amp; <span class="math inline">\(n\theta\)</span> &amp; <span class="math inline">\(n\theta(1-\theta)\)</span><br />
Multinoulli &amp; <span class="math inline">\(X \sim \text{Cat}(\vec{\theta})\)</span> &amp; <span class="math inline">\(\vec{x} \in \{0,1\}^K, \sum_{k=1}^K x_k=1\)</span> &amp; <span class="math inline">\(\prod\limits_{k=1}^K\theta_j^{\mathbb{I}(x_j=1)}\)</span> &amp; - &amp; -<br />
Multinomial &amp; <span class="math inline">\(X \sim \text{Mu}(n,\vec{\theta})\)</span> &amp; <span class="math inline">\(\vec{x} \in \{0,1,\cdots,n\}^K, \sum_{k=1}^K x_k=n\)</span> &amp; <span class="math inline">\(\dbinom{n}{x_1 \cdots x_k} \prod\limits_{k=1}^K\theta_j^{x_j}\)</span> &amp; - &amp; -<br />
Poisson &amp; <span class="math inline">\(X \sim \text{Poi}(\lambda)\)</span> &amp; <span class="math inline">\(x \in \{0,1,2,\cdots\}\)</span> &amp; <span class="math inline">\(e^{-\lambda}\dfrac{\lambda^x}{x!}\)</span> &amp; <span class="math inline">\(\lambda\)</span> &amp; <span class="math inline">\(\lambda\)</span><br />
</p>
<p>The Poisson distribution can be derived as a <strong>limiting case to the binomial distribution</strong> as the number of trials goes to infinity and the expected number of successes remains fixed,known as <strong>law of rare events</strong>.Assume that there exists a small enough subinterval for which the probability of an event occurring twice is “negligible”.Given only the information of expected number of total events in the whole interval,denoted as <span class="math inline">\(\lambda\)</span>,<strong>divide the whole interval into n subintervals</strong> <span class="math inline">\(I_1,I_2,...,I_n\)</span> of equal size,such that <span class="math inline">\(n &gt; \lambda\)</span>. The occurrence of an event in the whole interval can be seen as a <strong>Bernoulli trial</strong>, where the <span class="math inline">\(i\)</span>th trial corresponds to looking whether an event happens at the subinterval <span class="math inline">\(I_i\)</span> with probability <span class="math inline">\(p= \lambda / n\)</span>.Then the number of events <span class="math inline">\(x\)</span> that occur obey <strong>binomial distribution</strong> <span class="math inline">\(X\sim B(x;n,p)\)</span>. <span class="math display">\[\begin{aligned}
    &amp; P(x) &amp;= B(x;n,p)\\
    &amp;=&amp;C_n^x \left( p \right)^x \left( 1-p \right)^{n-x} \\
    &amp;=&amp;\frac{n!}{x!(n-x)!}p^x(1-p)^{n-x}\\
    &amp;=&amp;\frac{n(n-1)(n-2) \cdots (n-x+1)}{x!} \cdot \frac{\lambda^x}{n^x} \left( 1 - \frac{\lambda}{n} \right)^{n-x} \\
    &amp;=&amp;\frac{n}{n} \cdot \frac{n-1}{n} \cdots \frac{n-x+1}{n} \cdot \frac{\lambda^x}{x!}\left( 1 - \frac{\lambda}{n} \right)^{n-x} \\
    &amp;=&amp;\frac{n}{n} \cdot \frac{n-1}{n} \cdots \frac{n-x+1}{n} \cdot \frac{\lambda^x}{x!}
        \left(\left( 1 - \frac{\lambda}{n} \right)^{-\frac{n}{\lambda}}\right)^{-\lambda}\left( 1 - \frac{\lambda}{n} \right)^{-x}\end{aligned}\]</span> For <span class="math inline">\(n \rightarrow \infty\)</span>,we have <span class="math display">\[\begin{aligned}
&amp; (1-\frac{1}{n})(1-\frac{2}{n})\cdots(1-\frac{x-1}{n})&amp;\rightarrow 1 \\
&amp; \left(\left( 1 - \frac{\lambda}{n} \right)^{-\frac{n}{\lambda}}\right)^{-\lambda} &amp;\rightarrow e^{-\lambda} \\
&amp; (1-\frac{\lambda}{n})^{-x}&amp;\rightarrow 1\end{aligned}\]</span> So we have <span class="math display">\[\begin{aligned}
\lim_{n \rightarrow \infty} P(x) = \frac{e^{-\lambda} \lambda^x}{x!}\end{aligned}\]</span></p>
<h3 id="the-empirical-distribution">The empirical distribution</h3>
<p>The <strong>empirical distribution function</strong><a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a>, or <strong>empirical cdf</strong>, is the cumulative distribution function associated with the empirical measure of the sample. Let <span class="math inline">\(\mathcal{D}=\{x_1,x_2,\cdots,x_N\}\)</span> be a sample set, it is defined as <span class="math display">\[F_n(x) \triangleq \dfrac{1}{N}\sum\limits_{i=1}^N\mathbb{I}(x_i \leq x)\]</span></p>
<h2 id="some-common-continuous-distributions">Some common continuous distributions</h2>
<p>In this section we present some commonly used univariate (one-dimensional) continuous probability distributions.</p>
<h3 id="gaussian-normal-distribution">Gaussian (normal) distribution</h3>
<p><span>cccccc</span> Written as &amp; <span class="math inline">\(f(x)\)</span> &amp; <span class="math inline">\(\mathbb{E}[X]\)</span> &amp; mode &amp; <span class="math inline">\(\text{var}[X]\)</span><br />
<span class="math inline">\(X \sim \mathcal{N}(\mu,\sigma^2)\)</span> &amp; <span class="math inline">\(\dfrac{1}{\sqrt{2\pi}\sigma}e^{-\frac{1}{2\sigma^2}\left(x-\mu\right)^2}\)</span> &amp; <span class="math inline">\(\mu\)</span> &amp; <span class="math inline">\(\mu\)</span> &amp; <span class="math inline">\(\sigma^2\)</span><br />
</p>
<p>If <span class="math inline">\(X \sim N(0,1)\)</span>,we say <span class="math inline">\(X\)</span> follows a <strong>standard normal</strong> distribution.</p>
<p>The Gaussian distribution is the most widely used distribution in statistics. There are several reasons for this.</p>
<ol>
<li><p>First, it has two parameters which are easy to interpret, and which capture some of the most basic properties of a distribution, namely its mean and variance.</p></li>
<li><p>Second,the central limit theorem (Section TODO) tells us that sums of independent random variables have an approximately Gaussian distribution, making it a good choice for modeling residual errors or “noise”.</p></li>
<li><p>Third, the Gaussian distribution makes the least number of assumptions (has maximum entropy), subject to the constraint of having a specified mean and variance, as we show in Section TODO; this makes it a good default choice in many cases.</p></li>
<li><p>Finally, it has a simple mathematical form, which results in easy to implement, but often highly effective, methods, as we will see.</p></li>
</ol>
<p>See (Jaynes 2003, ch 7) for a more extensive discussion of why Gaussians are so widely used.More about Gaussian distribution is discussed in [sec:Gaussian distribution].</p>
<h3 id="students-t-distribution">Student’s t-distribution</h3>
<p><span>cccccc</span> Written as &amp; <span class="math inline">\(f(x)\)</span> &amp; <span class="math inline">\(\mathbb{E}[X]\)</span> &amp; mode &amp; <span class="math inline">\(\text{var}[X]\)</span><br />
<span class="math inline">\(X \sim \mathcal{T}(\mu,\sigma^2,\nu)\)</span> &amp; <span class="math inline">\(\dfrac{\Gamma(\frac{\nu+1}{2})}{\sqrt{\nu\pi}\Gamma(\frac{\nu}{2})}\left[1+\dfrac{1}{\nu}\left(\dfrac{x-\mu}{\nu}\right)^2\right]\)</span> &amp; <span class="math inline">\(\mu\)</span> &amp; <span class="math inline">\(\mu\)</span> &amp; <span class="math inline">\(\dfrac{\nu\sigma^2}{\nu-2}\)</span><br />
</p>
<p>where <span class="math inline">\(\Gamma(x)\)</span> is the gamma function: <span class="math display">\[\Gamma(x) \triangleq \int_0^\infty t^{x-1}e^{-t}\mathrm{d}t\]</span> <span class="math inline">\(\mu\)</span> is the mean, <span class="math inline">\(\sigma^2&gt;0\)</span> is the scale parameter, and <span class="math inline">\(\nu&gt;0\)</span> is called the <strong>degrees of freedom</strong>. See Figure [fig:pdfs-for-NTL] for some plots.</p>
<p><br />
</p>
<p>The variance is only defined if <span class="math inline">\(\nu&gt;2\)</span>. The mean is only defined if <span class="math inline">\(\nu&gt;1\)</span>.</p>
<p>As an illustration of the robustness of the Student distribution, consider Figure [fig:robustness]. We see that the Gaussian is affected a lot, whereas the Student distribution hardly changes. This is because the Student has heavier tails, at least for small <span class="math inline">\(\nu\)</span>(see Figure [fig:pdfs-for-NTL]).</p>
<p><br />
</p>
<p>If <span class="math inline">\(\nu=1\)</span>, this distribution is known as the <strong>Cauchy</strong> or <strong>Lorentz</strong> distribution. This is notable for having such heavy tails that the integral that defines the mean does not converge.</p>
<p>To ensure finite variance, we require <span class="math inline">\(\nu&gt;2\)</span>. It is common to use <span class="math inline">\(\nu=4\)</span>, which gives good performance in a range of problems (Lange et al. 1989). For <span class="math inline">\(\nu \gg 5\)</span>, the Student distribution rapidly approaches a Gaussian distribution and loses its robustness properties.</p>
<h3 id="the-laplace-distribution">The Laplace distribution</h3>
<p><span>cccccc</span> Written as &amp; <span class="math inline">\(f(x)\)</span> &amp; <span class="math inline">\(\mathbb{E}[X]\)</span> &amp; mode &amp; <span class="math inline">\(\text{var}[X]\)</span><br />
<span class="math inline">\(X \sim \text{Lap}(\mu,b)\)</span> &amp; <span class="math inline">\(\dfrac{1}{2b}\exp\left(-\dfrac{|x-\mu|}{b}\right)\)</span> &amp; <span class="math inline">\(\mu\)</span> &amp; <span class="math inline">\(\mu\)</span> &amp; <span class="math inline">\(2b^2\)</span><br />
</p>
<p>Here <span class="math inline">\(\mu\)</span> is a location parameter and <span class="math inline">\(b&gt;0\)</span> is a scale parameter. See Figure [fig:pdfs-for-NTL] for a plot.</p>
<p>Its robustness to outliers is illustrated in Figure [fig:robustness]. It also put mores probability density at 0 than the Gaussian. This property is a useful way to encourage sparsity in a model, as we will see in Section TODO.</p>
<h3 id="the-gamma-distribution">The gamma distribution</h3>
<p><span>ccccccc</span> Written as &amp; <span class="math inline">\(X\)</span> &amp; <span class="math inline">\(f(x)\)</span> &amp; <span class="math inline">\(\mathbb{E}[X]\)</span> &amp; mode &amp; <span class="math inline">\(\text{var}[X]\)</span><br />
<span class="math inline">\(X \sim \text{Ga}(a,b)\)</span> &amp; <span class="math inline">\(x \in \mathbb{R}^+\)</span> &amp; <span class="math inline">\(\dfrac{b^a}{\Gamma(a)}x^{a-1}e^{-xb}\)</span> &amp; <span class="math inline">\(\dfrac{a}{b}\)</span> &amp; <span class="math inline">\(\dfrac{a-1}{b}\)</span> &amp; <span class="math inline">\(\dfrac{a}{b^2}\)</span><br />
</p>
<p>Here <span class="math inline">\(a&gt;0\)</span> is called the shape parameter and <span class="math inline">\(b&gt;0\)</span> is called the rate parameter. See Figure [fig:gamma-distribution] for some plots.</p>
<p><br />
</p>
<h3 id="the-beta-distribution">The beta distribution</h3>
<p><span>ccccccc</span> Name &amp; Written as &amp; <span class="math inline">\(X\)</span> &amp; <span class="math inline">\(f(x)\)</span> &amp; <span class="math inline">\(\mathbb{E}[X]\)</span> &amp; mode &amp; <span class="math inline">\(\text{var}[X]\)</span><br />
Beta distribution &amp; <span class="math inline">\(X \sim \text{Beta}(a,b)\)</span> &amp; <span class="math inline">\(x \in [0,1]\)</span> &amp; <span class="math inline">\(\dfrac{1}{B(a,b)}x^{a-1}(1-x)^{b-1}\)</span> &amp; <span class="math inline">\(\dfrac{a}{a+b}\)</span> &amp; <span class="math inline">\(\dfrac{a-1}{a+b-2}\)</span> &amp; <span class="math inline">\(\dfrac{ab}{(a+b)^2(a+b+1)}\)</span><br />
</p>
<p>Here <span class="math inline">\(B(a, b)\)</span>is the beta function, <span class="math display">\[B(a,b) \triangleq \dfrac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)}\]</span></p>
<p>See Figure [fig:beta-distribution] for plots of some beta distributions. We require <span class="math inline">\(a, b &gt;0\)</span> to ensure the distribution is integrable (i.e., to ensure <span class="math inline">\(B(a, b)\)</span> exists). If <span class="math inline">\(a=b=1\)</span>, we get the uniform distirbution. If <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are both less than 1, we get a bimodal distribution with “spikes” at 0 and 1; if <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are both greater than 1, the distribution is unimodal.</p>
<div class="figure">
<img src="beta-distribution.png" alt="Some beta distributions." />
<p class="caption">Some beta distributions.<span data-label="fig:beta-distribution"></span></p>
</div>
<h3 id="pareto-distribution">Pareto distribution</h3>
<p><span>ccccccc</span> Name &amp; Written as &amp; <span class="math inline">\(X\)</span> &amp; <span class="math inline">\(f(x)\)</span> &amp; <span class="math inline">\(\mathbb{E}[X]\)</span> &amp; mode &amp; <span class="math inline">\(\text{var}[X]\)</span><br />
Pareto distribution &amp; <span class="math inline">\(X \sim \text{Pareto}(k,m)\)</span> &amp; <span class="math inline">\(x \geq m\)</span> &amp; <span class="math inline">\(km^kx^{-(k+1)}\mathbb{I}(x \geq m)\)</span> &amp; <span class="math inline">\(\dfrac{km}{k-1} \text{ if } k &gt; 1\)</span> &amp; <span class="math inline">\(m\)</span> &amp; <span class="math inline">\(\dfrac{m^2k}{(k-1)^2(k-2)} \text{ if } k&gt;2\)</span><br />
</p>
<p>The <strong>Pareto distribution</strong> is used to model the distribution of quantities that exhibit <strong>long tails</strong>, also called <strong>heavy tails</strong>.</p>
<p>As <span class="math inline">\(k \rightarrow \infty\)</span>, the distribution approaches <span class="math inline">\(\delta(x-m)\)</span>. See Figure [fig:Pareto-distribution](a) for some plots. If we plot the distribution on a log-log scale, it forms a straight line, of the form <span class="math inline">\(\log p(x)=a\log x+c\)</span> for some constants <span class="math inline">\(a\)</span> and <span class="math inline">\(c\)</span>. See Figure [fig:Pareto-distribution](b) for an illustration (this is known as a <strong>power law</strong>).</p>
<p><br />
</p>
<h2 id="sec:Gaussian distribution">The Gaussian Distribution</h2>
<p>The Gaussian distributions,also known as the normal distributions,is a widely used model for the distribution of continuous variables.In the case of single variable <span class="math inline">\(x\)</span>,the Gaussian is in the form <span class="math display">\[\begin{aligned}
\label{eqn:univariate Gaussian distributions}
\mathcal{N}(x|\mu,\sigma^2)=\dfrac{1}{(2\pi\sigma^2)^{1/2}}\exp\{-\dfrac{1}{(2\sigma)^2}(x-\mu)^2 \}\end{aligned}\]</span> where <span class="math inline">\(\mu\)</span> is the mean and <span class="math inline">\(\sigma^2\)</span> is the variance.For a <span class="math inline">\(D\)</span>-dimensional vector <span class="math inline">\(\vec{x}\)</span>,the multivariate Gaussian distribution takes the form <span class="math display">\[\begin{aligned}
\label{eqn:multivariate Gaussian}
    \mathcal{N}(\vec{x}|\vec{\mu},\vec{\Sigma})
    =\dfrac{1}{(2\pi)^{D/2}}\dfrac{1}{\mid \vec{\Sigma}\mid^{1/2}}
    \exp\{-\dfrac{1}{2}(\vec{x}-\vec{\mu})^T\vec{\Sigma}^{-1}(\vec{x}-\vec{\mu}) \}\end{aligned}\]</span> where <span class="math inline">\(\vec{\mu}\)</span> is a <span class="math inline">\(D\)</span>-dimensional mean vector,<span class="math inline">\(\vec{\Sigma}\)</span> is a <span class="math inline">\(D\times D\)</span> covariance matrix,and <span class="math inline">\(\mid\vec{\Sigma}\mid\)</span> denotes the determinant of <span class="math inline">\(\vec{\Sigma}\)</span>.</p>
<p>The Gaussian distribution arises in many different contexts and can be motivated from a variety of different perspectives,such as maximizing the entropy and the sum of multiple random variables.The <strong>central limit theorem</strong> tells us the latter.</p>
<h3 id="linear-transformation-based-on-eigenvector">Linear Transformation based on Eigenvector</h3>
<p>Recall from Section [sec:MVN],for a <span class="math inline">\(D-dimensionl\)</span> vector ,the multivariate Gaussian distribution(MVN),i.e. the <strong>pdf</strong> takes the form: <span class="math display">\[\mathcal{N}(\vec{x}|\vec{\mu},\Sigma) \triangleq \dfrac{1}{(2\pi)^{\frac{D}{2}}|\Sigma|^{\frac{1}{2}}}\exp\left[-\dfrac{1}{2}(\vec{x}-\vec{\mu})^T\Sigma^{-1}(\vec{x}-\vec{\mu})\right]\]</span> where <span class="math inline">\(\mu\)</span> is a <span class="math inline">\(D-dimensional\)</span> mean vector,<span class="math inline">\(\Sigma\)</span> is a <span class="math inline">\(D\times D\)</span> covariance matrix,and <span class="math inline">\(|\Sigma|\)</span> denotes the determinant of <span class="math inline">\(\Sigma\)</span>.</p>
<p>The Gaussian distribution arises in many different contexts and can be motivated from variety of different perspectives,such as the distribution that maximizes the entropy.Another situation in which the Gaussian distribution arises is when we consider the sum of multiple random variables.The <span class="math inline">\(central limit theorem\)</span> (due to Laplace),tells us that,subject to certain mild conditions,the sum of a set of random variables,which is of course itself a random variable,has a distribution that becomes increasingly Gaussian as the number of terms in the sum increases(Walker,1969).</p>
<p>The expression inside the exponent,which is the functional dependence of the Gaussian on <span class="math inline">\(\vec{x}\)</span>,is through the quadratic form <span class="math display">\[\label{eqn:Gaussian quadratic form}
\Delta^2 = (\vec{x}-\vec{\mu})^T\Sigma^{-1}(\vec{x}-\vec{\mu})\]</span> The quantity <span class="math inline">\(\Delta\)</span> is the <span class="math inline">\(Mahalanobis distance\)</span> between a data vector <span class="math inline">\(\vec{x}\)</span> and the mean vector <span class="math inline">\(\vec{\mu}\)</span> and reduces to the Euclidean distance when <span class="math inline">\(\Sigma\)</span> is the identity matrix. We can gain a better understanding of this quantity by performing an <strong>eigendecomposition</strong> of <span class="math inline">\(\vec{\Sigma}\)</span>. That is, we write <span class="math inline">\(\vec{\Sigma}=\vec{U}\vec{\Lambda}\vec{U}^T\)</span>, where <span class="math inline">\(\vec{U}\)</span> is an orthonormal matrix of eigenvectors satsifying <span class="math inline">\(\vec{U}^T\vec{U}=\vec{I}\)</span>, and <span class="math inline">\(\vec{\Lambda}\)</span> is a diagonal matrix of eigenvalues. Using the eigendecomposition, we have that <span class="math display">\[\vec{\Sigma}^{-1}=\vec{U}^{-T}\vec{\Lambda}^{-1}\vec{U}^{-1}=\vec{U}\vec{\Lambda}^{-1}\vec{U}^T=\sum\limits_{i=1}^D \dfrac{1}{\lambda_i}\vec{u}_i\vec{u}_i^T\]</span> where <span class="math inline">\(\vec{u}_i\)</span> is the <span class="math inline">\(i\)</span>’th column of <span class="math inline">\(\vec{U}\)</span>, containing the <span class="math inline">\(i\)</span>’th eigenvector. Hence we can rewrite the Mahalanobis distance as follows: <span class="math display">\[\begin{aligned}
(\vec{x}-\vec{\mu})^T\Sigma^{-1}(\vec{x}-\vec{\mu}) &amp; =(\vec{x}-\vec{\mu})^T\left(\sum\limits_{i=1}^D \dfrac{1}{\lambda_i}\vec{u}_i\vec{u}_i^T\right)(\vec{x}-\vec{\mu}) \\
&amp; =\sum\limits_{i=1}^D \dfrac{1}{\lambda_i}(\vec{x}-\vec{\mu})^T\vec{u}_i\vec{u}_i^T(\vec{x}-\vec{\mu}) \\
&amp; =\sum\limits_{i=1}^D \dfrac{y_i^2}{\lambda_i}\end{aligned}\]</span> where <span class="math inline">\(y_i \triangleq \vec{u}_i^T(\vec{x}-\vec{\mu})\)</span>. Recall that the equation for an ellipse in 2d is <span class="math display">\[\dfrac{y_1^2}{\lambda_1}+\dfrac{y_2^2}{\lambda_2}=1\]</span></p>
<p>Hence we see that the contours of equal probability density of a Gaussian lie along ellipses. This is illustrated in Figure [fig:2d-MVN]. The eigenvectors determine the orientation of the ellipse, and the eigenvalues determine how elogonated it is.</p>
<div class="figure">
<img src="2d-MVN.png" alt="Visualization of a 2 dimensional Gaussian density. The major and minor axes of the ellipse are defined by the first two eigenvectors of the covariance matrix, namely \vec{u}_1 and \vec{u}_2. Based on Figure 2.7 of (Bishop 2006a)" />
<p class="caption">Visualization of a 2 dimensional Gaussian density. The major and minor axes of the ellipse are defined by the first two eigenvectors of the covariance matrix, namely <span class="math inline">\(\vec{u}_1\)</span> and <span class="math inline">\(\vec{u}_2\)</span>. Based on Figure 2.7 of (Bishop 2006a)<span data-label="fig:2d-MVN"></span></p>
</div>
<p>In general, we see that the Mahalanobis distance corresponds to Euclidean distance in a transformed coordinate system, where we shift by <span class="math inline">\(\vec{\mu}\)</span> and rotate by <span class="math inline">\(\vec{U}\)</span>.</p>
<p>First of all,we note that the matrix <span class="math inline">\(\Sigma\)</span> can be taken to be symmetric,without loss of generality.Now we consider the eigenvector equation for the convariance matrix <span class="math display">\[\Sigma\vec{\mu_i} = \lambda_i\vec{\mu_i}\]</span> where <span class="math inline">\( i = 1,...,D \)</span>.Because <span class="math inline">\(\Sigma\)</span> is real,symmetric matrix its eigenvalues will be real,and its eigenvectors can be chosen to form an orthonormal set,so that <span class="math display">\[\vec{\mu_i}^T\vec{\mu_j} = \mathcal{I}_{ij}\]</span> where <span class="math inline">\(\mathcal{I}_{ij}\)</span> is the <span class="math inline">\(i,j\)</span> element of the identity matrix and satisfies <span class="math display">\[\begin{aligned}
    \mathcal{I}_{ij} = \begin{cases}
    &amp; 1,\text{if i=j} \\
    &amp; 0,\text{otherwise}.
    \end{cases}
    \end{aligned}\]</span> The covariance matrix <span class="math inline">\(\Sigma\)</span> can be expressed as an equation in terms of its eigenvectors in the form <span class="math display">\[\begin{aligned}
    \vec{\Sigma} &amp; =\vec{U}\vec{\Lambda}\vec{U}^T \\
    &amp; = \sum\limits_{i=1}^{D}\lambda_i\vec{\mu_i}\vec{\vec{u_i}^T}
    \end{aligned}\]</span> where <span class="math inline">\(\vec{u}_i\)</span> is the <span class="math inline">\(i\)</span>’th column of <span class="math inline">\(\vec{U}\)</span>, containing the <span class="math inline">\(i\)</span>’th eigenvector. Similarly,the inverse covariance matrix <span class="math inline">\(\Sigma^{-1}\)</span> can be expressed as <span class="math display">\[\label{eqn:Gaussian inverse covariance matrix eigen form}
    \vec{\Sigma^{-1}} = \sum\limits_{i=1}^{D}\dfrac{1}{\lambda_i}\vec{u_i}\vec{u_i}^T\]</span> Substituting [eqn:Gaussian inverse covariance matrix eigen form] into [eqn:Gaussian quadratic form] ,the quadratic form becomes <span class="math display">\[\Delta^2 = \sum\limits_{i=1}^{D}\dfrac{y_i^2}{\lambda_i}\]</span> where we have defined <span class="math display">\[y_i = \vec{u_i}^T(\vec{x}-\vec{u})\]</span> We can interpret <span class="math inline">\(\{y_i\}\)</span> as a new coordinate system defined by the orthonormal vectors <span class="math inline">\(\vec{u_i}\)</span> that are shifted and rotated with respect to the original <span class="math inline">\(x_i\)</span> coordinates.Forming the vector <span class="math inline">\(\vec{y}=(y_1,...,y_D)^T\)</span>,we have <span class="math display">\[\vec{y} = \vec{U}(\vec{x}-\vec{\mu})\]</span> where <span class="math display">\[\vec{U} = \begin{bmatrix}
    \vec{u_1}^T &amp; \vec{u_2}^T&amp;...&amp;\vec{u_n}^T
    \end{bmatrix}\]</span> <span class="math inline">\(\vec{U}\)</span> is an <span class="math inline">\(orthogonal\)</span> matrix. A matrix whose eigenvalue are strictly positive is said to be <strong>positive definite</strong>, and if all the eigenvalues are nonnegative,then the covariance matrix is said to be <strong>positive semidefinite</strong>.</p>
<p>Now consider the form of Gaussian distribution in the new coordinate system defined by the <span class="math inline">\(y_i\)</span>.In going from <span class="math inline">\(\vec{x}\)</span> to <span class="math inline">\(\vec{y}\)</span> coordinate system,we have a <strong>Jacobian matrix</strong> <span class="math inline">\(\vec{J}\)</span> with elements given by <span class="math display">\[\begin{aligned}
    J_{ij} = \dfrac{\partial x_i}{\partial y_i} = U_{ji}\\
    \vec{J} = \dfrac{\partial\vec{x}}{\partial\vec{y}}
    \end{aligned}\]</span> where <span class="math inline">\(U_{ji}\)</span> are the elements of the matrix <span class="math inline">\(\vec{U}^T\)</span>.Using the orthonormality property of the matrix <span class="math inline">\(\vec{U}\)</span>, we see that the square of the determinant of the Jacobian matrix is <span class="math display">\[|\vec{J}|^2 = |\vec{I}| = 1\]</span> Also,the determinant <span class="math inline">\(|\vec{\Sigma}|\)</span> of the covariance matrix can be written as the product of its eigenvalues,and hence <span class="math display">\[\begin{aligned}
    |\vec{\Sigma}|^{1/2} = \prod_{j=1}^{D}\lambda_j^{1/2}
    \end{aligned}\]</span> Thus in the <span class="math inline">\(y_i\)</span> coordinate system,the Gaussian distribution takes the form <span class="math display">\[p(\vec{y}) = p(\vec{x})|\vec{J}| = \prod_{j=1}^{D}\dfrac{1}{(2\pi \lambda_j)^{1/2}}\exp\{-\dfrac{y_j ^2}{2\lambda_j}\}\]</span> which is the product of <span class="math inline">\(D\)</span> independent univariate Gaussian distributions.So we have normalized the multivariate Gaussian.</p>
<p>Now we can check the first and second order moments of the Gaussian.</p>
<h4 id="limitations">limitations</h4>
<p>Gaussian distribution has quadratically growing parameters with dimension.A further limitation of the Gaussian distribution is that it is intrinsically unimodal(i.e.,has a single maximum) and so is unable to provide a good approximation to multimodal distributions.We will introduce <span class="math inline">\(latent\)</span> variables,also called <span class="math inline">\(hidden\)</span> variables or <span class="math inline">\(unobserved\)</span> variables to address both of these problems.</p>
<h4 id="mle-for-a-mvn">MLE for a MVN</h4>
<p>(<strong>MLE for a MVN</strong>) If we have <span class="math inline">\(N\)</span> iid samples <span class="math inline">\(\vec{x}_i \sim \mathcal{N}(\vec{\mu},\vec{\Sigma})\)</span>, then the MLE for the parameters is given by <span class="math display">\[\begin{aligned}
    \bar{\vec{\mu}}    &amp; =\dfrac{1}{N}\sum\limits_{i=1}^N \vec{x}_i \triangleq \bar{\vec{x}} \\
    \bar{\vec{\Sigma}} &amp; =\dfrac{1}{N}\sum\limits_{i=1}^N (\vec{x}_i-\bar{\vec{x}})(\vec{x}_i-\bar{\vec{x}})^T \\
    &amp; =\dfrac{1}{N}\left(\sum\limits_{i=1}^N \vec{x}_i\vec{x}_i^T\right)-\bar{\vec{x}}\bar{\vec{x}}^T
    \end{aligned}\]</span></p>
<h4 id="maximum-entropy-derivation-of-the-gaussian">Maximum entropy derivation of the Gaussian *</h4>
<p>In this section, we show that the multivariate Gaussian is the distribution with maximum entropy subject to having a specified mean and covariance (see also Section TODO). This is one reason the Gaussian is so widely used: the first two moments are usually all that we can reliably estimate from data, so we want a distribution that captures these properties, but otherwise makes as few addtional assumptions as possible.</p>
<p>To simplify notation, we will assume the mean is zero. The pdf has the form <span class="math display">\[f(\vec{x})=\dfrac{1}{Z}\exp\left(-\dfrac{1}{2}\vec{x}^T\vec{\Sigma}^{-1}\vec{x}\right)\]</span></p>
<h3 id="conditional-gaussian-distributions">Conditional Gaussian distributions</h3>
<p>If two sets of variables are jointly multivariate Gaussian, then the conditional distribution of one set conditioned on the other is again Gaussian. Similarly, the marginal distribution of either set is also Gaussian.</p>
<p>The multivariate normal distribution is given by <span class="math display">\[\begin{aligned}
\frac{1}{(2\pi)^{D/2}|\Sigma|^{1/2}} \exp\left(-\frac{1}{2} (\vec{x}-\vec{\mu})^T \Sigma^{-1} (\vec{x}-\vec{\mu})\right)\end{aligned}\]</span> Write <span class="math inline">\(\vec{x}\)</span> as <span class="math display">\[\vec{x} = \left[ \begin{array}{c} \vec{x}_a\\ \vec{x}_b \end{array} \right],
\vec{x} = \left[ \begin{array}{c} \vec{\mu}_a\\ \vec{\mu}_b \end{array} \right]\]</span> Consider the conditional distribution <span class="math inline">\(p(\vec{x}_a|\vec{x}_b)\)</span> and marginal distribution <span class="math inline">\(p(\vec{x}_a)\)</span>. Separate the components of the covariance matrix <span class="math inline">\(\vec{\Sigma}\)</span> into partitioned block matrix <span class="math display">\[\begin{aligned}
\vec{\Sigma} = \left[ \begin{array}{cc} \vec{\Sigma}_{aa} &amp; \vec{\Sigma}_{ab} \\ \vec{\Sigma}_{ba} &amp; \vec{\Sigma}_{bb} \end{array}\right]\end{aligned}\]</span> where <span class="math inline">\(\vec{\Sigma},\vec{\Sigma}_{aa},\vec{\Sigma}_{bb}\)</span> are all symmetric and <span class="math inline">\(\vec{\Sigma}_{ab} = \vec{\Sigma}_{ab}^T\)</span>.</p>
<p>From the product rule of probability, we see that this conditional distribution can be evaluated from the joint distribution <span class="math inline">\(p(\vec{x}) = p(\vec{x}_a,\vec{x}_b)\)</span> simple by fixing <span class="math inline">\(\vec{x}_b\)</span> to the observed value and <strong>normalizing</strong> the resulting expression to obtain a valid probability distribution over <span class="math inline">\(\vec{x}_a\)</span>.To evaluate the conditional probability from the joint probability,for any arbitrary <span class="math inline">\(\vec{x}_b\)</span>,we can use <strong>product and sum</strong> rule of probability <span class="math display">\[\begin{aligned}
p(\vec{x}_b) &amp;= \int p(\vec{x}_a,\vec{x}_b) d\vec{x}_a \\
p(\vec{x}_a\vec{x}_b) &amp;= \dfrac{p(\vec{x}_a,\vec{x}_b)}{p(\vec{x}_b)}\end{aligned}\]</span> Because <span class="math inline">\(p(\vec{x}_b)\)</span> is observed value,so we have <span class="math display">\[\begin{aligned}
p(\vec{x}_a\vec{x}_b) &amp;\sim \dfrac{p(\vec{x}_a,\vec{x}_b)}{p(\vec{x}_b)} \\
&amp;\sim p(\vec{x}_a,\vec{x}_b)\end{aligned}\]</span> which has the <strong>exponential quadratic form</strong>,and hence the corresponding conditional distribution will be Gaussian. <span class="math display">\[\begin{aligned}
&amp;-\dfrac{1}{2}(\vec{x}-\vec{\mu})\vec{\Sigma}^{-1}(\vec{x}-\vec{\mu}) \\
&amp;=-\dfrac{1}{2}\left[ \begin{array}{cc} 
(\vec{x}_a-\vec{\mu}_a)^T &amp; (\vec{x}_b-\vec{\mu}_b)^T 
\end{array}\right]
\left[ \begin{array}{cc} 
\vec{\varLambda}_{aa} &amp; \vec{\varLambda}_{ab}  \\
\vec{\varLambda}_{ba} &amp; \vec{\varLambda}_{bb}    
\end{array}\right]
\left[ \begin{array}{cc} 
(\vec{x}_a-\vec{\mu}_a) \\
(\vec{x}_b-\vec{\mu}_b)
\end{array}\right] \\
&amp;=-\dfrac{1}{2}\left[ \begin{array}{cc} 
(\vec{x}_a-\vec{\mu}_a)^T\vec{\varLambda}_{aa}+(\vec{x}_b-\vec{\mu}_b)^T\vec{\varLambda}_{ba} &amp;
(\vec{x}_a-\vec{\mu}_a)^T\vec{\varLambda}_{ab}+(\vec{x}_b-\vec{\mu}_b)^T\vec{\varLambda}_{bb}    
\end{array}\right]
\left[ \begin{array}{cc} 
(\vec{x}_a-\vec{\mu}_a) \\
(\vec{x}_b-\vec{\mu}_b)
\end{array}\right] \\
&amp;=-\dfrac{1}{2}((\vec{x}_a-\vec{\mu}_a)^T\vec{\varLambda}_{aa}(\vec{x}_a-\vec{\mu}_a)
+(\vec{x}_b-\vec{\mu}_b)^T\vec{\varLambda}_{ba}(\vec{x}_a-\vec{\mu}_a) \\
&amp;+ (\vec{x}_a-\vec{\mu}_a)^T\vec{\varLambda}_{ab}(\vec{x}_b-\vec{\mu}_b)
+(\vec{x}_b-\vec{\mu}_b)^T\vec{\varLambda}_{bb}(\vec{x}_b-\vec{\mu}_b) )\end{aligned}\]</span> With the operation called ’completing the square’ of the exponent form in a General Gaussian distribution <span class="math inline">\(\mathcal{N}(\vec{x}|\vec{\mu,\vec{\Sigma}})\)</span> <span class="math display">\[\begin{aligned}
-\dfrac{1}{2}(\vec{x}-\vec{\mu})^T\vec{\Sigma}^{-1}(\vec{x}-\vec{\mu}) &amp;= 
-\dfrac{1}{2}\vec{x}^T\vec{\Sigma}^{-1}\vec{x}+\vec{x}^T\vec{\Sigma}\vec{\mu} + \text{const}\end{aligned}\]</span> Based on this,we can apply <strong>method of undetermined coefficients</strong> to determinate the super parameters.Covariance matrix is in the corresponding second-order term in <span class="math inline">\(\vec{x}_a\)</span>,and mean vector is in the corresponding linear term in <span class="math inline">\(\vec{x}_a\)</span>.</p>
<p>We need to make use of <strong>Schur complement</strong> for the inverse covariance matrix(<strong>precision matrix</strong>) <span class="math display">\[\begin{aligned}
\left[ \begin{array}{cc} 
\vec{A} &amp; \vec{B} \\
\vec{C} &amp; \vec{D}  
\end{array}\right]^{-1}
= \left[ \begin{array}{cc} 
\vec{M} &amp; -\vec{M}\vec{B}\vec{D}^{-1} \\
-\vec{D}^{-1}\vec{C}\vec{M} &amp; \vec{D}^{-1}\vec{C}\vec{M}\vec{B}\vec{D}^{-1}
\end{array}\right]^{-1} \\\end{aligned}\]</span> where <span class="math display">\[\begin{aligned}
\vec{M} = (\vec{A}-\vec{B}\vec{D}^{-1}\vec{C})^{-1}\end{aligned}\]</span> Then the precision matrix <span class="math display">\[\begin{aligned}
\left[ \begin{array}{cc}
 \vec{\Sigma}_{aa} &amp; \vec{\Sigma}_{ab} \\
 \vec{\Sigma}_{ba} &amp; \vec{\Sigma}_{bb} 
 \end{array}\right]^{-1} =
 \left[ \begin{array}{cc} \vec{\varLambda}_{aa} &amp; \vec{\varLambda}_{ab} \\
  \vec{\varLambda}_{ba} &amp; \vec{\varLambda}_{bb} \end{array}\right]\end{aligned}\]</span> can be evaluated.</p>
<h3 id="marginal-gaussian-distributions">Marginal Gaussian distributions</h3>
<p>Wrapping up,given joint Gaussian distribution,we can obtain partitioned Gaussians. Conditional distribution: <span class="math display">\[\begin{aligned}
p(\vec{x}_a|\vec{x}_b) &amp;= \mathcal{N}(\vec{x}|\vec{\mu}_{a|b},\vec{\varLambda}_{aa}^{-1}) \\
\vec{\mu}_{a|b} &amp;= \vec{\mu}_a-\vec{\varLambda}_{aa}^{-1}\vec{\varLambda}_{ab}(\vec{x}_b-\vec{\mu}_b)\end{aligned}\]</span> Marginal distribution: <span class="math display">\[\begin{aligned}
p(\vec{x}_a) = \mathcal{N}(\vec{x}_a|\vec{\mu}_a,\vec{\Sigma}_{aa})\end{aligned}\]</span></p>
<h3 id="bayes-theorem-for-gaussian-variables">Baye’s theorem for Gaussian variables</h3>
<h2 id="sec:exponential-family">The exponential family</h2>
<p>Before defining the exponential family, we mention several reasons why it is important:</p>
<ul>
<li><p><span>It can be shown that, under certain regularity conditions, the exponential family is the only family of distributions with finite-sized sufficient statistics, meaning that we can compress the data into a fixed-sized summary without loss of information. This is particularly useful for online learning, as we will see later.</span></p></li>
<li><p><span>The exponential family is the only family of distributions for which conjugate priors exist, which simplifies the computation of the posterior (see Section [sec:Bayes-for-the-exponential-family]).</span></p></li>
<li><p><span>The exponential family can be shown to be the family of distributions that makes the least set of assumptions subject to some user-chosen constraints (see Section [sec:Maximum-entropy-derivation-of-the-exponential-family]).</span></p></li>
<li><p><span>The exponential family is at the core of generalized linear models, as discussed in Section [sec:GLMs].</span></p></li>
<li><p><span>The exponential family is at the core of variational inference, as discussed in Section TODO.</span></p></li>
</ul>
<h3 id="definition">Definition</h3>
<p>A pdf or pmf <span class="math inline">\(p(\vec{x}|\vec{\theta})\)</span>,for <span class="math inline">\(\vec{x} \in \mathbb{R}^m\)</span> and <span class="math inline">\(\vec{\theta} \in \mathbb{R}^D\)</span>, is said to be in the <strong>exponential family</strong> if it is of the form <span class="math display">\[\begin{aligned}
p(\vec{x}|\vec{\theta}) 
&amp;= h(\vec{x})g(\vec{\theta})\exp\{\vec{\theta}^T\vec{\phi}(\vec{x})\} \\
&amp;=\dfrac{1}{Z(\vec{\theta})}h(\vec{x})\exp[\vec{\theta}^T\phi(\vec{x})] \\
&amp; = h(\vec{x})\exp[\vec{\theta}^T\phi(\vec{x})-A(\vec{\theta})] \label{eqn:exponential-family}\end{aligned}\]</span> where <span class="math display">\[\begin{aligned}
Z(\vec{\theta}) &amp; =\int h(\vec{x})\exp[\vec{\theta}^T\phi(\vec{x})]\mathrm{d}\vec{x} \\
A(\vec{\theta}) &amp; =\log Z(\vec{\theta})\end{aligned}\]</span></p>
<p>Here <span class="math inline">\(\vec{\theta}\)</span> are called the <strong>natural parameters</strong> or <strong>canonical parameters</strong>, <span class="math inline">\(\phi(\vec{x}) \in \mathbb{R}^D\)</span> is called a vector of <strong>sufficient statistics</strong>, <span class="math inline">\(Z(\vec{\theta})\)</span> is called the <strong>partition function</strong>, <span class="math inline">\(A(\vec{\theta})\)</span> is called the <strong>log partition function</strong> or <strong>cumulant function</strong>, and <span class="math inline">\(h(\vec{x})\)</span> is the a scaling constant, often 1. If <span class="math inline">\(\phi(\vec{x})=\vec{x}\)</span>, we say it is a <strong>natural exponential family</strong>.</p>
<p>Equation [eqn:exponential-family] can be generalized by writing <span class="math display">\[p(\vec{x}|\vec{\theta}) = h(\vec{x})\exp[\eta(\vec{\theta})^T\phi(\vec{x})-A(\eta(\vec{\theta}))]\]</span> where <span class="math inline">\(\eta\)</span> is a function that maps the parameters <span class="math inline">\(\vec{\theta}\)</span> to the canonical parameters <span class="math inline">\(\vec{\eta}=\eta(\vec{\theta})\)</span>.If <span class="math inline">\(\mathrm{dim}(\vec{\theta})&lt;\mathrm{dim}(\eta(\vec{\theta}))\)</span>, it is called a <strong>curved exponential family</strong>, which means we have more sufficient statistics than parameters. If <span class="math inline">\(\eta(\vec{\theta})=\vec{\theta}\)</span>, the model is said to be in <strong>canonical form</strong>. We will assume models are in canonical form unless we state otherwise.</p>
<h3 id="maximum-likelihood-and-sufficient-statistics">Maximum likelihood and sufficient statistics</h3>
<h3 id="conjugate-priors">Conjugate priors</h3>
<h3 id="noninformative-prios">Noninformative prios</h3>
<p>In many cases,however,we may have little idea of what form the distribution should take.We may then seek a form of prior distribution,called a <strong>noninformative prior</strong>,which is intended to have little influence on the posterior distribution as possible(Jeffries,1946;Box and Tao,1973;Bernardo and Smith,1994).</p>
<p>Suppose we have a distribution <span class="math inline">\(p(x|\lambda)\)</span> governed by a parameter <span class="math inline">\(\lambda\)</span>,we might be tempted to propose a prior distribution <span class="math inline">\(p(\lambda) = \)</span>const as a suitable prior.Improper priors are ones in which the domain <span class="math inline">\(\lambda\)</span> is unbounded and the prior distribution cannot be correctly normalized because the integral over <span class="math inline">\(\lambda\)</span> diverges.A second difficulty arises from the transformation behaviour of a probability density under a nonlinear change of variables.For example <span class="math display">\[p_\eta(\eta) = p_\lambda(\lambda) \left | \dfrac{d\lambda}{d\eta} \right | = p_\lambda(\eta^2)\eta\propto \eta\]</span> so the density over <span class="math inline">\(\eta\)</span> will not be constant.</p>
<p>Here are two simple examples of noninformative priors(Berger,1985). First of all,if a density takes the form <span class="math display">\[p(x|\mu) = f(x-\mu)\]</span> then the parameter <span class="math inline">\(\mu\)</span> is known as a <strong>location parameter</strong>.This family of densities exhibits <strong>translation invariance</strong> because if we shift <span class="math inline">\(x\)</span> by a constant to give <span class="math inline">\(\hat{x} = x+c\)</span>,then <span class="math display">\[p(\hat{x}|\hat{\mu}) = f(\hat{x}-\hat{\mu})\]</span> where <span class="math inline">\(\hat{\mu} = \mu +c\)</span>.</p>
<p>A second example, <span class="math display">\[\label{eqn:density scale parameter}
p(x|\sigma) = \dfrac{1}{\sigma}f(\dfrac{x}{\sigma})\]</span> where <span class="math inline">\(\sigma&gt;0\)</span>.Note that this will be a normalized density provided f(x) is correctly normalized.The parameter <span class="math inline">\(\sigma\)</span> is known as a <strong>scale parameter</strong>,and the density exhibits <strong>scale invariance</strong>.</p>
<h3 id="examples">Examples</h3>
<h4 id="bernoulli">Bernoulli</h4>
<p>The Bernoulli for <span class="math inline">\(x \in \{0,1\}\)</span> can be written in exponential family form as follows: <span class="math display">\[\begin{split}
\mathrm{Ber}(x|\mu)&amp; =\mu^x(1-\mu)^{1-x} \\
&amp; =\exp[x\log\mu+(1-x)\log(1-\mu)]
\end{split}\]</span> where <span class="math inline">\(\phi(x)=(\mathbb{I}(x=0),\mathbb{I}(x=1))\)</span> and <span class="math inline">\(\vec{\theta}=(\log\mu,\log(1-\mu))\)</span>.</p>
<p>However, this representation is <strong>over-complete</strong> since <span class="math inline">\(\vec{1}^T\phi(x)=\mathbb{I}(x=0)+\mathbb{I}(x=1)=1\)</span>. Consequently <span class="math inline">\(\vec{\theta}\)</span> is not uniquely identifiable. It is common to require that the representation be <strong>minimal</strong>, which means there is a unique <span class="math inline">\(\theta\)</span> associated with the distribution. In this case, we can just define <span class="math display">\[\begin{aligned}
\mathrm{Ber}(x|\mu) &amp; =(1-\mu)\exp\left(x\log\dfrac{\mu}{1-\mu}\right) \\
\text{where } \phi(x) &amp; =x, \theta=\log\dfrac{\mu}{1-\mu}, Z=\dfrac{1}{1-\mu}  \nonumber\end{aligned}\]</span></p>
<p>We can recover the mean parameter <span class="math inline">\(\mu\)</span> from the canonical parameter using <span class="math display">\[\mu=\mathrm{sigm}(\theta)=\dfrac{1}{1+e^{-\theta}}\]</span></p>
<h4 id="multinoulli">Multinoulli</h4>
<p>We can represent the multinoulli as a minimal exponential family as follows: <span class="math display">\[\begin{split}
&amp; \mathrm{Cat}(\vec{x}|\vec{\mu}) = \prod\limits_{k=1}^K = \exp\left(\sum\limits_{k=1}^K x_k\log\mu_k\right) \\
&amp; = \exp\left[\sum\limits_{k=1}^{K-1} x_k\log\mu_k+  (1-\sum\limits_{k=1}^{K-1} x_k)\log(1-\sum\limits_{k=1}^{K-1} \mu_k)\right] \\
&amp; = \exp\left[\sum\limits_{k=1}^{K-1} x_k\log\dfrac{\mu_k}{1-\sum_{k=1}^{K-1} \mu_k} + \log(1-\sum\limits_{k=1}^{K-1} \mu_k) \right] \\
&amp; = \exp\left[\sum\limits_{k=1}^{K-1} x_k\log\dfrac{\mu_k}{\mu_K}+\log\mu_K\right] \text{, where } \mu_K \triangleq 1-\sum\limits_{k=1}^{K-1} \mu_k
\end{split}\]</span></p>
<p>We can write this in exponential family form as follows: <span class="math display">\[\begin{aligned}
\mathrm{Cat}(\vec{x}|\vec{\mu}) &amp; = \exp[\vec{\theta}^T\phi(\vec{x})-A(\vec{\theta})] \\
\vec{\theta} &amp; \triangleq (\log\dfrac{\mu_1}{\mu_K},\cdots,\log\dfrac{\mu_{K-1}}{\mu_K}) \\
\phi(\vec{x}) &amp; \triangleq (x_1,\cdots,x_{K-1})\end{aligned}\]</span></p>
<p>We can recover the mean parameters from the canonical parameters using <span class="math display">\[\begin{aligned}
\mu_k &amp; = \dfrac{e^{\theta_k}}{1+\sum_{j=1}^{K-1} e^{\theta_j}} \\
\mu_K &amp; = 1- \dfrac{\sum_{j=1}^{K-1} e^{\theta_j}}{1+\sum_{j=1}^{K-1} e^{\theta_j}}=\dfrac{1}{1+\sum_{j=1}^{K-1} e^{\theta_j}}\end{aligned}\]</span> and hence <span class="math display">\[A(\vec{\theta]} = -\log\mu_K=\log(1+\sum\limits_{j=1}^{K-1} e^{\theta_j})\]</span></p>
<h4 id="univariate-gaussian">Univariate Gaussian</h4>
<p>The univariate Gaussian can be written in exponential family form as follows: <span class="math display">\[\begin{aligned}
\mathcal{N}(x|\mu,\sigma^2) &amp; =\dfrac{1}{\sqrt{2\pi}\sigma}\exp\left[-\dfrac{1}{2\sigma^2}(x-\mu)^2\right] \nonumber \\
&amp; = \dfrac{1}{\sqrt{2\pi}\sigma}\exp\left[-\dfrac{1}{2\sigma^2}x^2+\dfrac{\mu}{\sigma^2}x-\dfrac{1}{2\sigma^2}\mu^2\right] \nonumber \\
&amp; = \dfrac{1}{Z(\vec{\theta})}\exp[\vec{\theta}^T\phi(x)]\end{aligned}\]</span> where <span class="math display">\[\begin{aligned}
\vec{\theta} &amp; = (\dfrac{\mu}{\sigma^2}, -\dfrac{1}{2\sigma^2}) \\
\phi(x) &amp; =(x,x^2) \\
Z(\vec{\theta}) &amp; =\sqrt{2\pi}\sigma\exp(\dfrac{\mu^2}{2\sigma^2})\end{aligned}\]</span></p>
<h4 id="non-examples">Non-examples</h4>
<p>Not all distributions of interest belong to the exponential family. For example, the uniform distribution,<span class="math inline">\(X \sim U(a,b)\)</span>, does not, since the support of the distribution depends on the parameters. Also, the Student T distribution (Section TODO) does not belong, since it does not have the required form.</p>
<h3 id="log-partition-function">Log partition function</h3>
<p>An important property of the exponential family is that derivatives of the log partition function can be used to generate <strong>cumulants</strong> of the sufficient statistics.<a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a> For this reason, <span class="math inline">\(A(\vec{\theta})\)</span> is sometimes called a <strong>cumulant function</strong>. We will prove this for a 1-parameter distribution; this can be generalized to a <span class="math inline">\(K\)</span>-parameter distribution in a straightforward way. For the first derivative we have</p>
<p>For the second derivative we have <span class="math display">\[\begin{aligned}
\dfrac{\mathrm{d} A}{\mathrm{d} \theta} &amp; = \dfrac{\mathrm{d}}{\mathrm{d} \theta}\left\{\log\int\exp\left[\theta\phi(x)\right]h(x)\mathrm{d}x\right\} \nonumber \\
&amp; = \dfrac{\frac{\mathrm{d}}{\mathrm{d} \theta}\int\exp\left[\theta\phi(x)\right]h(x)\mathrm{d}x}{\int\exp\left[\theta\phi(x)\right]h(x)\mathrm{d}x} \nonumber \\
&amp; = \dfrac{\int\phi(x)exp\left[\theta\phi(x)\right]h(x)\mathrm{d}x}{\exp(A(\theta))} \nonumber \\
&amp; = \int \phi(x)\exp\left[\theta\phi(x)-A(\theta)\right]h(x)\mathrm{d}x \nonumber \\
&amp; = \int \phi(x)p(x)\mathrm{d}x=\mathbb{E}[\phi(x)]\end{aligned}\]</span></p>
<p>For the second derivative we have <span class="math display">\[\begin{aligned}
\dfrac{\mathrm{d}^2 A}{\mathrm{d} \theta^2} &amp; = \int \phi(x)\exp\left[\theta\phi(x)-A(\theta)\right]h(x)\left[\phi(x)-A&#39;(\theta)\right]\mathrm{d}x \nonumber \\
&amp; = \int \phi(x)p(x)\left[\phi(x)-A&#39;(\theta)\right]\mathrm{d}x \nonumber \\
&amp; = \int \phi^2(x)p(x)\mathrm{d}x-A&#39;(\theta)\int \phi(x)p(x)\mathrm{d}x \nonumber \\
&amp; = \mathbb{E}[\phi^2(x)]-\mathbb{E}[\phi(x)]^2=\mathrm{var}[\phi(x)]\end{aligned}\]</span></p>
<p>In the multivariate case, we have that <span class="math display">\[\dfrac{\partial^2 A}{\partial \theta_i \partial \theta_j}=\mathbb{E}[\phi_i(x)\phi_j(x)]-\mathbb{E}[\phi_i(x)]\mathbb{E}[\phi_j(x)]\]</span> and hence <span class="math display">\[\nabla^2A(\vec{\theta}) = \mathrm{cov}[\phi(\vec{x})]\]</span></p>
<p>Since the covariance is positive definite, we see that <span class="math inline">\(A(\vec{\theta})\)</span> is a convex function (see Section [sec:Convexity]).</p>
<h3 id="mle-for-the-exponential-family">MLE for the exponential family</h3>
<p>The likelihood of an exponential family model has the form <span class="math display">\[p(\mathcal{D}|\vec{\theta})=\left[\prod\limits_{i=1}^N h(\vec{x}_i)\right]g(\vec{\theta})^N\exp\left[\vec{\theta}^T\left(\sum\limits_{i=1}^N \phi(\vec{x}_i)\right)\right]\]</span></p>
<p>We see that the sufficient statistics are <span class="math inline">\(N\)</span> and <span class="math display">\[\phi(\mathcal{D})=\sum\limits_{i=1}^N \phi(\vec{x}_i)=(\sum\limits_{i=1}^N \phi_1(\vec{x}_i),\cdots,\sum\limits_{i=1}^N \phi_K(\vec{x}_i))\]</span></p>
<p>The <strong>Pitman-Koopman-Darmois theorem</strong> states that, under certain regularity conditions, the exponential family is the only family of distributions with finite sufficient statistics. (Here, finite means of a size independent of the size of the data set.)</p>
<p>One of the conditions required in this theorem is that the support of the distribution not be dependent on the parameter.</p>
<h3 id="sec:Bayes-for-the-exponential-family">Bayes for the exponential family</h3>
<p>TODO</p>
<h4 id="likelihood">Likelihood</h4>
<h3 id="sec:Maximum-entropy-derivation-of-the-exponential-family">Maximum entropy derivation of the exponential family *</h3>
<h2 id="nonparametric-methods">Nonparametric Methods</h2>
<h3 id="kernel-density-estimators">Kernel density estimators</h3>
<h3 id="nearest-neighbour-methods">Nearest-neighbour methods</h3>
<h2 id="joint-probability-distributions">Joint probability distributions</h2>
<p>Given a <strong>multivariate random variable</strong> or <strong>random vector</strong> <a href="#fn5" class="footnoteRef" id="fnref5"><sup>5</sup></a> <span class="math inline">\(X \in \mathbb{R}^D\)</span>, the <strong>joint probability distribution</strong><a href="#fn6" class="footnoteRef" id="fnref6"><sup>6</sup></a> is a probability distribution that gives the probability that each of <span class="math inline">\(X_1, X_2, \cdots,X_D\)</span> falls in any particular range or discrete set of values specified for that variable. In the case of only two random variables, this is called a <strong>bivariate distribution</strong>, but the concept generalizes to any number of random variables, giving a <strong>multivariate distribution</strong>.</p>
<p>The joint probability distribution can be expressed either in terms of a <strong>joint cumulative distribution function</strong> or in terms of a <strong>joint probability density function</strong> (in the case of continuous variables) or <strong>joint probability mass function</strong> (in the case of discrete variables).</p>
<h3 id="covariance-and-correlation">Covariance and correlation</h3>
<p>The <strong>covariance</strong> between two rv’s <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> measures the degree to which <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are (linearly) related. Covariance is defined as <span class="math display">\[\begin{split}
\mathrm{cov}[X,Y] &amp; \triangleq \mathbb{E}\left[(X-\mathbb{E}[X])(Y-\mathbb{E}[Y])\right] \\
         &amp; =\mathbb{E}[XY]-\mathbb{E}[X]\mathbb{E}[Y]
\end{split}\]</span></p>
<p>If <span class="math inline">\(X\)</span> is a <span class="math inline">\(D\)</span>-dimensional random vector, its <strong>covariance matrix</strong> is defined to be the following symmetric, positive definite matrix: <span class="math display">\[\begin{aligned}
\mathrm{cov}[\vec{x}] &amp; \triangleq \mathbb{E}\left[(\vec{x}-\mathbb{E}[\vec{x}])(\vec{x}-\mathbb{E}[\vec{x}])^T\right] \\
       &amp;  = \left( \begin{array}{cccc}
           \text{var}[\vec{x_1}] &amp; \text{Cov}[\vec{x_1},\vec{x_2}] &amp; \cdots &amp; \text{Cov}[\vec{x_1},\vec{x_D}] \\
           \text{Cov}[\vec{x_2},\vec{x_1}] &amp; \text{var}[x_2] &amp; \cdots &amp; \text{Cov}[\vec{x_2},\vec{x_D}] \\
           \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
           \text{Cov}[\vec{x_D},\vec{x_1}] &amp; \text{Cov}[\vec{x_D},\vec{x_2}] &amp; \cdots &amp; \text{var}[\vec{x_D}] \end{array} \right)\end{aligned}\]</span></p>
<p>For</p>
<p>The (Pearson) <strong>correlation coefficient</strong> between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is defined as <span class="math display">\[\text{corr}[X,Y] \triangleq \dfrac{\text{Cov}[X,Y]}{\sqrt{\text{var}[X],\text{var}[Y]}}\]</span></p>
<p>A <strong>correlation matrix</strong> has the form <span class="math display">\[\mathbf{R} \triangleq \left( \begin{array}{cccc}
           \text{corr}[X_1,X_1] &amp; \text{corr}[X_1,X_2] &amp; \cdots &amp; \text{corr}[X_1,X_D] \\
           \text{corr}[X_2,X_1] &amp; \text{corr}[X_2,X_2] &amp; \cdots &amp; \text{corr}[X_2,X_D] \\
           \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
           \text{corr}[X_D,X_1] &amp; \text{corr}[X_D,X_2] &amp; \cdots &amp; \text{corr}[X_D,X_D] \end{array} \right)\]</span></p>
<p>The correlation coefficient can viewed as a degree of linearity between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, see Figure [fig:Correlation-examples].</p>
<p>[hbtp] <img src="Correlation-examples.png" alt="image" /></p>
<p><strong>Uncorrelated does not imply independent</strong>. For example, let <span class="math inline">\(X \sim U(-1,1)\)</span> and <span class="math inline">\(Y =X^2\)</span>. Clearly <span class="math inline">\(Y\)</span> is dependent on <span class="math inline">\(X\)</span>(in fact, <span class="math inline">\(Y\)</span> is uniquely determined by <span class="math inline">\(X\)</span>), yet one can show that corr<span class="math inline">\([X, Y]=0\)</span>. Some striking examples of this fact are shown in Figure [fig:Correlation-examples]. This shows several data sets where there is clear dependence between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, and yet the correlation coefficient is 0. A more general measure of dependence between random variables is mutual information, see Section TODO.</p>
<h3 id="sec:MVN">Multivariate Gaussian distribution</h3>
<p>The <strong>multivariate Gaussian</strong> or <strong>multivariate normal</strong>(MVN) is the most widely used joint probability density function for continuous variables. We discuss MVNs in detail in Chapter 4; here we just give some definitions and plots.</p>
<p>The pdf of the MVN in <span class="math inline">\(D\)</span> dimensions is defined by the following: <span class="math display">\[\mathcal{N}(\vec{x}|\vec{\mu},\Sigma) \triangleq \dfrac{1}{(2\pi)^{D/2}|\Sigma|^{1/2}}\exp\left[-\dfrac{1}{2}(\vec{x}-\vec{\mu})^T\Sigma^{-1}(\vec{x}-\vec{\mu})\right]\]</span> where <span class="math inline">\(\vec{\mu}=\mathbb{E}[X] \in \mathbb{R}^D\)</span> is the mean vector, and <span class="math inline">\(\Sigma=\text{Cov}[X]\)</span> is the <span class="math inline">\(D \times D\)</span> covariance matrix. The normalization constant <span class="math inline">\((2\pi)^{D/2}|\Sigma|^{1/2}\)</span> just ensures that the pdf integrates to 1.</p>
<p>Figure [fig:2d-Gaussions] plots some MVN densities in 2d for three different kinds of covariance matrices. A full covariance matrix has A <span class="math inline">\(D(D+1)/2\)</span> parameters (we divide by 2 since <span class="math inline">\(\Sigma\)</span> is symmetric). A diagonal covariance matrix has <span class="math inline">\(D\)</span> parameters, and has 0s in the off-diagonal terms. A spherical or isotropic covariance,<span class="math inline">\(\Sigma=\sigma^2\vec{I}_D\)</span>, has one free parameter.</p>
<p><br />
<br />
<br />
</p>
<h3 id="multivariate-students-t-distribution">Multivariate Student’s t-distribution</h3>
<p>A more robust alternative to the MVN is the multivariate Student’s t-distribution, whose pdf is given by <span class="math display">\[\begin{aligned}
&amp; \mathcal{T}(x|\vec{\mu},\Sigma,\nu) \nonumber \\
&amp; \triangleq \dfrac{\Gamma(\frac{\nu+D}{2})}{\Gamma(\frac{\nu}{2})}\dfrac{|\Sigma|^{-\frac{1}{2}}}{\left(\nu\pi\right)^{\frac{D}{2}}}\left[1+\dfrac{1}{\nu}\left(\vec{x}-\vec{\mu}\right)^T\Sigma^{-1}\left(\vec{x}-\vec{\mu}\right)\right]^{-\frac{\nu+D}{2}} \\
&amp;= \dfrac{\Gamma(\frac{\nu+D}{2})}{\Gamma(\frac{\nu}{2})}\dfrac{|\Sigma|^{-\frac{1}{2}}}{\left(\nu\pi\right)^{\frac{D}{2}}}\left[1+\left(\vec{x}-\vec{\mu}\right)^T\vec{V}^{-1}\left(\vec{x}-\vec{\mu}\right)\right]^{-\frac{\nu+D}{2}}\end{aligned}\]</span> where <span class="math inline">\(\Sigma\)</span> is called the scale matrix (since it is not exactly the covariance matrix) and <span class="math inline">\(\vec{V}=\nu\Sigma\)</span>. This has fatter tails than a Gaussian. The smaller <span class="math inline">\(\nu\)</span> is, the fatter the tails. As <span class="math inline">\(\nu \rightarrow \infty\)</span>, the distribution tends towards a Gaussian. The distribution has the following properties <span class="math display">\[\text{mean}=\vec{\mu} \text{ , mode}=\vec{\mu} \text{ , Cov}= \dfrac{\nu}{\nu-2}\Sigma\]</span></p>
<h3 id="dirichlet-distribution">Dirichlet distribution</h3>
<p>A multivariate generalization of the beta distribution is the <strong>Dirichlet distribution</strong>, which has support over the probability simplex, defined by <span class="math display">\[S_K=\left\{\vec{x}:0 \leq x_k \leq 1,\sum\limits_{k=1}^K x_k=1\right\}\]</span></p>
<p>The pdf is defined as follows: <span class="math display">\[\text{Dir}(\vec{x}|\vec{\alpha}) \triangleq \dfrac{1}{B(\vec{\alpha})}\prod\limits_{k=1}^K x_k^{\alpha_k-1}\mathbb{I}(\vec{x} \in S_K)\]</span> where <span class="math inline">\(B(\alpha_1,\alpha_2,\cdots,\alpha_K)\)</span> is the natural generalization of the beta function to <span class="math inline">\(K\)</span> variables: <span class="math display">\[B(\alpha) \triangleq \dfrac{\prod_{k=1}^K \Gamma(\alpha_k)}{\Gamma(\alpha_0)} \text{ where } \alpha_0 \triangleq \sum_{k=1}^K \alpha_k\]</span></p>
<p>Figure [fig:3d-Dirichlet] shows some plots of the Dirichlet when <span class="math inline">\(K=3\)</span>, and Figure [fig:5d-Dirichlet] for some sampled probability vectors. We see that <span class="math inline">\(\alpha_0\)</span> controls the strength of the distribution (how peaked it is), and theαkcontrol where the peak occurs. For example, Dir<span class="math inline">\((1,1,1)\)</span> is a uniform distribution, Dir<span class="math inline">\((2,2,2)\)</span> is a broad distribution centered at <span class="math inline">\((1/3,1/3,1/3)\)</span>, and Dir<span class="math inline">\((20,20,20)\)</span> is a narrow distribution centered at <span class="math inline">\((1/3,1/3,1/3)\)</span>.If <span class="math inline">\(\alpha_k &lt; 1\)</span> for all <span class="math inline">\(k\)</span>, we get “spikes” at the corner of the simplex.</p>
<p><br />
<br />
<br />
</p>
<p><br />
</p>
<p>For future reference, the distribution has these properties <span class="math display">\[\label{eqn:Dirichlet-properties}
\mathbb{E}(x_k)=\dfrac{\alpha_k}{\alpha_0} \text{, mode}[x_k]=\dfrac{\alpha_k-1}{\alpha_0-K} \text{, var}[x_k]=\dfrac{\alpha_k(\alpha_0-\alpha_k)}{\alpha_0^2(\alpha_0+1)}\]</span></p>
<h2 id="transformations-of-random-variables">Transformations of random variables</h2>
<p>If <span class="math inline">\(\vec{x} \sim P()\)</span> is some random variable, and <span class="math inline">\(\vec{y}=f(\vec{x})\)</span>, what is the distribution of <span class="math inline">\(Y\)</span>? This is the question we address in this section.</p>
<h3 id="linear-transformations">Linear transformations</h3>
<p>Suppose <span class="math inline">\(g()\)</span> is a linear function: <span class="math display">\[g(\vec{x})=A\vec{x}+b\]</span></p>
<p>First, for the mean, we have <span class="math display">\[\mathbb{E}[\vec{y}]=\mathbb{E}[A\vec{x}+b]=A\mathbb{E}[\vec{x}]+b\]</span> this is called the <strong>linearity of expectation</strong>.</p>
<p>For the covariance, we have <span class="math display">\[\text{Cov}[\vec{y}]=\text{Cov}[A\vec{x}+b]=A\Sigma A^T\]</span></p>
<h3 id="sec:General-transformations">General transformations</h3>
<p>If <span class="math inline">\(X\)</span> is a discrete rv, we can derive the pmf for <span class="math inline">\(y\)</span> by simply summing up the probability mass for all the <span class="math inline">\(x\)</span>’s such that <span class="math inline">\(f(x)=y\)</span>: <span class="math display">\[\label{eqn:transformation-discrete}
p_Y(y)=\sum\limits_{x:g(x)=y}p_X(x)\]</span></p>
<p>If <span class="math inline">\(X\)</span> is continuous, we cannot use Equation [eqn:transformation-discrete] since <span class="math inline">\(p_X(x)\)</span> is a density, not a pmf, and we cannot sum up densities. Instead, we work with cdf’s, and write <span class="math display">\[F_Y(y)=P(Y \leq y)=P(g(X) \leq y)=\int\limits_{g(X) \leq y} f_X(x)\mathrm{d}x\]</span></p>
<p>We can derive the pdf of <span class="math inline">\(Y\)</span> by differentiating the cdf: <span class="math display">\[\label{eqn:General-transformations}
f_Y(y)=f_X(x)|\dfrac{dx}{dy}|\]</span></p>
<p>This is called <strong>change of variables</strong> formula. We leave the proof of this as an exercise.</p>
<p>For example, suppose <span class="math inline">\(X \sim U(−1,1)\)</span>, and <span class="math inline">\(Y=X^2\)</span>. Then <span class="math inline">\(p_Y(y)=\dfrac{1}{2}y^{-\frac{1}{2}}\)</span>.</p>
<h4 id="multivariate-change-of-variables">Multivariate change of variables *</h4>
<p>Let <span class="math inline">\(f\)</span> be a function <span class="math inline">\(f:\mathbb{R}^n \rightarrow \mathbb{R}^n\)</span>, and let <span class="math inline">\(\vec{y}=f(\vec{x})\)</span>. Then its Jacobian matrix <span class="math inline">\(\vec{J}\)</span> is given by <span class="math display">\[\vec{J}_{\vec{x} \rightarrow \vec{y}} \triangleq \frac{\partial \vec{y}}{\partial \vec{x}} \triangleq \left(\begin{array}{ccc}
\frac{\partial y_1}{\partial x_1} &amp; \cdots &amp; \frac{\partial y_1}{\partial x_n} \\
\vdots &amp; \vdots &amp; \vdots \\
\frac{\partial y_n}{\partial x_1} &amp; \cdots &amp; \frac{\partial y_n}{\partial x_n}
\end{array}\right)\]</span> <span class="math inline">\(|\mathrm{det}(\vec{J})|\)</span> measures how much a unit cube changes in volume when we apply <span class="math inline">\(f\)</span>.</p>
<p>If <span class="math inline">\(f\)</span> is an invertible mapping, we can define the pdf of the transformed variables using the Jacobian of the inverse mapping <span class="math inline">\(\vec{y} \rightarrow \vec{x}\)</span>: <span class="math display">\[\label{eqn:Multivariate-transformation}
p_y(\vec{y})=p_x(\vec{x})|\mathrm{det}(\frac{\partial \vec{x}}{\partial \vec{y}})|=p_x(\vec{x})|\mathrm{det}(\vec{J}_{\vec{y} \rightarrow \vec{x}})|\]</span></p>
<h3 id="central-limit-theorem">Central limit theorem</h3>
<p>Given <span class="math inline">\(N\)</span> random variables <span class="math inline">\(X_1,X_2,\cdots,X_N\)</span>, each variable is <strong>independent and identically distributed</strong><a href="#fn7" class="footnoteRef" id="fnref7"><sup>7</sup></a>(<strong>iid</strong> for short), and each has the same mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>, then <span class="math display">\[\dfrac{\sum\limits_{i=1}^n X_i-N\mu}{\sqrt{N}\sigma} \sim \mathcal{N}(0,1)\]</span> this can also be written as <span class="math display">\[\dfrac{\bar{X}-\mu}{\sigma/\sqrt{N}} \sim \mathcal{N}(0,1) \quad \text{, where } \bar{X} \triangleq \dfrac{1}{N}\sum\limits_{i=1}^n X_i\]</span></p>
<h2 id="sec:Monte-Carlo-approximation">Monte Carlo approximation</h2>
<p>In general, computing the distribution of a function of an rv using the change of variables formula can be difficult. One simple but powerful alternative is as follows. First we generate <span class="math inline">\(S\)</span> samples from the distribution, call them <span class="math inline">\(x_1,\cdots,x_S\)</span>. (There are many ways to generate such samples; one popular method, for high dimensional distributions, is called Markov chain Monte Carlo or MCMC; this will be explained in Chapter TODO.) Given the samples, we can approximate the distribution of <span class="math inline">\(f(X)\)</span> by using the empirical distribution of <span class="math inline">\(\left\{f(x_s)\right\}_{s=1}^S\)</span>. This is called a <strong>Monte Carlo approximation</strong><a href="#fn8" class="footnoteRef" id="fnref8"><sup>8</sup></a>, named after a city in Europe known for its plush gambling casinos.</p>
<p>We can use Monte Carlo to approximate the expected value of any function of a random variable. We simply draw samples, and then compute the arithmetic mean of the function applied to the samples. This can be written as follows: <span class="math display">\[\mathbb{E}[g(X)]=\int g(x)p(x)\mathrm{d}x \approx \dfrac{1}{S}\sum\limits_{s=1}^S f(x_s)\]</span> where <span class="math inline">\(x_s \sim p(X)\)</span>.</p>
<p>This is called <strong>Monte Carlo integration</strong><a href="#fn9" class="footnoteRef" id="fnref9"><sup>9</sup></a>, and has the advantage over numerical integration (which is based on evaluating the function at a fixed grid of points) that the function is only evaluated in places where there is non-negligible probability.</p>
<h2 id="information-theory">Information theory</h2>
<h3 id="sec:Entropy">Entropy</h3>
<p>The entropy of a random variable <span class="math inline">\(X\)</span> with distribution <span class="math inline">\(p\)</span>, denoted by <span class="math inline">\(\mathbb{H}(X)\)</span> or sometimes <span class="math inline">\(\mathbb{H}(p)\)</span>, is a measure of its uncertainty. In particular, for a discrete variable with <span class="math inline">\(K\)</span> states, it is defined by <span class="math display">\[\mathbb{H}(X) \triangleq -\sum\limits_{k=1}^{K}{p(X=k)\log_2p(X=k)}\]</span></p>
<p>Usually we use log base 2, in which case the units are called <strong>bits</strong>(short for binary digits). If we use log base <span class="math inline">\(e\)</span> , the units are called <strong>nats</strong>.</p>
<p>The discrete distribution with maximum entropy is the uniform distribution (see Section XXX for a proof). Hence for a K-ary random variable, the entropy is maximized if <span class="math inline">\(p(x = k)=1/K\)</span>; in this case, <span class="math inline">\(\mathbb{H}(X)=\log_2K\)</span>.</p>
<p>Conversely, the distribution with minimum entropy (which is zero) is any <strong>delta-function</strong> that puts all its mass on one state. Such a distribution has no uncertainty.</p>
<h3 id="kl-divergence">KL divergence</h3>
<p>One way to measure the dissimilarity of two probability distributions, <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span> , is known as the <strong>Kullback-Leibler divergence</strong>(<strong>KL divergence</strong>)or <strong>relative entropy</strong>. This is defined as follows: <span class="math display">\[\mathbb{KL}(P||Q) \triangleq 
\sum\limits_{x}{p(x)\log_2\dfrac{p(x)}{q(x)}}\]</span> where the sum gets replaced by an integral for pdfs<a href="#fn10" class="footnoteRef" id="fnref10"><sup>10</sup></a>. The KL divergence is only defined if P and Q both sum to 1 and if <span class="math inline">\(q(x)=0\)</span> implies <span class="math inline">\(p(x)=0\)</span> for all <span class="math inline">\(x\)</span>(absolute continuity). If the quantity <span class="math inline">\(0\ln0\)</span> appears in the formula, it is interpreted as zero because <span class="math inline">\(\lim\limits_{x \to 0}x\ln x\)</span>. We can rewrite this as <span class="math display">\[\begin{split}
\mathbb{KL}(p||q) &amp; \triangleq \sum\limits_{x}{p(x)\log_2p(x)}-\sum\limits_{k=1}^{K}{p(x)\log_2q(x)} \\
    &amp; =\mathbb{H}(p)-\mathbb{H}(p,q)
\end{split}\]</span> where <span class="math inline">\(\mathbb{H}(p,q)\)</span> is called the <strong>cross entropy</strong>, <span class="math display">\[\label{eqn:cross-entropy}
\mathbb{H}(p,q)=\sum\limits_{x}{p(x)\log_2q(x)}\]</span></p>
<p>One can show (Cover and Thomas 2006) that the cross entropy is the average number of bits needed to encode data coming from a source with distribution <span class="math inline">\(p\)</span> when we use model <span class="math inline">\(q\)</span> to define our codebook. Hence the “regular” entropy <span class="math inline">\(\mathbb{H}(p)=\mathbb{H}(p,p)\)</span>, defined in section §[sec:Entropy],is the expected number of bits if we use the true model, so the KL divergence is the diference between these. In other words, the KL divergence is the average number of <em>extra</em> bits needed to encode the data, due to the fact that we used distribution <span class="math inline">\(q\)</span> to encode the data instead of the true distribution <span class="math inline">\(p\)</span>.</p>
<p>The “extra number of bits” interpretation should make it clear that <span class="math inline">\(\mathbb{KL}(p||q) \geq 0\)</span>, and that the KL is only equal to zero if <span class="math inline">\(q = p\)</span>. We now give a proof of this important result.</p>
<p>(<strong>Information inequality</strong>) <span class="math inline">\(\mathbb{KL}(p||q) \geq 0 \text{ with equality iff } p=q\)</span>.</p>
<p>One important consequence of this result is that <em>the discrete distribution with the maximum entropy is the uniform distribution</em>.</p>
<h3 id="sec:Mutual-information">Mutual information</h3>
<p><strong>Mutual information</strong> or <strong>MI</strong>, is defined as follows: <span class="math display">\[\begin{split}
\mathbb{I}(X;Y) &amp; \triangleq \mathbb{KL}(P(X,Y)||P(X)P(X)) \\
    &amp; =\sum\limits_x\sum\limits_yp(x,y)\log\dfrac{p(x,y)}{p(x)p(y)}
\end{split}\]</span> We have <span class="math inline">\(\mathbb{I}(X;Y) \geq 0\)</span> with equality if <span class="math inline">\(P(X,Y)=P(X)P(Y)\)</span>. That is, the MI is zero if the variables are independent.</p>
<p>To gain insight into the meaning of MI, it helps to re-express it in terms of joint and conditional entropies. One can show that the above expression is equivalent to the following: <span class="math display">\[\begin{aligned}
\mathbb{I}(X;Y)&amp;=&amp;\mathbb{H}(X)-\mathbb{H}(X|Y)\\
               &amp;=&amp;\mathbb{H}(Y)-\mathbb{H}(Y|X)\\
               &amp;=&amp;\mathbb{H}(X)+\mathbb{H}(Y)-\mathbb{H}(X,Y)\\
               &amp;=&amp;\mathbb{H}(X,Y)-\mathbb{H}(X|Y)-\mathbb{H}(Y|X)\end{aligned}\]</span> where <span class="math inline">\(\mathbb{H}(X)\)</span> and <span class="math inline">\(\mathbb{H}(Y)\)</span> are the <strong>marginal entropies</strong>, <span class="math inline">\(\mathbb{H}(X|Y)\)</span> and <span class="math inline">\(\mathbb{H}(Y|X)\)</span> are the <strong>conditional entropies</strong>, and <span class="math inline">\(\mathbb{H}(X,Y)\)</span> is the <strong>joint entropy</strong> of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, see Fig. [fig:mi]<a href="#fn11" class="footnoteRef" id="fnref11"><sup>11</sup></a>.</p>
<div class="figure">
<img src="mutual-information.png" alt="Individual \mathbb{H}(X),\mathbb{H}(Y), joint \mathbb{H}(X,Y), and conditional entropies for a pair of correlated subsystems X,Y with mutual information \mathbb{I}(X;Y)." />
<p class="caption">Individual <span class="math inline">\(\mathbb{H}(X),\mathbb{H}(Y)\)</span>, joint <span class="math inline">\(\mathbb{H}(X,Y)\)</span>, and conditional entropies for a pair of correlated subsystems <span class="math inline">\(X,Y\)</span> with mutual information <span class="math inline">\(\mathbb{I}(X;Y)\)</span>.<span data-label="fig:mi"></span></p>
</div>
<p>Intuitively, we can interpret the MI between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> as the reduction in uncertainty about <span class="math inline">\(X\)</span> after observing <span class="math inline">\(Y\)</span>, or, by symmetry, the reduction in uncertainty about <span class="math inline">\(Y\)</span> after observing <span class="math inline">\(X\)</span>.</p>
<p>A quantity which is closely related to MI is the <strong>pointwise mutual information</strong> or <strong>PMI</strong>. For two events (not random variables) <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, this is defined as <span class="math display">\[PMI(x,y) \triangleq \log\dfrac{p(x,y)}{p(x)p(y)}=\log\dfrac{p(x|y)}{p(x)}=\log\dfrac{p(y|x)}{p(y)}\]</span></p>
<p>This measures the discrepancy between these events occuring together compared to what would be expected by chance. Clearly the MI of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is just the expected value of the PMI. Interestingly, we can rewrite the PMI as follows: <span class="math display">\[PMI(x,y)=\log\dfrac{p(x|y)}{p(x)}=\log\dfrac{p(y|x)}{p(y)}\]</span></p>
<p>This is the amount we learn from updating the prior <span class="math inline">\(p(x)\)</span> into the posterior <span class="math inline">\(p(x|y)\)</span> , or equivalently, updating the prior <span class="math inline">\(p(y)\)</span> into the posterior <span class="math inline">\(p(y |x)\)</span> .</p>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p><a href="http://en.wikipedia.org/wiki/Probability_mass_function" class="uri">http://en.wikipedia.org/wiki/Probability_mass_function</a><a href="#fnref1">↩</a></p></li>
<li id="fn2"><p><a href="http://en.wikipedia.org/wiki/Probability_density_function" class="uri">http://en.wikipedia.org/wiki/Probability_density_function</a><a href="#fnref2">↩</a></p></li>
<li id="fn3"><p><a href="http://en.wikipedia.org/wiki/Empirical_distribution_function" class="uri">http://en.wikipedia.org/wiki/Empirical_distribution_function</a><a href="#fnref3">↩</a></p></li>
<li id="fn4"><p>The first and second cumulants of a distribution are its mean <span class="math inline">\(\mathbb{E}[X]\)</span> and variance <span class="math inline">\(\mathrm{var}[X]\)</span>, whereas the first and second moments are its mean <span class="math inline">\(\mathbb{E}[X]\)</span> and <span class="math inline">\(\mathbb{E}[X^2]\)</span>.<a href="#fnref4">↩</a></p></li>
<li id="fn5"><p><a href="http://en.wikipedia.org/wiki/Multivariate_random_variable" class="uri">http://en.wikipedia.org/wiki/Multivariate_random_variable</a><a href="#fnref5">↩</a></p></li>
<li id="fn6"><p><a href="http://en.wikipedia.org/wiki/Joint_probability_distribution" class="uri">http://en.wikipedia.org/wiki/Joint_probability_distribution</a><a href="#fnref6">↩</a></p></li>
<li id="fn7"><p><a href="http://en.wikipedia.org/wiki/Independent_identically_distributed" class="uri">http://en.wikipedia.org/wiki/Independent_identically_distributed</a><a href="#fnref7">↩</a></p></li>
<li id="fn8"><p><a href="http://en.wikipedia.org/wiki/Monte_Carlo_method" class="uri">http://en.wikipedia.org/wiki/Monte_Carlo_method</a><a href="#fnref8">↩</a></p></li>
<li id="fn9"><p><a href="http://en.wikipedia.org/wiki/Monte_Carlo_integration" class="uri">http://en.wikipedia.org/wiki/Monte_Carlo_integration</a><a href="#fnref9">↩</a></p></li>
<li id="fn10"><p>The KL divergence is not a distance, since it is asymmetric. One symmetric version of the KL divergence is the <strong>Jensen-Shannon divergence</strong>, defined as <span class="math inline">\(JS(p_1,p_2)=0.5\mathbb{KL}(p_1||q)+0.5\mathbb{KL}(p_2||q)\)</span>,where <span class="math inline">\(q=0.5p_1+0.5p_2\)</span><a href="#fnref10">↩</a></p></li>
<li id="fn11"><p><a href="http://en.wikipedia.org/wiki/Mutual_information" class="uri">http://en.wikipedia.org/wiki/Mutual_information</a><a href="#fnref11">↩</a></p></li>
</ol>
</div>
</body>
</html>
