<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>
<h1 id="generative-models-for-discrete-data">Generative models for discrete data</h1>
<h2 id="generative-classifier">Generative classifier</h2>
<p><span class="math display">\[\label{eqn:Generative-classifier}
p(y=c|\vec{x},\vec{\theta})=\dfrac{p(y=c|\vec{\theta})p(\vec{x}|y=c,\vec{\theta})}{\sum_{c&#39;}{p(y=c&#39;|\vec{\theta})p(\vec{x}|y=c&#39;,\vec{\theta})}}\]</span></p>
<p>This is called a <strong>generative classifier</strong>, since it specifies how to generate the data using the <strong>class conditional density</strong> <span class="math inline">\(p(\vec{x}|y=c)\)</span> and the class prior <span class="math inline">\(p(y=c)\)</span>. An alternative approach is to directly fit the class posterior, <span class="math inline">\(p(y=c|\vec{x})\)</span> ;this is known as a <strong>discriminative classifier</strong>.</p>
<h2 id="bayesian-concept-learning">Bayesian concept learning</h2>
<p>Psychological research has shown that people can learn concepts from positive examples alone (Xu and Tenenbaum 2007).</p>
<p>We can think of learning the meaning of a word as equivalent to <strong>concept learning</strong>, which in turn is equivalent to binary classification. To see this, define <span class="math inline">\(f(\vec{x})=1\)</span> if xis an example of the concept <span class="math inline">\(C\)</span>, and <span class="math inline">\(f(\vec{x})=0\)</span> otherwise. Then the goal is to learn the indicator function <span class="math inline">\(f\)</span>, which just defines which elements are in the set <span class="math inline">\(C\)</span>.</p>
<h3 id="likelihood">Likelihood</h3>
<p><span class="math display">\[p(\mathcal{D}|h) \triangleq \left(\dfrac{1}{\text{size}(h)}\right)^N=\left(\dfrac{1}{|h|}\right)^N\]</span></p>
<p>This crucial equation embodies what Tenenbaum calls the <strong>size principle</strong>, which means the model favours the simplest (smallest) hypothesis consistent with the data. This is more commonly known as <strong>Occam’s razor</strong><a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>.</p>
<h3 id="prior">Prior</h3>
<p>The prior is decided by human, not machines, so it is subjective. The subjectivity of the prior is controversial. For example, that a child and a math professor will reach different answers. In fact, they presumably not only have different priors, but also different hypothesis spaces. However, we can finesse that by defining the hypothesis space of the child and the math professor to be the same, and then setting the child’s prior weight to be zero on certain “advanced” concepts. Thus there is no sharp distinction between the prior and the hypothesis space.</p>
<p>However, the prior is the mechanism by which background knowledge can be brought to bear on a problem. Without this, rapid learning (i.e., from small samples sizes) is impossible.</p>
<h3 id="posterior">Posterior</h3>
<p>The posterior is simply the likelihood times the prior, normalized. <span class="math display">\[p(h|\mathcal{D}) \triangleq \dfrac{p(\mathcal{D}|h)p(h)}{\sum_{h&#39; \in \mathcal{H}}p(\mathcal{D}|h&#39;)p(h&#39;)}=\dfrac{\mathbb{I}(\mathcal{D} \in h)p(h)}{\sum_{h&#39; \in \mathcal{H}}\mathbb{I}(\mathcal{D} \in h&#39;)p(h&#39;)}\]</span> where <span class="math inline">\(\mathbb{I}(\mathcal{D} \in h)p(h)\)</span> is 1 <strong>iff</strong>(iff and only if) all the data are in the extension of the hypothesis <span class="math inline">\(h\)</span>.</p>
<p>In general, when we have enough data, the posterior <span class="math inline">\(p(h|\mathcal{D})\)</span> becomes peaked on a single concept, namely the MAP estimate, i.e., <span class="math display">\[p(h|\mathcal{D}) \rightarrow \hat{h}^{MAP}\]</span> where <span class="math inline">\(\hat{h}^{MAP}\)</span> is the posterior mode, <span class="math display">\[\begin{split}
\hat{h}^{MAP} &amp; \triangleq \arg\max\limits_h p(h|\mathcal{D})=\arg\max\limits_h p(\mathcal{D}|h)p(h) \\
    &amp; =\arg\max\limits_h [\log p(\mathcal{D}|h) + \log p(h)]
\end{split}\]</span></p>
<p>Since the likelihood term depends exponentially on <span class="math inline">\(N\)</span>, and the prior stays constant, as we get more and more data, the MAP estimate converges towards the <strong>maximum likelihood estimate</strong> or <strong>MLE</strong>: <span class="math display">\[\hat{h}^{MLE} \triangleq \arg\max\limits_h p(\mathcal{D}|h)=\arg\max\limits_h \log p(\mathcal{D}|h)\]</span></p>
<p>In other words, if we have enough data, we see that the <strong>data overwhelms the prior</strong>.</p>
<h3 id="posterior-predictive-distribution">Posterior predictive distribution</h3>
<p>The concept of <strong>posterior predictive distribution</strong><a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a> is normally used in a Bayesian context, where it makes use of the entire posterior distribution of the parameters given the observed data to yield a probability distribution over an interval rather than simply a point estimate. <span class="math display">\[p(\tilde{\vec{x}}|\mathcal{D}) \triangleq \mathbb{E}_{h|\mathcal{D}}[p(\tilde{\vec{x}}|h)] = \begin{cases}
\sum_h p(\tilde{\vec{x}}|h)p(h|\mathcal{D}) \\
\int p(\tilde{\vec{x}}|h)p(h|\mathcal{D})\mathrm{d}h
\end{cases}\]</span></p>
<p>This is just a weighted average of the predictions of each individual hypothesis and is called <strong>Bayes model averaging</strong>(Hoeting et al. 1999).</p>
<h2 id="the-beta-binomial-model">The beta-binomial model</h2>
<h3 id="likelihood-1">Likelihood</h3>
<p>Given <span class="math inline">\(X \sim \text{Bin}(\theta)\)</span>, the likelihood of <span class="math inline">\(\mathcal{D}\)</span> is given by <span class="math display">\[p(\mathcal{D}|\theta)= \text{Bin}(N_1|N,\theta)\]</span></p>
<h3 id="prior-1">Prior</h3>
<p><span class="math display">\[\text{Beta}(\theta|a,b) \propto \theta^{a-1}(1-\theta)^{b-1}\]</span></p>
<p>The parameters of the prior are called <strong>hyper-parameters</strong>.</p>
<h3 id="posterior-1">Posterior</h3>
<p><span class="math display">\[\begin{split}\label{eqn:beta-binomial-posterior}
p(\theta|\mathcal{D}) &amp; \propto \text{Bin}(N_1|N_1+N_0,\theta)\text{Beta}(\theta|a,b) \\
    &amp; =\text{Beta}(\theta|N_1+a,N_0b)
\end{split}\]</span></p>
<p>Note that updating the posterior sequentially is equivalent to updating in a single batch. To see this, suppose we have two data sets <span class="math inline">\(\mathcal{D}_a\)</span> and <span class="math inline">\(\mathcal{D}_b\)</span> with sufficient statistics <span class="math inline">\(N_1^a,N_0^a\)</span> and <span class="math inline">\(N_1^b,N_0^b\)</span>. Let <span class="math inline">\(N_1=N_1^a+N_1^b\)</span> and <span class="math inline">\(N_0=N_0^a+N_0^b\)</span> be the sufficient statistics of the combined datasets. In batch mode we have <span class="math display">\[\begin{aligned}
p(\theta|\mathcal{D}_a,\mathcal{D}_b)&amp; = p(\theta,\mathcal{D}_b|\mathcal{D}_a)p(\mathcal{D}_a) \\
               &amp;\propto p(\theta,\mathcal{D}_b|\mathcal{D}_a) \\
               &amp; = p(\mathcal{D}_b,\theta|\mathcal{D}_a) \\
               &amp; = p(\mathcal{D}_b|\theta)p(\theta|\mathcal{D}_a) \\
               &amp; \text{Combine Equation \ref{eqn:beta-binomial-posterior} and \ref{eqn:binomial-pmf}} \\
               &amp; =\text{Bin}(N_1^b|\theta, N_1^b+N_0^b)\text{Beta}(\theta|N_1^a+a,N_0^a+b) \\
               &amp; =\text{Beta}(\theta|N_1^a+N_1^b+a,N_0^a+N_0^b+b)\end{aligned}\]</span></p>
<p>This makes Bayesian inference particularly well-suited to <strong>online learning</strong>, as we will see later.</p>
<h4 id="sec:beta-binomial-Posterior-mean-and-mode">Posterior mean and mode</h4>
<p>From Table [tab:beta-distribution], the posterior mean is given by <span class="math display">\[\bar{\theta}=\dfrac{a+N_1}{a+b+N}\]</span></p>
<p>The mode is given by <span class="math display">\[\hat{\theta}_{MAP}=\dfrac{a+N_1-1}{a+b+N-2}\]</span></p>
<p>If we use a uniform prior, then the MAP estimate reduces to the MLE, <span class="math display">\[\hat{\theta}_{MLE}=\dfrac{N_1}{N}\]</span></p>
<p>We will now show that the posterior mean is convex combination of the prior mean and the MLE, which captures the notion that the posterior is a compromise between what we previously believed and what the data is telling us.</p>
<h4 id="posterior-variance">Posterior variance</h4>
<p>The mean and mode are point estimates, but it is useful to know how much we can trust them. The variance of the posterior is one way to measure this. The variance of the Beta posterior is given by <span class="math display">\[\text{var}(\theta|\mathcal{D})=\dfrac{(a+N_1)(b+N_0)}{(a+N_1+b+N_0)^2(a+N_1+b+N_0+1)}\]</span></p>
<p>We can simplify this formidable expression in the case that <span class="math inline">\(N \gg a, b\)</span>, to get <span class="math display">\[\text{var}(\theta|\mathcal{D}) \approx \dfrac{N_1N_0}{NNN}=\dfrac{\hat{\theta}_{MLE}(1-\hat{\theta}_{MLE})}{N}\]</span></p>
<h3 id="posterior-predictive-distribution-1">Posterior predictive distribution</h3>
<p>So far, we have been focusing on inference of the unknown parameter(s). Let us now turn our attention to prediction of future observable data.</p>
<p>Consider predicting the probability of heads in a single future trial under a Beta<span class="math inline">\((a, b)\)</span>posterior. We have <span class="math display">\[\begin{aligned}
p(\tilde{x}|\mathcal{D})&amp; =\int_0^1 p(\tilde{x}|\theta)p(\theta|\mathcal{D})\mathrm{d}\theta \nonumber \\
                        &amp; =\int_0^1 \theta\text{Beta}(\theta|a,b)\mathrm{d}\theta \nonumber \\
                        &amp; =\mathbb{E}[\theta|\mathcal{D}]=\dfrac{a}{a+b}\end{aligned}\]</span></p>
<h4 id="overfitting-and-the-black-swan-paradox">Overfitting and the black swan paradox</h4>
<p>Let us now derive a simple Bayesian solution to the problem. We will use a uniform prior, so <span class="math inline">\(a=b=1\)</span>. In this case, plugging in the posterior mean gives <strong>Laplace’s rule of succession</strong> <span class="math display">\[p(\tilde{x}|\mathcal{D})=\dfrac{N_1+1}{N_0+N_1+1}\]</span></p>
<p>This justifies the common practice of adding 1 to the empirical counts, normalizing and then plugging them in, a technique known as <strong>add-one smoothing</strong>. (Note that plugging in the MAP parameters would not have this smoothing effect, since the mode becomes the MLE if <span class="math inline">\(a=b=1\)</span>, see Section [sec:beta-binomial-Posterior-mean-and-mode].)</p>
<h4 id="predicting-the-outcome-of-multiple-future-trials">Predicting the outcome of multiple future trials</h4>
<p>Suppose now we were interested in predicting the number of heads, <span class="math inline">\(\tilde{x}\)</span>, in <span class="math inline">\(M\)</span> future trials. This is given by <span class="math display">\[\begin{aligned}
p(\tilde{x}|\mathcal{D})&amp; =\int_0^1 \text{Bin}(\tilde{x}|M,\theta)\text{Beta}(\theta|a,b)\mathrm{d}\theta \\
                        &amp; =\dbinom{M}{\tilde{x}}\dfrac{1}{B(a,b)}\int_0^1 \theta^{\tilde{x}}(1-\theta)^{M-\tilde{x}}\theta^{a-1}(1-\theta)^{b-1}\mathrm{d}\theta\end{aligned}\]</span></p>
<p>We recognize the integral as the normalization constant for a Beta<span class="math inline">\((a+\tilde{x}, M−\tilde{x}+b)\)</span> distribution. Hence <span class="math display">\[\int_0^1 \theta^{\tilde{x}}(1-\theta)^{M-\tilde{x}}\theta^{a-1}(1-\theta)^{b-1}\mathrm{d}\theta=B(\tilde{x}+a,M-\tilde{x}+b)\]</span></p>
<p>Thus we find that the posterior predictive is given by the following, known as the (compound) <strong>beta-binomial distribution</strong>: <span class="math display">\[Bb(x|a,b,M) \triangleq \dbinom{M}{x}\dfrac{B(x+a,M-x+b)}{B(a,b)}\]</span></p>
<p>This distribution has the following mean and variance <span class="math display">\[\text{mean}=M\dfrac{a}{a+b} \text{ , var}=\dfrac{Mab}{(a+b)^2}\dfrac{a+b+M}{a+b+1}\]</span></p>
<p>This process is illustrated in Figure [fig:beta-binomial-distribution]. We start with a Beta<span class="math inline">\((2,2)\)</span> prior, and plot the posterior predictive density after seeing <span class="math inline">\(N_1 =3\)</span> heads and <span class="math inline">\(N_0 =17\)</span> tails. Figure [fig:beta-binomial-distribution](b) plots a plug-in approximation using a MAP estimate. We see that the Bayesian prediction has longer tails, spreading its probability mass more widely, and is therefore less prone to overfitting and blackswan type paradoxes.</p>
<p><br />
</p>
<h2 id="the-dirichlet-multinomial-model">The Dirichlet-multinomial model</h2>
<p>In the previous section, we discussed how to infer the probability that a coin comes up heads. In this section, we generalize these results to infer the probability that a dice with <span class="math inline">\(K\)</span> sides comes up as face <span class="math inline">\(k\)</span>.</p>
<h3 id="likelihood-2">Likelihood</h3>
<p>Suppose we observe <span class="math inline">\(N\)</span> dice rolls, <span class="math inline">\(\mathcal{D}=\{x_1,x_2,\cdots,x_N\}\)</span>, where <span class="math inline">\(x_i \in \{1,2,\cdots,K\}\)</span>. The likelihood has the form <span class="math display">\[p(\mathcal{D}|\vec{\theta}) = \dbinom{N}{N_1 \cdots N_k} \prod\limits_{k=1}^K\theta_k^{N_k} \quad \text{where } N_k=\sum\limits_{i=1}^N \mathbb{I}(y_i=k)\]</span> almost the same as Equation [eqn:multinomial-pmf].</p>
<h3 id="prior-2">Prior</h3>
<p><span class="math display">\[\text{Dir}(\vec{\theta}|\vec{\alpha}) = \dfrac{1}{B(\vec{\alpha})}\prod\limits_{k=1}^K \theta_k^{\alpha_k-1}\mathbb{I}(\vec{\theta} \in S_K)\]</span></p>
<h3 id="posterior-2">Posterior</h3>
<p><span class="math display">\[\begin{aligned}
p(\vec{\theta}|\mathcal{D})&amp; \propto p(\mathcal{D}|\vec{\theta})p(\vec{\theta}) \\
     &amp; \propto \prod\limits_{k=1}^K\theta_k^{N_k}\theta_k^{\alpha_k-1} = \prod\limits_{k=1}^K\theta_k^{N_k+\alpha_k-1}\\
     &amp; =\text{Dir}(\vec{\theta}|\alpha_1+N_1,\cdots,\alpha_K+N_K)\end{aligned}\]</span></p>
<p>From Equation [eqn:Dirichlet-properties], the MAP estimate is given by <span class="math display">\[\label{eqn:Dir-MAP}
\hat{\theta}_k=\dfrac{N_k+\alpha_k-1}{N+\alpha_0-K}\]</span></p>
<p>If we use a uniform prior, <span class="math inline">\(\alpha_k=1\)</span>, we recover the MLE: <span class="math display">\[\label{eqn:Dirichlet-multinomial-posterior-MLE}
\hat{\theta}_k=\dfrac{N_k}{N}\]</span></p>
<h3 id="posterior-predictive-distribution-2">Posterior predictive distribution</h3>
<p>The posterior predictive distribution for a single multinoulli trial is given by the following expression: <span class="math display">\[\begin{aligned}
p(X=j|\mathcal{D})&amp; =\int p(X=j|\vec{\theta})p(\vec{\theta}|\mathcal{D})\mathrm{d}\vec{\theta} \\
    &amp; =\int p(X=j|\theta_j)\left[\int p(\vec{\theta}_{-j}, \theta_j|\mathcal{D})\mathrm{d}\vec{\theta}_{-j}\right]\mathrm{d}\theta_j \\
    &amp; =\int \theta_jp(\theta_j|\mathcal{D})\mathrm{d}\theta_j=\mathbb{E}[\theta_j|\mathcal{D}]=\dfrac{\alpha_j+N_j}{\alpha_0+N}\end{aligned}\]</span> where <span class="math inline">\(\vec{\theta}_{-j}\)</span> are all the components of except <span class="math inline">\(\theta_j\)</span>.</p>
<p>The above expression avoids the zero-count problem. In fact, this form of Bayesian smoothing is even more important in the multinomial case than the binary case, since the likelihood of data sparsity increases once we start partitioning the data into many categories.</p>
<h2 id="sec:NBC">Naive Bayes classifiers</h2>
<p>Assume the features are <strong>conditionally independent</strong> given the class label, then the class conditional density has the following form <span class="math display">\[p(\vec{x}|y=c,\vec{\theta})=\prod\limits_{j=1}^D p(x_j|y=c,\vec{\theta}_{jc})\]</span></p>
<p>The resulting model is called a <strong>naive Bayes classifier</strong>(NBC).</p>
<p>The form of the class-conditional density depends on the type of each feature. We give some possibilities below:</p>
<ul>
<li><p><span>In the case of real-valued features, we can use the Gaussian distribution: <span class="math inline">\(p(\vec{x}|y,\vec{\theta})=\prod_{j=1}^D \mathcal{N}(x_j|\mu_{jc},\sigma_{jc}^2)\)</span>, where <span class="math inline">\(\mu_{jc}\)</span> is the mean of feature <span class="math inline">\(j\)</span> in objects of class <span class="math inline">\(c\)</span>, and <span class="math inline">\(\sigma_{jc}^2\)</span> is its variance.</span></p></li>
<li><p><span>In the case of binary features, <span class="math inline">\(x_j \in \{0,1\}\)</span>, we can use the Bernoulli distribution: <span class="math inline">\(p(\vec{x}|y,\vec{\theta})=\prod_{j=1}^D \text{Ber}(x_j|\mu_{jc})\)</span>, where <span class="math inline">\(\mu_{jc}\)</span> is the probability that feature <span class="math inline">\(j\)</span> occurs in class <span class="math inline">\(c\)</span>. This is sometimes called the <strong>multivariate Bernoulli naive Bayes</strong> model. We will see an application of this below.</span></p></li>
<li><p><span>In the case of categorical features, <span class="math inline">\(x_j \in \{a_{j1},a_{j2},\cdots, a_{jS_j}\}\)</span>, we can use the multinoulli distribution: <span class="math inline">\(p(\vec{x}|y,\vec{\theta})=\prod_{j=1}^D \text{Cat}(x_j|\vec{\mu}_{jc})\)</span>, where <span class="math inline">\(\vec{\mu}_{jc}\)</span> is a histogram over the <span class="math inline">\(K\)</span> possible values for <span class="math inline">\(x_j\)</span> in class <span class="math inline">\(c\)</span>.</span></p></li>
</ul>
<p>Obviously we can handle other kinds of features, or use different distributional assumptions. Also, it is easy to mix and match features of different types.</p>
<h3 id="sec:NBC-Optimization">Optimization</h3>
<p>We now discuss how to “train” a naive Bayes classifier. This usually means computing the MLE or the MAP estimate for the parameters. However, we will also discuss how to compute the full posterior, <span class="math inline">\(p(\vec{\theta}|\mathcal{D})\)</span>.</p>
<h4 id="mle-for-nbc">MLE for NBC</h4>
<p>The probability for a single data case is given by <span class="math display">\[\begin{split}
p(\vec{x}_i,y_i|\vec{\theta}) &amp; =p(y_i|\vec{\pi})\prod\limits_j p(x_{ij}|\vec{\theta}_j) \\
  &amp; =\prod\limits_c \pi_c^{\mathbb{I}(y_i=c)} \prod\limits_j\prod\limits_c p(x_{ij}|\vec{\theta}_{jc})^{\mathbb{I}(y_i=c)}
\end{split}\]</span></p>
<p>Hence the log-likelihood is given by <span class="math display">\[p(\mathcal{D}|\vec{\theta})=\sum\limits_{c=1}^C{N_c\log\pi_c}+ \sum\limits_{j=1}^D{\sum\limits_{c=1}^C{\sum\limits_{i:y_i=c}{\log p(x_{ij}|\vec{\theta}_{jc})}}}\]</span> where <span class="math inline">\(N_c \triangleq \sum\limits_i \mathbb{I}(y_i=c)\)</span> is the number of feature vectors in class <span class="math inline">\(c\)</span>.</p>
<p>We see that this expression decomposes into a series of terms, one concerning <span class="math inline">\(\vec{\pi}\)</span>, and <span class="math inline">\(DC\)</span> terms containing the <span class="math inline">\(\theta_{jc}\)</span>’s. Hence we can optimize all these parameters separately.</p>
<p>From Equation [eqn:Dirichlet-multinomial-posterior-MLE], the MLE for the class prior is given by <span class="math display">\[\hat{\pi}_c=\dfrac{N_c}{N}\]</span></p>
<p>The MLE for <span class="math inline">\(\theta_{jc}\)</span>’s depends on the type of distribution we choose to use for each feature.</p>
<p>In the case of binary features, <span class="math inline">\(x_j \in \{0,1\}\)</span>, <span class="math inline">\(x_j|y=c \sim \text{Ber}(\theta_{jc})\)</span>, hence <span class="math display">\[\hat{\theta}_{jc}=\dfrac{N_{jc}}{N_c}\]</span> where <span class="math inline">\(N_{jc} \triangleq \sum\limits_{i:y_i=c} \mathbb{I}(y_i=c)\)</span> is the number that feature <span class="math inline">\(j\)</span> occurs in class <span class="math inline">\(c\)</span>.</p>
<p>In the case of categorical features, <span class="math inline">\(x_j \in \{a_{j1},a_{j2},\cdots, a_{jS_j}\}\)</span>, <span class="math inline">\(x_j|y=c \sim \text{Cat}(\vec{\theta}_{jc})\)</span>, hence <span class="math display">\[\hat{\vec{\theta}}_{jc}=(\dfrac{N_{j1c}}{N_c},\dfrac{N_{j2c}}{N_c}, \cdots, \dfrac{N_{jS_j}}{N_c})^T\]</span> where <span class="math inline">\(N_{jkc} \triangleq \sum\limits_{i=1}^N \mathbb{I}(x_{ij}=a_{jk}, y_i=c)\)</span> is the number that feature <span class="math inline">\(x_j=a_{jk}\)</span> occurs in class <span class="math inline">\(c\)</span>.</p>
<h4 id="sec:Bayesian-naive-Bayes">Bayesian naive Bayes</h4>
<p>Use a Dir<span class="math inline">\((\vec{\alpha})\)</span> prior for <span class="math inline">\(\vec{\pi}\)</span>.</p>
<p>In the case of binary features, use a Beta<span class="math inline">\((\beta0,\beta1)\)</span> prior for each <span class="math inline">\(\theta_{jc}\)</span>; in the case of categorical features, use a Dir<span class="math inline">\((\vec{\alpha})\)</span> prior for each <span class="math inline">\(\vec{\theta}_{jc}\)</span>. Often we just take <span class="math inline">\(\vec{\alpha}=\vec{1}\)</span> and <span class="math inline">\(\vec{\beta}=\vec{1}\)</span>, corresponding to <strong>add-one</strong> or <strong>Laplace smoothing</strong>.</p>
<h3 id="using-the-model-for-prediction">Using the model for prediction</h3>
<p>The goal is to compute <span class="math display">\[\begin{split}
y=f(\vec{x}) &amp; =\arg\max\limits_{c}{P(y=c|\vec{x},\vec{\theta})} \\
   &amp; =P(y=c|\vec{\theta})\prod_{j=1}^D P(x_j|y=c,\vec{\theta})
\end{split}\]</span></p>
<p>We can the estimate parameters using MLE or MAP, then the posterior predictive density is obtained by simply plugging in the parameters <span class="math inline">\(\bar{\vec{\theta}}\)</span>(MLE) or <span class="math inline">\(\hat{\vec{\theta}}\)</span>(MAP).</p>
<p>Or we can use BMA, just integrate out the unknown parameters.</p>
<h3 id="the-log-sum-exp-trick">The log-sum-exp trick</h3>
<p>when using generative classifiers of any kind, computing the posterior over class labels using Equation [eqn:Generative-classifier] can fail due to <strong>numerical underflow</strong>. The problem is that <span class="math inline">\(p(\vec{x}|y=c)\)</span> is often a very small number, especially if is a high-dimensional vector. This is because we require that <span class="math inline">\(\sum_{\vec{x}}p(\vec{x}|y)=1\)</span>, so the probability of observing any particular high-dimensional vector is small. The obvious solution is to take logs when applying Bayes rule, as follows: <span class="math display">\[\log p(y=c|\vec{x},\vec{\theta})=b_c-\log\left(\sum\limits_{c&#39;}e^{b_{c&#39;}}\right)\]</span> where <span class="math inline">\(b_c \triangleq \log p(\vec{x}|y=c,\vec{\theta})+\log p(y=c|\vec{\theta})\)</span>.</p>
<p>We can factor out the largest term, and just represent the remaining numbers relative to that. For example, <span class="math display">\[\begin{split}
\log(e^{-120}+e^{-121}) &amp; =\log(e^{-120}(1+e^{-1})) \\
   &amp; =\log(1+e^{-1})-120
\end{split}\]</span></p>
<p>In general, we have <span class="math display">\[\sum\limits_{c}e^{b_{c}}=\log\left[(\sum e^{b_c-B})e^B\right]=\log\left(\sum e^{b_c-B}\right)+B\]</span> where <span class="math inline">\(B \triangleq \max\{b_c\}\)</span>.</p>
<p>This is called the <strong>log-sum-exp</strong> trick, and is widely used.</p>
<h3 id="feature-selection-using-mutual-information">Feature selection using mutual information</h3>
<p>Since an NBC is fitting a joint distribution over potentially many features, it can suffer from overfitting. In addition, the run-time cost is <span class="math inline">\(O(D)\)</span>, which may be too high for some applications.</p>
<p>One common approach to tackling both of these problems is to perform <strong>feature selection</strong>, to remove “irrelevant” features that do not help much with the classification problem. The simplest approach to feature selection is to evaluate the relevance of each feature separately, and then take the top K,whereKis chosen based on some tradeoff between accuracy and complexity. This approach is known as <strong>variable ranking</strong>, <strong>filtering</strong>, or <strong>screening</strong>.</p>
<p>One way to measure relevance is to use mutual information (Section [sec:Mutual-information]) between feature <span class="math inline">\(X_j\)</span> and the class label <span class="math inline">\(Y\)</span> <span class="math display">\[\mathbb{I}(X_j,Y)=\sum\limits_{x_j}{\sum\limits_{y}{p(x_j,y)\log \dfrac{p(x_j,y)}{p(x_j)p(y)}}}\]</span></p>
<p>If the features are binary, it is easy to show that the MI can be computed as follows <span class="math display">\[\mathbb{I}_j = \sum\limits_c \left[\theta_{jc}\pi_c\log{\dfrac{\theta_{jc}}{\theta_j}}+(1-\theta_{jc})\pi_c\log{\dfrac{1-\theta_{jc}}{1-\theta_j}}\right]\]</span> where <span class="math inline">\(\pi_c=p(y=c)\)</span>, <span class="math inline">\(\theta_{jc}=p(x_j=1|y=c)\)</span>, and <span class="math inline">\(\theta_j=p(x_j=1)=\sum_{c} \pi_c\theta_{jc}\)</span>.</p>
<h3 id="classifying-documents-using-bag-of-words">Classifying documents using bag of words</h3>
<p><strong>Document classification</strong> is the problem of classifying text documents into different categories.</p>
<h4 id="bernoulli-product-model">Bernoulli product model</h4>
<p>One simple approach is to represent each document as a binary vector, which records whether each word is present or not, so <span class="math inline">\(x_{ij} =1\)</span> iff word <span class="math inline">\(j\)</span> occurs in document <span class="math inline">\(i\)</span>, otherwise <span class="math inline">\(x_{ij}=0\)</span>. We can then use the following class conditional density: <span class="math display">\[\begin{split}
p(\vec{x}_i|y_i=c,\vec{\theta}) &amp; =\prod\limits_{j=1}^D \mathrm{Ber}(x_{ij}|\theta_{jc}) \\
  &amp; =\prod\limits_{j=1}^D \theta_{jc}^{x_{ij}}(1-\theta_{jc})^{1-x_{ij}}
\end{split}\]</span></p>
<p>This is called the <strong>Bernoulli product model</strong>, or the <strong>binary independence model</strong>.</p>
<h4 id="multinomial-document-classifier">Multinomial document classifier</h4>
<p>However, ignoring the number of times each word occurs in a document loses some information (McCallum and Nigam 1998). A more accurate representation counts the number of occurrences of each word. Specifically, let <span class="math inline">\(\vec{x}_i\)</span> be a vector of counts for document <span class="math inline">\(i\)</span>, so <span class="math inline">\(x_{ij} \in \{0,1,\cdots,N_i\}\)</span>, where <span class="math inline">\(N_i\)</span> is the number of terms in document <span class="math inline">\(i\)</span>(so <span class="math inline">\(\sum\limits_{j=1}^D x_{ij}=N_i\)</span>). For the class conditional densities, we can use a multinomial distribution: <span class="math display">\[\label{eqn:Multinomial-document-classifier}
p(\vec{x}_i|y_i=c,\vec{\theta})=\text{Mu}(\vec{x}_i|N_i,\vec{\theta}_c)=\dfrac{N_i!}{\prod_{j=1}^D x_{ij}!}\prod\limits_{j=1}^D \theta_{jc}^{x_{ij}}\]</span> where we have implicitly assumed that the document length <span class="math inline">\(N_i\)</span> is independent of the class. Here <span class="math inline">\(θ_{jc}\)</span> is the probability of generating word <span class="math inline">\(j\)</span> in documents of class <span class="math inline">\(c\)</span>; these parameters satisfy the constraint that <span class="math inline">\(\sum_{j=1}^D \theta_{jc}=1\)</span> for each class c.</p>
<p>Although the multinomial classifier is easy to train and easy to use at test time, it does not work particularly well for document classification. One reason for this is that it does not take into account the <strong>burstiness</strong> of word usage. This refers to the phenomenon that most words never appear in any given document, but if they do appear once, they are likely to appear more than once, i.e., words occur in bursts.</p>
<p>The multinomial model cannot capture the burstiness phenomenon. To see why, note that Equation [eqn:Multinomial-document-classifier] has the form <span class="math inline">\(\theta_{jc}^{x_{ij}}\)</span>, and since <span class="math inline">\(\theta_{jc} \ll 1\)</span> for rare words, it becomes increasingly unlikely to generate many of them. For more frequent words, the decay rate is not as fast. To see why intuitively, note that the most frequent words are function words which are not specific to the class, such as “and”, “the”, and “but”; the chance of the word “and” occuring is pretty much the same no matter how many time it has previously occurred (modulo document length), so the independence assumption is more reasonable for common words. However, since rare words are the ones that matter most for classification purposes, these are the ones we want to model the most carefully.</p>
<h4 id="dcm-model">DCM model</h4>
<p>Various ad hoc heuristics have been proposed to improve the performance of the multinomial document classifier (Rennie et al. 2003). We now present an alternative class conditional density that performs as well as these ad hoc methods, yet is probabilistically sound (Madsen et al. 2005).</p>
<p>Suppose we simply replace the multinomial class conditional density with the <strong>Dirichlet Compound Multinomial</strong> or <strong>DCM</strong> density, defined as follows: <span class="math display">\[\begin{split}
p(\vec{x}_i|y_i=c,\vec{\alpha}) &amp; =\int \text{Mu}(\vec{x}_i|N_i,\vec{\theta}_c)\text{Dir}(\vec{\theta}_c|\vec{\alpha}_c) \\
   &amp; =\dfrac{N_i!}{\prod_{j=1}^D x_{ij}!}\prod\limits_{j=1}^D\dfrac{B(\vec{x}_i+\vec{\alpha}_c)}{B(\vec{\alpha}_c)}
\end{split}\]</span></p>
<p>(This equation is derived in Equation TODO.) Surprisingly this simple change is all that is needed to capture the burstiness phenomenon. The intuitive reason for this is as follows: After seeing one occurence of a word, say wordj, the posterior counts on θj gets updated, making another occurence of wordjmore likely. By contrast, ifθj is fixed, then the occurences of each word are independent. The multinomial model corresponds to drawing a ball from an urn with Kcolors of ball, recording its color, and then replacing it. By contrast, the DCM model corresponds to drawing a ball, recording its color, and then replacing it with one additional copy; this is called the <strong>Polya urn</strong>.</p>
<p>Using the DCM as the class conditional density gives much better results than using the multinomial, and has performance comparable to state of the art methods, as described in (Madsen et al. 2005). The only disadvantage is that fitting the DCM model is more complex; see (Minka 2000e; Elkan 2006) for the details.</p>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p><a href="http://en.wikipedia.org/wiki/Occam%27s_razor">http://en.wikipedia.org/wiki/Occam%27s_razor</a><a href="#fnref1">↩</a></p></li>
<li id="fn2"><p><a href="http://en.wikipedia.org/wiki/Posterior_predictive_distribution" class="uri">http://en.wikipedia.org/wiki/Posterior_predictive_distribution</a><a href="#fnref2">↩</a></p></li>
</ol>
</div>
</body>
</html>
