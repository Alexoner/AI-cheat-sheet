<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>
<h1 id="combining-models">Combining Models</h1>
<h2 id="introduction">Introduction</h2>
<dl>
<dt><strong>committees</strong></dt>
<dd><p>Train <span class="math inline">\(L\)</span> different models and then make predictions using the average of the predictions made by each model.</p>
</dd>
<dt><strong>boosting</strong></dt>
<dd><p>Train multiple models in sequence in which the error function used to train a particular model depends on the performance of the previous models.</p>
</dd>
<dt><strong>decision trees</strong></dt>
<dd><p>Different models are responsible for making predictions in different regions of input space.</p>
</dd>
<dt><strong>mixtures of experts</strong></dt>
<dd><p>Models are viewed as mixture distributions in which the component densities,as well as the mixing coefficients,are conditioned on the input variables. <span class="math display">\[\begin{aligned}
    p(t|\vec{x})=\sum\limits_{k=1}^{K}\pi_k(\vec{x})p(t|\vec{x},k)
    \end{aligned}\]</span> in which <span class="math inline">\(\pi_k(\vec{x})=p(k|\vec{x})\)</span> represents the input-dependent mixing coefficients,and <span class="math inline">\(k\)</span> indexes the model.</p>
</dd>
</dl>
<h2 id="bayesian-model-averaging">Bayesian Model Averaging</h2>
<p>In Bayesian model averaging the whole data set is generated by a single model.By contrast,when we combine multiple models,we see that different data points within the data set can potentially be generated from different values of the latent variable <span class="math inline">\(\vec{z}\)</span> and hence by different components.</p>
<h2 id="committees">Committees</h2>
<p>The simplest way to construct a committee is to average the predictions of a set of individual models,to cancel the contribution arising from variance and bias.</p>
<p><strong>Bootstrap</strong> data to introduce variability between the different models within the committee.Suppose we generate <span class="math inline">\(M\)</span> bootstrap data sets <span class="math display">\[\begin{aligned}
y_{COM}(\vec{x})=\dfrac{1}{M}\sum\limits_{m=1}^{M}y_m(\vec{x})\end{aligned}\]</span> where <span class="math inline">\(m=1,...,M\)</span>.This procedure is known as <strong>bootstrap aggregation</strong> or <strong>bagging</strong>.</p>
<h2 id="boosting">Boosting</h2>
<p>Here we describe the most widely used form of boosting algorithm:<strong>AdaBoost</strong>,short for ’adaptive boosting’.The base classifiers are known as <strong>weak learners</strong> and are trained in <strong>sequence</strong> using a <strong>weighted form of the data set</strong> in which the weighting coefficient associated with each data point depends on the performance of the previous classifiers.</p>
<p>Consider a two-class classification problem,in which the training data comprises input vectors <span class="math inline">\(\vec{x}_1,...,\vec{x}_N\)</span> along with corresponding binary target variables <span class="math inline">\(t_1,...,t_N\)</span> where <span class="math inline">\(t_n\in \{-1,1\}\)</span>.Each data point is given an associated weighting parameter <span class="math inline">\(w_n\)</span>,initially set <span class="math inline">\(1/N\)</span> for all.A base classifier function <span class="math inline">\(y(\vec{x})\in \{-1,1\}\)</span></p>
<p><embed src="prml/Figure14.1" /></p>
<p>[H]</p>
<p>1. Initialize the data weighting coefficients <span class="math inline">\(\{w_n\}\)</span> by setting <span class="math inline">\(w_n^{1}=1/N\)</span> for <span class="math inline">\(n=1,...,N\)</span>. 2. 3. Make predictions using the final model,which is given by <span class="math display">\[\begin{aligned}
    Y_M(\vec{x})=sign(\sum_{m=1}^{M}\alpha_m y_m(\vec{x}))
    \end{aligned}\]</span></p>
<h3 id="minimizing-exponential-error">Minimizing exponential error</h3>
<p>Consider the exponential error function defined by <span class="math display">\[\begin{aligned}
E=\sum_{n=1}^{N}\exp\{-t_n f_m(\vec{x}_n)\}\end{aligned}\]</span> where <span class="math inline">\(f_m(\vec{x})\)</span> is a classifier defined in terms of a linear combination of base classifiers <span class="math inline">\(y_l(\vec{x})\)</span> of the form <span class="math display">\[\begin{aligned}
f_m(\vec{x})=\dfrac{1}{2}\sum_{l=1}^{m}\alpha_l y_l(\vec{x})\end{aligned}\]</span> and <span class="math inline">\(t_n\in \{-1,1\}\)</span> are the training set target values.Our goal is to minimize <span class="math inline">\(E\)</span> with respect to the weighting coefficients <span class="math inline">\(\alpha_l\)</span> and parameters of the base classifiers <span class="math inline">\(y_l(\vec{x})\)</span>.</p>
<p>Separating off the contribution from base classifier <span class="math inline">\(y_m(\vec{x})\)</span>, <span class="math display">\[\begin{aligned}
E&amp;=\sum_{n=1}^{N}\exp\{-t_n f_m(\vec{x}_n)\} \\
&amp;=\sum_{n=1}^{N}\exp\{-t_n \dfrac{1}{2}\sum_{l=1}^{m}\alpha_l y_l(\vec{x}) \} \\
&amp;=\sum_{n=1}^{N}\exp\{-t_n f_{m-1}(\vec{x}_n)-\dfrac{1}{2}t_n \alpha_m y_m(\vec{x}_n)\} \\
&amp;=\sum_{n=1}^{N}w_n^{(m)}\exp\{-\dfrac{1}{2}t_n \alpha_m y_m(\vec{x}_n)\}\end{aligned}\]</span> where the coefficients <span class="math inline">\(w_n^{(m)}=\exp\{-t_n f_{m-1}(\vec{x}_n)\}\)</span> can be viewed as constants because we are optimizing only <span class="math inline">\(\alpha_m\)</span> and <span class="math inline">\(y_m(\vec{x})\)</span>.Denote by <span class="math inline">\(\mathcal{T}_m\)</span> the set of data points correctly classified by <span class="math inline">\(y_m(\vec{x})\)</span> and misclassified points by <span class="math inline">\(\mathcal{M}_m\)</span>,then we in turn rewrite the error function <span class="math display">\[\begin{aligned}
E &amp;=e^{-\alpha_m/2}\sum_{n\in\mathcal{T}_m}w_n^{(m)}+e^{\alpha_m/2}\sum_{n\in\mathcal{M}_m}w_n^{(m)} \\
&amp;=(e^{\alpha_m/2}-e^{-\alpha_m/2})\sum_{n=1}^{N}w_n^{(m)}I(y_m(\vec{x}_n)\neq t_n)+e^{-\alpha_m/2}\sum_{n=1}^{N}w_n^{(m)}\end{aligned}\]</span> Then we can minimize this with respect to <span class="math inline">\(y_m(\vec{x}_n)\)</span> and <span class="math inline">\(\alpha_m\)</span>. <span class="math display">\[\begin{aligned}
\because w_n^{(m)}=\exp\{-t_n f_{m-1}(\vec{x}_n)\} \\
\therefore w_n^{(m+1)}=\exp\{-t_n f_{m}(\vec{x}_n)\} \\
\therefore w_n^{(m+1)}=w_n^{(m)}\exp\{-\dfrac{1}{2}t_n\alpha_m y_m(\vec{x}_n)\}\end{aligned}\]</span> Making use of the fact that <span class="math display">\[\begin{aligned}
t_n y_m(\vec{x})=1-2I(y_m(\vec{x}_n)\neq t_n)\end{aligned}\]</span> we see updates at the next iteration <span class="math display">\[\begin{aligned}
w_n^{(m+1)}=w_n^{(m)}\exp(-\alpha_m/2)\exp\{\alpha_m I(y_m(\vec{x}_n)\neq t_n)\}\end{aligned}\]</span> Because the term <span class="math inline">\(\exp(-\alpha_m/2)\)</span> is independent of <span class="math inline">\(n\)</span>,so can be discarded.</p>
<h3 id="error-functions-for-boosting">Error functions for boosting</h3>
<h2 id="tree-based-models">Tree-based Models</h2>
<h2 id="conditional-mixture-models">Conditional Mixture Models</h2>
</body>
</html>
