<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>
<h1 id="chap:Gaussian Models">Gaussian Models</h1>
<p>The Gaussian,also known as the <strong>normal distribution</strong>,is a widely used model for the <strong>distribution of continuous variables</strong>.In the case of a single variable <span class="math inline">\(x\)</span>,the Gaussian distribution can be written in the form <span class="math display">\[\mathcal{N}(x|\mu,\sigma^2) = \dfrac{1}{(2\pi\sigma^2)^{\frac{1}{2}}}\exp\{\dfrac{1}{2\sigma^2 (x-\mu)^2} \}\]</span> where the <span class="math inline">\(\mu\)</span> is the mean and <span class="math inline">\(\sigma^2\)</span> is the variance. Now we will discuss the <strong>multivariate Gaussian</strong> or <strong>multivariate normal(MVN)</strong>, which is the most widely used joint probability density function for continuous variables. It will form the basis for many of the models.</p>
<h2 id="gaussian-discriminant-analysis">Gaussian discriminant analysis</h2>
<p>One important application of MVNs is to define the the class conditional densities in a generative classifier, i.e., <span class="math display">\[p(\vec{x}|y=c,\vec{\theta})=\mathcal{N}(\vec{x}|\vec{\mu}_c,\vec{\Sigma}_c)\]</span></p>
<p>The resulting technique is called (Gaussian) <strong>discriminant analysis</strong> or <strong>GDA</strong> (even though it is a generative, not discriminative, classifier — see Section TODO for more on this distinction). If <span class="math inline">\(\vec{\Sigma}_c\)</span> is diagonal, this is equivalent to naive Bayes.</p>
<p>We can classify a feature vector using the following decision rule, derived from Equation [eqn:Generative-classifier]: <span class="math display">\[y=\arg\max_{c} \left[\log p(y=c|\vec{\pi})+\log p(\vec{x}|\vec{\theta})\right]\]</span></p>
<p>When we compute the probability of <span class="math inline">\(\vec{x}\)</span> under each class conditional density, we are measuring the distance from <span class="math inline">\(\vec{x}\)</span> to the center of each class, <span class="math inline">\(\vec{\mu}_c\)</span>, using Mahalanobis distance. This can be thought of as a <strong>nearest centroids classifier</strong>.</p>
<p><br />
</p>
<p>As an example, Figure [fig:2d-Gaussians-Visualization] shows two Gaussian class-conditional densities in 2d, representing the height and weight of men and women. We can see that the features are correlated, as is to be expected (tall people tend to weigh more). The ellipses for each class contain 95% of the probability mass. If we have a uniform prior over classes, we can classify a new test vector as follows: <span class="math display">\[y=\arg\max_{c} (\vec{x}-\vec{\mu}_c)^T\vec{\Sigma}_c^{-1}(\vec{x}-\vec{\mu}_c)\]</span></p>
<h3 id="quadratic-discriminant-analysis-qda">Quadratic discriminant analysis (QDA)</h3>
<p>By plugging in the definition of the Gaussian density to Equation [eqn:Generative-classifier], we can get <span class="math display">\[\label{eqn:QDA}
p(y|\vec{x},\vec{\theta})=\dfrac{\pi_c|2\pi\vec{\Sigma}_c|^{-\frac{1}{2}}\exp\left[-\frac{1}{2}(\vec{x}-\vec{\mu})^T\vec{\Sigma}^{-1}(\vec{x}-\vec{\mu})\right]}{\sum_{c&#39;}\pi_{c&#39;}|2\pi\vec{\Sigma}_{c&#39;}|^{-\frac{1}{2}}\exp\left[-\frac{1}{2}(\vec{x}-\vec{\mu})^T\vec{\Sigma}^{-1}(\vec{x}-\vec{\mu})\right]}\]</span></p>
<p>Thresholding this results in a quadratic function of <span class="math inline">\(\vec{x}\)</span>. The result is known as quadratic discriminant analysis(QDA). Figure [fig:QDA] gives some examples of what the decision boundaries look like in 2D.</p>
<p><br />
</p>
<h3 id="sec:Linear-discriminant-analysis">Linear discriminant analysis (LDA)</h3>
<p>We now consider a special case in which the covariance matrices are <strong>tied</strong> or <strong>shared</strong> across classes,<span class="math inline">\(\vec{\Sigma}_c=\vec{\Sigma}\)</span>. In this case, we can simplify Equation [eqn:QDA] as follows: <span class="math display">\[\begin{aligned}
p(y|\vec{x},\vec{\theta})&amp; \propto \pi_c\exp\left(\vec{\mu}_c\vec{\Sigma}^{-1}\vec{x}-\dfrac{1}{2}\vec{x}^T\vec{\Sigma}^{-1}\vec{x}-\dfrac{1}{2}\vec{\mu}_c^T\vec{\Sigma}^{-1}\vec{\mu}_c\right) \nonumber \\
 &amp; =\exp\left(\vec{\mu}_c\vec{\Sigma}^{-1}\vec{x}-\dfrac{1}{2}\vec{\mu}_c^T\vec{\Sigma}^{-1}\vec{\mu}_c+\log \pi_c\right) \nonumber \\
 &amp; \quad \exp\left(-\dfrac{1}{2}\vec{x}^T\vec{\Sigma}^{-1}\vec{x}\right) \nonumber \\
 &amp; \propto \exp\left(\vec{\mu}_c\vec{\Sigma}^{-1}\vec{x}-\dfrac{1}{2}\vec{\mu}_c^T\vec{\Sigma}^{-1}\vec{\mu}_c+\log \pi_c\right)\end{aligned}\]</span></p>
<p>Since the quadratic term <span class="math inline">\(\vec{x}^T\vec{\Sigma}^{-1}\vec{x}\)</span> is independent of <span class="math inline">\(c\)</span>, it will cancel out in the numerator and denominator. If we define <span class="math display">\[\begin{aligned}
\gamma_c&amp; \triangleq -\dfrac{1}{2}\vec{\mu}_c^T\vec{\Sigma}^{-1}\vec{\mu}_c+\log \pi_c \\
\vec{\beta}_c&amp; \triangleq \vec{\Sigma}^{-1}\vec{\mu}_c\end{aligned}\]</span> then we can write <span class="math display">\[\label{eqn:LDA}
p(y|\vec{x},\vec{\theta})=\dfrac{e^{\vec{\beta}_c^T\vec{x}+\gamma_c}}{\sum_{c&#39;}e^{\vec{\beta}_{c&#39;}^T\vec{x}+\gamma_{c&#39;}}} \triangleq \sigma(\vec{\eta}, c)\]</span> where <span class="math inline">\(\vec{\eta} \triangleq (e^{\vec{\beta}_1^T\vec{x}}+\gamma_1,\cdots, e^{\vec{\beta}_C^T\vec{x}}+\gamma_C)\)</span>, <span class="math inline">\(\sigma()\)</span> is the <strong>softmax activation function</strong><a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>, defined as follows: <span class="math display">\[\sigma(\vec{q},i) \triangleq \dfrac{\exp(q_i)}{\sum_{j=1}^n \exp(q_j)}\]</span></p>
<p>When parameterized by some constant, <span class="math inline">\(\alpha &gt; 0\)</span>, the following formulation becomes a smooth, differentiable approximation of the maximum function: <span class="math display">\[\mathcal{S}_{\alpha}(\vec{x}) = \dfrac{\sum_{j=1}^D x_je^{\alpha x_j}}{\sum_{j=1}^D e^{\alpha x_j}}\]</span></p>
<p><span class="math inline">\(\mathcal{S}_{\alpha}\)</span> has the following properties:</p>
<ol>
<li><p><span class="math inline">\(\mathcal{S}_{\alpha} \rightarrow \max\)</span> as <span class="math inline">\(\alpha \rightarrow \infty\)</span></p></li>
<li><p><span class="math inline">\(\mathcal{S}_0\)</span> is the average of its inputs</p></li>
<li><p><span class="math inline">\(\mathcal{S}_{\alpha} \rightarrow \min\)</span> as <span class="math inline">\(\alpha \rightarrow -\infty\)</span></p></li>
</ol>
<p>Note that the softmax activation function comes from the area of statistical physics, where it is common to use the <strong>Boltzmann distribution</strong>, which has the same form as the softmax activation function.</p>
<p>An interesting property of Equation [eqn:LDA] is that, if we take logs, we end up with a linear function of <span class="math inline">\(\vec{x}\)</span>. (The reason it is linear is because the <span class="math inline">\(\vec{x}^T\vec{\Sigma}^{-1}\vec{x}\)</span> cancels from the numerator and denominator.) Thus the decision boundary between any two classes, says <span class="math inline">\(c\)</span> and <span class="math inline">\(c&#39;\)</span>, will be a straight line. Hence this technique is called <strong>linear discriminant analysis</strong> or <strong>LDA</strong>.</p>
<p>An alternative to fitting an LDA model and then deriving the class posterior is to directly fit <span class="math inline">\(p(y|\vec{x},\vec{W})=\text{Cat}(y|\vec{W}\vec{x})\)</span> for some <span class="math inline">\(C \times D\)</span> weight matrix <span class="math inline">\(\vec{W}\)</span>. This is called <strong>multi-class logistic regression</strong>, or <strong>multinomial logistic regression</strong>. We will discuss this model in detail in Section TODO. The difference between the two approaches is explained in Section TODO.</p>
<h3 id="two-class-lda">Two-class LDA</h3>
<p>To gain further insight into the meaning of these equations, let us consider the binary case. In this case, the posterior is given by <span class="math display">\[\begin{aligned}
p(y=1|\vec{x},\vec{\theta})&amp; =\dfrac{e^{\vec{\beta}_1^T\vec{x}+\gamma_1}}{e^{\vec{\beta}_0^T\vec{x}+\gamma_0}+e^{\vec{\beta}_1^T\vec{x}+\gamma_1}}) \\
  &amp; =\dfrac{1}{1+e^(\vec{\beta}_0-\vec{\beta}_1)^T\vec{x}+(\gamma_0-\gamma_1)} \\
  &amp; =\text{sigm}((\vec{\beta}_1-\vec{\beta}_0)^T\vec{x}+(\gamma_0-\gamma_1))\end{aligned}\]</span> where sigm<span class="math inline">\((x)\)</span> refers to the sigmoid function<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a>.</p>
<p>Now <span class="math display">\[\begin{aligned}
\gamma_1-\gamma_0&amp; = -\dfrac{1}{2}\vec{\mu}_1^T\vec{\Sigma}^{-1}\vec{\mu}_1+\dfrac{1}{2}\vec{\mu}_0^T\vec{\Sigma}^{-1}\vec{\mu}_0 + \log(\pi_1/\pi_0) \\
 &amp; =-\dfrac{1}{2}(\vec{\mu}_1-\vec{\mu}_0)^T\vec{\Sigma}^{-1}(\vec{\mu}_1+\vec{\mu}_0)+ \log(\pi_1/\pi_0)\end{aligned}\]</span></p>
<p>So if we define <span class="math display">\[\begin{aligned}
\vec{w}&amp; =\vec{\beta}_1-\vec{\beta}_0=\vec{\Sigma}^{-1}(\vec{\mu}_1-\vec{\mu}_0) \\
\vec{x}_0&amp; =\dfrac{1}{2}(\vec{\mu}_1+\vec{\mu}_0)-(\vec{\mu}_1-\vec{\mu}_0)\dfrac{\log(\pi_1/\pi_0)}{(\vec{\mu}_1-\vec{\mu}_0)^T\vec{\Sigma}^{-1}(\vec{\mu}_1-\vec{\mu}_0)}\end{aligned}\]</span> then we have <span class="math inline">\(\vec{w}^T\vec{x}_0=-(\gamma_1-\gamma_0)\)</span>, and hence <span class="math display">\[p(y=1|\vec{x},\vec{\theta})=\text{sigm}(\vec{w}^T(\vec{x}-\vec{x}_0))\]</span></p>
<p>(This is closely related to logistic regression, which we will discuss in Section TODO.) So the final decision rule is as follows: shift <span class="math inline">\(\vec{x}\)</span> by <span class="math inline">\(\vec{x}_0\)</span>, project onto the line , and see if the result is positive or negative.</p>
<div class="figure">
<img src="2d-LDA.png" alt="Geometry of LDA in the 2 class case where \vec{\Sigma}_1=\vec{\Sigma}_2=\vec{I}." />
<p class="caption">Geometry of LDA in the 2 class case where <span class="math inline">\(\vec{\Sigma}_1=\vec{\Sigma}_2=\vec{I}\)</span>.<span data-label="fig:2d-LDA"></span></p>
</div>
<p>If <span class="math inline">\(\vec{\Sigma}=\sigma^2\vec{I}\)</span>, then <span class="math inline">\(\vec{w}\)</span> is in the direction of <span class="math inline">\(\vec{\mu}_1-\vec{\mu}_0\)</span>. So we classify the point based on whether its projection is closer to <span class="math inline">\(\vec{\mu}_0\)</span> or <span class="math inline">\(\vec{\mu}_1\)</span> . This is illustrated in Figure [fig:2d-LDA]. Furthemore, if <span class="math inline">\(\vec{\pi}_1=\vec{\pi}_0\)</span>, then <span class="math inline">\(\vec{x}_0=\frac{1}{2}(\vec{\mu}_1+\vec{\mu}_0)\)</span>, which is half way between the means. If we make <span class="math inline">\(\vec{\pi}_1&gt;\vec{\pi}_0\)</span>, then <span class="math inline">\(\vec{x}_0\)</span> gets closer to <span class="math inline">\(\vec{\mu}_0\)</span>, so more of the line belongs to class 1 a <em>priori</em>. Conversely if <span class="math inline">\(\vec{\pi}_1&lt;\vec{\pi}_0\)</span>, the boundary shifts right. Thus we see that the class prior, πc, just changes the decision threshold, and not the overall geometry, as we claimed above. (A similar argument applies in the multi-class case.)</p>
<p>The magnitude of <span class="math inline">\(\vec{w}\)</span> determines the steepness of the logistic function, and depends on how well-separated the means are, relative to the variance. In psychology and signal detection theory, it is common to define the <strong>discriminability</strong> of a signal from the background noise using a quantity called <strong>d-prime</strong>: <span class="math display">\[d&#39; \triangleq \dfrac{\mu_1-\mu_0}{\sigma}\]</span> where <span class="math inline">\(\mu_1\)</span> is the mean of the signal and <span class="math inline">\(\mu_0\)</span> is the mean of the noise, and <span class="math inline">\(\sigma\)</span> is the standard deviation of the noise. If <span class="math inline">\(d&#39;\)</span> is large, the signal will be easier to discriminate from the noise.</p>
<h3 id="sec:MLE-for-discriminant-analysis">MLE for discriminant analysis</h3>
<p>The log-likelihood function is as follows: <span class="math display">\[p(\mathcal{D}|\vec{\theta})=\sum\limits_{c=1}^C{\sum\limits_{i:y_i=c}{\log\pi_c}}+\sum\limits_{c=1}^C{\sum\limits_{i:y_i=c}{\log\mathcal{N}(\vec{x}_i|\vec{\mu}_c,\vec{\Sigma}_c)}}\]</span></p>
<p>The MLE for each parameter is as follows: <span class="math display">\[\begin{aligned}
\bar{\vec{\mu}}_c&amp; = \dfrac{N_c}{N} \\
\bar{\vec{\mu}}_c&amp; = \dfrac{1}{N_c}\sum\limits_{i:y_i=c}\vec{x}_i \\
\bar{\vec{\Sigma}}_c&amp; = \dfrac{1}{N_c}\sum\limits_{i:y_i=c}(\vec{x}_i-\bar{\vec{\mu}}_c)(\vec{x}_i-\bar{\vec{\mu}}_c)^T\end{aligned}\]</span></p>
<h3 id="strategies-for-preventing-overfitting">Strategies for preventing overfitting</h3>
<p>The speed and simplicity of the MLE method is one of its greatest appeals. However, the MLE can badly overfit in high dimensions. In particular, the MLE for a full covariance matrix is singular if <span class="math inline">\(N_c &lt;D\)</span>. And even when <span class="math inline">\(N_c &gt;D\)</span>, the MLE can be ill-conditioned, meaning it is close to singular. There are several possible solutions to this problem:</p>
<ul>
<li><p><span>Use a diagonal covariance matrix for each class, which assumes the features are conditionally independent; this is equivalent to using a naive Bayes classifier (Section [sec:NBC])</span>.</p></li>
<li><p><span>Use a full covariance matrix, but force it to be the same for all classes,<span class="math inline">\(\vec{\Sigma}_c=\vec{\Sigma}\)</span>. This is an example of <strong>parameter tying</strong> or <strong>parameter sharing</strong>, and is equivalent to LDA (Section [sec:Linear-discriminant-analysis]).</span></p></li>
<li><p><span>Use a diagonal covariance matrix and forced it to be shared. This is called diagonal covariance LDA, and is discussed in Section TODO.</span></p></li>
<li><p><span>Use a full covariance matrix, but impose a prior and then integrate it out. If we use a conjugate prior, this can be done in closed form, using the results from Section TODO; this is analogous to the “Bayesian naive Bayes” method in Section [sec:Bayesian-naive-Bayes]. See (Minka 2000f) for details.</span></p></li>
<li><p><span>Fit a full or diagonal covariance matrix by MAP estimation. We discuss two different kindsof prior below.</span></p></li>
<li><p><span>Project the data into a low dimensional subspace and fit the Gaussians there. See Section TODO for a way to find the best (most discriminative) linear projection.</span></p></li>
</ul>
<p>We discuss some of these options below.</p>
<h3 id="regularized-lda">Regularized LDA *</h3>
<h3 id="diagonal-lda">Diagonal LDA</h3>
<h3 id="nearest-shrunken-centroids-classifier">Nearest shrunken centroids classifier *</h3>
<p>One drawback of diagonal LDA is that it depends on all of the features. In high dimensional problems, we might prefer a method that only depends on a subset of the features, for reasons of accuracy and interpretability. One approach is to use a screening method, perhaps based on mutual information, as in Section 3.5.4. We now discuss another approach to this problem known as the <strong>nearest shrunken centroids</strong> classifier (Hastie et al. 2009, p652).</p>
<h2 id="sec:Inference-in-jointly-Gaussian-distributions">Inference in jointly Gaussian distributions</h2>
<p>Given a joint distribution, <span class="math inline">\(p(\vec{x}_1,\vec{x}_2)\)</span>, it is useful to be able to compute marginals <span class="math inline">\(p(\vec{x}_1)\)</span> and conditionals <span class="math inline">\(p(\vec{x}_1|\vec{x}_2)\)</span>. We discuss how to do this below, and then give some applications. These operations take <span class="math inline">\(O(D^3)\)</span> time in the worst case. See Section TODO for faster methods.</p>
<h3 id="statement-of-the-result">Statement of the result</h3>
<p>(<strong>Marginals and conditionals of an MVN</strong>). Suppose <span class="math inline">\(X=(\vec{x}_1,\vec{x}_2)\)</span>is jointly Gaussian with parameters <span class="math display">\[\vec{\mu}=\left(\begin{array}{c}\vec{\mu}_1 \\
                                \vec{\mu}_2\end{array}\right),
\vec{\Sigma}=\left(\begin{array}{cc}
                   \vec{\Sigma}_{11} &amp; \vec{\Sigma}_{12} \\
                   \vec{\Sigma}_{21} &amp; \vec{\Sigma}_{22} \end{array}\right),
\vec{\Lambda}=\vec{\Sigma}^{-1}=\left(\begin{array}{cc}
                   \vec{\Lambda}_{11} &amp; \vec{\Lambda}_{12} \\
                   \vec{\Lambda}_{21} &amp; \vec{\Lambda}_{22} \end{array}\right),\]</span></p>
<p>Then the marginals are given by <span class="math display">\[\begin{split}
p(\vec{x}_1)= \mathcal{N}(\vec{x}_1|\vec{\mu}_1,\vec{\Sigma}_{11})\\
p(\vec{x}_2)= \mathcal{N}(\vec{x}_2|\vec{\mu}_2,\vec{\Sigma}_{22})
\end{split}\]</span> and the posterior conditional is given by <span class="math display">\[\label{eqn:Marginals-and-conditionals-of-an-MVN}
  \boxed{\begin{split}
    p(\vec{x}_1|\vec{x}_2)&amp; =\mathcal{N}(\vec{x}_1|\vec{\mu}_{1|2},\vec{\Sigma}_{1|2}) \\
    \vec{\mu}_{1|2}&amp; = \vec{\mu}_1+\vec{\Sigma}_{12}\vec{\Sigma}_{22}^{-1}(\vec{x}_2-\vec{\mu}_2) \\
                   &amp; = \vec{\mu}_1-\vec{\Lambda}_{11}^{-1}\vec{\Lambda}_{12}(\vec{x}_2-\vec{\mu}_2) \\
                   &amp; = \vec{\Sigma}_{1|2}(\vec{\Lambda}_{11}\vec{\mu}_1-\vec{\Lambda}_{12}(\vec{x}_2-\vec{\mu}_2)) \\
    \vec{\Sigma}_{1|2}&amp; = \vec{\Sigma}_{11}-\vec{\Sigma}_{12}\vec{\Sigma}_{22}^{-1}\vec{\Sigma}_{21}=\vec{\Lambda}_{11}^{-1}
  \end{split}}\]</span></p>
<p>Equation [eqn:Marginals-and-conditionals-of-an-MVN] is of such crucial importance in this book that we have put a box around it, so you can easily find it. For the proof, see Section TODO.</p>
<p>We see that both the marginal and conditional distributions are themselves Gaussian. For the marginals, we just extract the rows and columns corresponding to <span class="math inline">\(\vec{x}_1\)</span> or <span class="math inline">\(\vec{x}_2\)</span>. For the conditional, we have to do a bit more work. However, it is not that complicated: the conditional mean is just a linear function of <span class="math inline">\(\vec{x}_2\)</span>, and the conditional covariance is just a constant matrix that is independent of <span class="math inline">\(\vec{x}_2\)</span>. We give three different (but equivalent) expressions for the posterior mean, and two different (but equivalent) expressions for the posterior covariance; each one is useful in different circumstances.</p>
<h3 id="examples">Examples</h3>
<p>Below we give some examples of these equations in action, which will make them seem more intuitive.</p>
<h4 id="marginals-and-conditionals-of-a-2d-gaussian">Marginals and conditionals of a 2d Gaussian</h4>
<h3 id="information-form">information form</h3>
<h3 id="proof">Proof</h3>
<h2 id="linear-gaussian-systems">Linear Gaussian systems</h2>
<p>Suppose we have two variables, <span class="math inline">\(\vec{x}\)</span> and <span class="math inline">\(\vec{y}\)</span>.Let <span class="math inline">\(\vec{x} \in \mathbb{R}^{D_x}\)</span> be a hidden variable, and <span class="math inline">\(\vec{y} \in \mathbb{R}^{D_y}\)</span> be a noisy observation of <span class="math inline">\(\vec{x}\)</span>. Let us assume we have the following prior and likelihood: <span class="math display">\[\label{eqn:Linear-Gaussian-system}
  \boxed{\begin{split}
    p(\vec{x})&amp;=\mathcal{N}(\vec{x}|\vec{\mu}_x,\vec{\Sigma}_x) \\
    p(\vec{y}|\vec{x})&amp;=\mathcal{N}(\vec{y}|\vec{W}\vec{x}+\vec{\mu}_y,\vec{\Sigma}_y)
  \end{split}}\]</span> where <span class="math inline">\(\vec{W}\)</span> is a matrix of size <span class="math inline">\(D_y \times D_x\)</span>. This is an example of a <strong>linear Gaussian system</strong>. We can represent this schematically as <span class="math inline">\(\vec{x} \rightarrow \vec{y}\)</span>, meaning <span class="math inline">\(\vec{x}\)</span> generates <span class="math inline">\(\vec{y}\)</span>. In this section, we show how to “invert the arrow”, that is, how to infer <span class="math inline">\(\vec{x}\)</span> from <span class="math inline">\(\vec{y}\)</span>. We state the result below, then give several examples, and finally we derive the result. We will see many more applications of these results in later chapters.</p>
<h3 id="statement-of-the-result-1">Statement of the result</h3>
<p>(<strong>Bayes rule for linear Gaussian systems</strong>). Given a linear Gaussian system, as in Equation [eqn:Linear-Gaussian-system], the posterior <span class="math inline">\(p(\vec{x}|\vec{y})\)</span> is given by the following: <span class="math display">\[\label{eqn:Linear-Gaussian-system-posterior}
  \boxed{\begin{split}
    p(\vec{x}|\vec{y})&amp;=\mathcal{N}(\vec{x}|\vec{\mu}_{x|y},\vec{\Sigma}_{x|y}) \\
    \vec{\Sigma}_{x|y}&amp;=\vec{\Sigma}_x^{-1}+\vec{W}^T\vec{\Sigma}_y^{-1}\vec{W} \\
    \vec{\mu}_{x|y}&amp;=\vec{\Sigma}_{x|y}\left[\vec{W}^T\vec{\Sigma}_y^{-1}(\vec{y}-\vec{\mu}_y)+\vec{\Sigma}_x^{-1}\vec{\mu}_x\right]
  \end{split}}\]</span> In addition, the normalization constant <span class="math inline">\(p(\vec{y})\)</span> is given by <span class="math display">\[\label{eqn:Linear-Gaussian-system-normalizer}
  \boxed{
    p(\vec{y})=\mathcal{N}(\vec{y}|\vec{W}\vec{\mu}_x+\vec{\mu}_y,\vec{\Sigma}_y+\vec{W}\vec{\Sigma}_x\vec{W}^T)
  }\]</span></p>
<p>For the proof, see Section 4.4.3 TODO.</p>
<h3 id="examples-1">Examples</h3>
<h3 id="proof-1">Proof</h3>
<h2 id="digression-the-wishart-distribution">Digression: The Wishart distribution *</h2>
<h2 id="inferring-the-parameters-of-an-mvn">Inferring the parameters of an MVN</h2>
<h3 id="posterior-distribution-of-mu">Posterior distribution of <span class="math inline">\(\mu\)</span></h3>
<h3 id="posterior-distribution-of-sigma">Posterior distribution of <span class="math inline">\(\Sigma\)</span> *</h3>
<h3 id="sec:Posterior-distribution-of-mu-and-Sigma">Posterior distribution of <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\Sigma\)</span> *</h3>
<h3 id="sensor-fusion-with-unknown-precisions">Sensor fusion with unknown precisions *</h3>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p><a href="http://en.wikipedia.org/wiki/Softmax_activation_function" class="uri">http://en.wikipedia.org/wiki/Softmax_activation_function</a><a href="#fnref1">↩</a></p></li>
<li id="fn2"><p><a href="http://en.wikipedia.org/wiki/Sigmoid_function" class="uri">http://en.wikipedia.org/wiki/Sigmoid_function</a><a href="#fnref2">↩</a></p></li>
</ol>
</div>
</body>
</html>
