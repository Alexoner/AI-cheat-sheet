<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>
<h1 id="chap:EM">Mixture models and EM</h1>
<h2 id="introduction">Introduction</h2>
<p>If we define a joint distribution over observed and latent variables,the corresponding distribution of the observed variables alone is marginalization.Mixture distributions can be interpreted in terms of discrete latent variables.As well as providing framework for building more complex probability distributions,mixture models can also be used to cluster data.</p>
<h2 id="k-means-clusetering"><span class="math inline">\(K\)</span>-means Clusetering</h2>
<h3 id="representation">representation</h3>
<p>Suppose we have a data set <span class="math inline">\(\{\vec{x}_1,...,\vec{x}_N\}\)</span> consisting of <span class="math inline">\(N\)</span> observations of a random <span class="math inline">\(D\)</span>-dimensional Euclidean variable <span class="math inline">\(\vec{x}\)</span>.Our goal is to partition the data set into some number <span class="math inline">\(K\)</span> of clusters.First introduce a set of <span class="math inline">\(D\)</span>-dimensional vectors <span class="math inline">\(\vec{\mu}_k\)</span>,where <span class="math inline">\(k=1,...,K\)</span>,in which <span class="math inline">\(\vec{\mu}_k\)</span> is a prototype associated with the <span class="math inline">\(k^{th}\)</span> cluster,representing the centers of the clusters.</p>
<p>Our goal is then to find an assignment of data points to clusters,as well as a set of vectors <span class="math inline">\(\{\vec{\mu}_k\}\)</span>,such that the sum of squares of the distances of each data point to its closest vector <span class="math inline">\(\vec{\mu}_k\)</span>,is a minimum.</p>
<p>A corresponding set of binary indicator variables <span class="math inline">\(r_{nk}\in \{0,1\}\)</span>,describing which of the <span class="math inline">\(K\)</span> clusters the data point <span class="math inline">\(\vec{x}_n\)</span> is assigned to,so that if data point <span class="math inline">\(\vec{x}_n\)</span> is assigned to cluster <span class="math inline">\(k\)</span> then <span class="math inline">\(r_{nk}=1\)</span>,and <span class="math inline">\(r_{nj}=0\)</span> for <span class="math inline">\(j\neq k\)</span>.This is the 1-of-<span class="math inline">\(K\)</span> coding scheme.</p>
<h3 id="evaluation">evaluation</h3>
<p>Define the objective function,sometimes called a <strong>distortion measure</strong> as <span class="math display">\[J=\sum_{n=1}^{N}\sum_{k=1}^{K}r_{nk}\parallel \vec{x}_n-\vec{\mu}_k\parallel^2\]</span></p>
<h3 id="optimization">optimization</h3>
<p>Our goal is to find value for the <span class="math inline">\(\{r_{nk}\}\)</span> and the <span class="math inline">\(\{\vec{\mu}_k\}\)</span> so as to minimize <span class="math inline">\(J\)</span>.We can do this through an iterative procedure in which each iteration involves two successive steps corresponding to successive optimizations with respect to the <span class="math inline">\(r_{nk}\)</span> and the <span class="math inline">\(\vec{\mu}_k\)</span>,which can be interpreted as parameters of each cluster’s probability distribution.</p>
<p>Consider first the determination of the <span class="math inline">\(r_{nk}\)</span>,which can be interpreted as likelihood function of each cluster.We simply assign the <span class="math inline">\(n^{th}\)</span> data point to the closest cluster centre. <span class="math display">\[r_{nk} = \begin{cases}
1,\text{if }k=\arg\min_j\parallel \vec{x}_n-\vec{\mu}_j\parallel^2 \\
0,\text{otherwise} \\
\end{cases}\]</span> Note that <span class="math display">\[\begin{aligned}
\arg\min_j\parallel \vec{x}_n-\vec{\mu}_j\parallel^2
&amp;= \arg\min_j(\parallel \vec{x}_n\parallel ^2-2\parallel \vec{x}_n\parallel\parallel \vec{\mu}_j\parallel + \parallel\vec{\mu}_j\parallel^2)  \\
&amp;=\arg\min_j(-2\parallel \vec{x}_n\parallel\parallel \vec{\mu}_j\parallel + \parallel\vec{\mu}_j\parallel^2)   \\
&amp;=\arg\max_j(\vec{x}_n^T\vec{\mu}_j-\dfrac{\vec{\mu}_j^T\vec{\mu}_j}{2})\end{aligned}\]</span> Now consider the optimization of the <span class="math inline">\(\vec{\mu}_k\)</span> with the <span class="math inline">\(_{nk}\)</span> held fixed.Setting the objective function <span class="math inline">\(J\)</span>’s derivative with respect to <span class="math inline">\(\vec{\mu}_k\)</span> to zero giving: <span class="math display">\[\begin{aligned}
    \nabla_{\vec{\mu}_k} J
    &amp;=2\sum_{n=1}^{N}r_{nk}(\vec{x}_n-\vec{\mu}_k)=0 \\
    \Rightarrow 
    \vec{\mu}_k &amp;=\dfrac{\sum_n{r_{nk}\vec{x}_n}}{\sum_n{r_{nk}}} \\
    \Rightarrow
    \vec{\mu}_k^T &amp;= \dfrac{\sum_n{r_{nk}\vec{x}_n^T}}{\sum_n{r_{nk}}}\end{aligned}\]</span> which can be vectorized when implementing.</p>
<p>[H]</p>
<p>initialize <span class="math inline">\(\vec{\mu}_k\)</span></p>
<p>The objective may converge to a local rather than global minimum(MacQueen 1967).</p>
<p>Derive an on-line stochastic rather than the batch version of <span class="math inline">\(K\)</span>-means,by applying Robbin-Monro procedure : <span class="math display">\[\begin{aligned}
\vec{\mu}_k^{new} = 
\vec{\mu}_k^{old} + \eta_n(\vec{x}_n-\vec{\mu}_k^{old})\end{aligned}\]</span> where <span class="math inline">\(\eta_n\)</span> is the learning rate parameter,which is typically made to decrease monotonically as more data points are considered.</p>
<p>Generalize the <span class="math inline">\(K\)</span>-means algorithm,introducing a general dissimilarity measure <span class="math inline">\(\nu(\vec{x},\vec{x}&#39;)\)</span> <span class="math display">\[\begin{aligned}
\hat{J} = \sum_{n=1}^{N}\sum_{k=1}^{K}r_{nk}\nu(\vec{x}_n,\vec{\mu}_k)\end{aligned}\]</span> which gives the <strong><span class="math inline">\(K\)</span>-medoids</strong> algorithm.The computation cost of E step is <span class="math inline">\(O(KN)\)</span>.The M step is potentially more complex.So it is common to restrict each cluster prototype to be equal to one of the data vectors assigned to that cluster,requiring <span class="math inline">\(O(N_k^2)\)</span> evaluations of <span class="math inline">\(\nu(\cdot,\cdot)\)</span>.</p>
<h2 id="mixtures-of-gaussians">Mixtures of Gaussians</h2>
<h3 id="representation-1">representation</h3>
<p>The Gaussian mixture distribution can be written as a linear superposition of Gaussians in the form <span class="math display">\[\begin{aligned}
\label{def:Gaussian mixture}
p(\vec{x}) =
\sum_{k=1}^{K}\pi_k\mathcal{N}(\vec{x}|\vec{\mu}_k,\vec{\Sigma}_k)\end{aligned}\]</span> Introduce a <span class="math inline">\(K\)</span>-dimensional binary random variable <span class="math inline">\(\vec{z}\)</span> having a <span class="math inline">\(1\)</span>-of-<span class="math inline">\(K\)</span> representation. <span class="math display">\[\begin{aligned}
z_k &amp;\in \{ 0,1\} \\
\sum_k z_k &amp;=1\end{aligned}\]</span> The marginal distribution over <span class="math inline">\(\vec{z}\)</span> is specified in terms of the mixing coefficients <span class="math inline">\(\pi_k\)</span>,such that <span class="math display">\[\begin{aligned}
p(z_k=1) = \pi_k\end{aligned}\]</span> where the parameters <span class="math inline">\(\{ \pi_k\}\)</span> must satisfy <span class="math display">\[\begin{aligned}
0 \leq &amp;\pi_k \leq 1\\
\sum_{k=1}^{K} &amp;\pi_k = 1\end{aligned}\]</span> Write this distribution in the form <span class="math display">\[\begin{aligned}
p(\vec{z}) = \prod_{k=1}^{K}\pi_k^{z_k}\end{aligned}\]</span> The conditional distribution of <span class="math inline">\(\vec{x}\)</span> given a particular value for <span class="math inline">\(\vec{z}\)</span> is a Gaussian: <span class="math display">\[\begin{aligned}
p(\vec{x}|z_k=1) &amp;= \mathcal{N}(\vec{x}|\vec{\mu}_k,\vec{\Sigma}_k) \\
p(\vec{x}|\vec{z}) 
&amp;=\prod_{k=1}^{K}\mathcal{N}(\vec{x}|\vec{\mu}_k,\vec{\Sigma}_k)^{z_k}\end{aligned}\]</span> The joint distribution is given by <span class="math inline">\(p(\vec{z})p(\vec{x}|\vec{z})\)</span>,and the marginal distribution of <span class="math inline">\(\vec{x}\)</span> is obtained by summing the joint distribution over states of <span class="math inline">\(\vec{z}\)</span>: <span class="math display">\[\begin{aligned}
p(\vec{x}) = \sum_{\vec{z}}{p(\vec{z})p(\vec{x}|\vec{z})} =
\sum_{k=1}^{K}\pi_k\mathcal{N}(\vec{x}|\vec{\mu}_k,\vec{\Sigma}_k)\end{aligned}\]</span> which is a Gaussian mixture [def:Gaussian mixture] .</p>
<p>The conditional probability of <span class="math inline">\(\vec{z}\)</span> given <span class="math inline">\(\vec{x}\)</span> can be evaluated by Bayes’ theorem <span class="math display">\[\begin{aligned}
\label{eqn:responsibility}
\gamma(z_k)\equiv p(z_k=1|\vec{x}) 
&amp;=\dfrac{p(z_k=1)p(\vec{x}|z_k=1)} {\sum_{j=1}^{K}p(z_j=1)p(\vec{x}|z_j=1)} \\
&amp;=\dfrac{\pi_k \mathcal{N}(\vec{x}|\vec{\mu}_k,\vec{\Sigma}_k)}  
    {\sum_{j=1}^{K}\pi_j\mathcal{N}(\vec{x}|\vec{\mu}_j,\vec{\Sigma}_j)}\end{aligned}\]</span> We shall view <span class="math inline">\(\pi_k\)</span> as the prior probability of <span class="math inline">\(z_k=1\)</span>,and the quantity <span class="math inline">\(\gamma(z_k)\)</span> as the corresponding posterior.<span class="math inline">\(\gamma(z_k)\)</span> can also be viewed as the <strong>responsibility</strong> that component <span class="math inline">\(k\)</span> takes for ’explaining’ the observation <span class="math inline">\(\vec{x}\)</span>.</p>
<h3 id="maximum-likelihood">Maximum likelihood</h3>
<p>Assuming that the data points are drawn independently from the same distribution(i.i.d),the log of the likelihood function is given by <span class="math display">\[\begin{aligned}
\label{eqn:Gaussian mixture log likelihood}
\ln(p(\vec{X}|\vec{\pi},\vec{\mu},\vec{\Sigma})) =
\sum_{n=1}^{N}\ln\{\sum_{k=1}^{K}\pi_k\mathcal{N}(\vec{x}_n|\vec{\mu}_k,\vec{\Sigma}_k) \}\end{aligned}\]</span></p>
<p>It is worth emphasizing that there are significant problems associated with the maximum likelihood framework applied to GMM,due to the presence of <strong>singularities</strong>.If one of the components of the mixture model ’collapse’ onto a specific data point(<span class="math inline">\(\sigma_j\rightarrow 0\)</span>),the log likelihood function will go to infinity.These singularities provide another example of severe over-fitting of maximum likelihood approach.A further issue is <strong>identifiability</strong>(Casella and Berger,2002),which is that a <span class="math inline">\(K\)</span>-component mixture results in <span class="math inline">\(K!\)</span> equivalent solutions.</p>
<h3 id="em-for-gaussian-mixtures">EM for Gaussian mixtures</h3>
<p>An elegant and powerful method for finding the maximum likelihood solutions for models with latent variables is called the <strong>expectation-maximization</strong>,or EM algorithm.</p>
<p>Begin by writing down the conditions that must be satisfied at a maximum of the likelihood function.Setting the derivatives of <span class="math inline">\(\ln p(\vec{X}|\vec{\pi},\vec{\mu},\vec{\Sigma})\)</span> [eqn:Gaussian mixture log likelihood] with respect to the means <span class="math inline">\(\vec{\mu}_k\)</span> of the Gaussian components to zero,we obtain <span class="math display">\[\begin{aligned}
\because 0&amp;=-\sum_{n=1}^{N}\dfrac{\pi_k\mathcal{N}(\vec{x}_n|\vec{\mu}_k,\vec{\Sigma}_k)}
{\sum_{j=1}^{K}\pi_j\mathcal{N}(\vec{x}_n|\vec{\mu}_j,\vec{\Sigma}_j)}
\vec{\Sigma}_k^{-1}(\vec{x}_n-\vec{\mu}_k) \\
\therefore 0&amp;=-\sum_{n=1}^{N}\gamma(z_{nk})(\vec{x}_n-\vec{\mu}_k) \\
\because
N_k&amp;=\sum_{n=1}^{N}\gamma(z_{nk}) \\
\therefore \vec{\mu}_k&amp;=\dfrac{1}{N_k}\sum_{n=1}^{N}\gamma(z_{nk})\vec{x}_n \\\end{aligned}\]</span> where we have made use of <span class="math inline">\(\dfrac{de^x}{dx}=e^x\)</span> and [eqn:multivariate Gaussian].We can interpret <span class="math inline">\(N_k\)</span> as the effective number of points assigned to cluster <span class="math inline">\(k\)</span>.</p>
<p>Now set the derivative of <span class="math inline">\(\ln(\vec{X}|\vec{\pi},\vec{\mu},\vec{\Sigma})\)</span> with respect to <span class="math inline">\(\vec{\Sigma}_k\)</span> to zero: <span class="math display">\[\begin{aligned}
\dfrac{\partial\ln p(\vec{X}|\vec{\pi},\vec{\mu},\vec{\Sigma})}{\partial\vec{\Sigma}_k} 
&amp;=\sum_{n=1}^{N}\dfrac{\dfrac{\partial(\pi_k\mathcal{N}(\vec{x}_n|\vec{\mu}_k,\vec{\Sigma}_k))}{\partial\vec{\Sigma}_k}}
{\sum_{j=1}^{K}\pi_k\mathcal{N}(\vec{x}_n|\vec{\mu}_k,\vec{\Sigma}_k)}\\
\because
 \dfrac{\partial \ln y}{\partial x} &amp;= \dfrac{1}{y}\dfrac{\partial y}{\partial x} \\
\therefore \dfrac{\partial y}{\partial x} &amp;= y\dfrac{\partial\ln y}{\partial x}\\
\because \dfrac{\partial(\pi_k\mathcal{N}(\vec{x}_n|\vec{\mu}_k,\vec{\Sigma}_k))}{\partial\vec{\Sigma}_k} 
&amp;= \pi_k\dfrac{\partial\mathcal{N}(\vec{x}_n|\vec{\mu}_k,\vec{\Sigma}_k)}{\partial\vec{\Sigma}_k}\\
&amp;= \pi_k\mathcal{N}(\vec{x}_n|\vec{\mu}_k,\vec{\Sigma}_k)
\dfrac{\partial\ln\mathcal{N}(\vec{x}_n|\vec{\mu}_k,\vec{\Sigma}_k)}{\partial\vec{\Sigma}_k}\\
\because 
\mathcal{N}(\vec{x}|\vec{\mu},\vec{\Sigma})
&amp;=\dfrac{1}{(2\pi)^{D/2}}\dfrac{1}{\mid \vec{\Sigma}\mid^{1/2}}
\exp\{-\dfrac{1}{2}(\vec{x}-\vec{\mu})^T\vec{\Sigma}^{-1}(\vec{x}-\vec{\mu}) \} \\
\therefore \dfrac{\partial\ln\mathcal{N}(\vec{x}_n|\vec{\mu}_k,\vec{\Sigma}_k)}{\partial\vec{\Sigma}_k}
&amp;=-\dfrac{1}{2}(\dfrac{\ln(\mid\vec{\Sigma}_k)\mid}{\partial\vec{\Sigma}_k}
 +\dfrac{\partial(\vec{x}_n-\vec{\mu}_j)\vec{\Sigma}^{-1}(\vec{x}_n-\vec{\mu}_j)}{\partial\vec{\Sigma}_k}) \\
\because
 \dfrac{\partial det(\vec{X})}{\vec{X}} &amp;=det(\vec{X})(\vec{X}^{-1})^T \\
\dfrac{\partial\vec{a}^T\vec{X}^{-1}\vec{b}}{\partial\vec{X}}
&amp;= -\vec{X}^{-T}\vec{a}\vec{b}^T\vec{X}^{-T} \\
\therefore 
\dfrac{\ln(\mid\vec{\Sigma}_k)\mid}{\partial\vec{\Sigma}_k}
&amp;=\dfrac{\dfrac{\partial\mid\vec{\Sigma}_k\mid}{\partial\vec{\Sigma}_k}}{\mid\vec{\Sigma}_k\mid} =\dfrac{\mid\vec{\Sigma}_k\mid\vec{\Sigma}_k^{-T}}{\mid\vec{\Sigma}_k\mid} =\vec{\Sigma}_k^{-1}\\
\therefore 
\dfrac{\partial\ln\mathcal{N}(\vec{x}_n|\vec{\mu}_k,\vec{\Sigma}_k)}{\partial\vec{\Sigma}_k} 
&amp;=-\dfrac{1}{2}(\vec{\Sigma}_k^{-1}-\vec{\Sigma}_k^{-1}(\vec{x}_n-\vec{\mu}_k)(\vec{x}_n-\vec{\mu}_k)^T\vec{\Sigma}_k^{-1}) \\
\therefore derivative&amp;=\sum_{n=1}^{N}\gamma(z_{nk})(-1/2)[\vec{\Sigma}_k^{-1}-\vec{\Sigma}_k^{-1}(\vec{x}_n-\vec{\mu}_k)(\vec{x}_n-\vec{\mu}_k)^T\vec{\Sigma}_k^{-1}] =0 \\
\therefore \vec{\Sigma}_k&amp;=\dfrac{1}{N_k}\sum_{n=1}^{N}\gamma(z_{nk})(\vec{x}_n-\vec{\mu}_k)(\vec{x}_n-\vec{\mu}_k)^T\end{aligned}\]</span> where we have made use of [eqn:multivariate Gaussian] and <strong>The Matrix Cookbook,2012</strong>.</p>
<p>Finally,we maximize <span class="math inline">\(\ln p(\vec{X}|\vec{\pi},\vec{\mu},\vec{\Sigma})\)</span> [eqn:Gaussian mixture log likelihood] with respect to the mixing coefficients <span class="math inline">\(\pi_k\)</span>.Achieve this using a Lagrange multiplier <span class="math display">\[\begin{aligned}
\ln p(\vec{X}|\vec{\pi},\vec{\mu},\vec{\Sigma}) +\lambda(\sum_{k=1}^{K}\pi_k-1) &amp;=0 \\
\therefore \sum_{n=1}^{N}\dfrac{\mathcal{N}(\vec{x}_n|\vec{\mu}_k,\vec{\Sigma}_k)}
{\sum_{j=1}^{K}\pi_j\mathcal{N}(\vec{x}_n|\vec{\mu}_j,\vec{\Sigma}_j)} +\lambda &amp;= 0\end{aligned}\]</span> where again we see the appearance of the responsibilities [eqn:responsibility].If we now multiply both sides by <span class="math inline">\(\pi_k\)</span> and sum over <span class="math inline">\(k\)</span> making use of constraint <span class="math inline">\(\sum_{k=1}^{K}\pi_k = 1\)</span>,we find <span class="math inline">\(\lambda = -N\)</span>.Then <span class="math display">\[\begin{aligned}
\pi_k = \dfrac{N_k}{N}\end{aligned}\]</span> There do suggest s ample iterative scheme for finding a solution to the maximum likelihood problem.</p>
<p>[H]</p>
<h2 id="latent-variables-view-of-em">Latent Variables View of EM</h2>
<h3 id="representation-2">representation</h3>
<p>We denote the set of all observed data by <span class="math inline">\(\vec{X}\)</span>,in which the <span class="math inline">\(n^{th}\)</span> row represents <span class="math inline">\(\vec{x}_n^T\)</span>,and denote the set of all latent variables by <span class="math inline">\(\vec{Z}\)</span>,with corresponding rows <span class="math inline">\(\vec{z}_n^T\)</span>.The set of all model parameters is denoted by <span class="math inline">\(\vec{\theta}\)</span>,so the log likelihood function is given by <span class="math display">\[\begin{aligned}
\ln p(\vec{X}|\vec{\theta}) &amp;=
\ln\{\sum_{\vec{z}}p(\vec{X},\vec{Z}|\vec{\theta}) \}\end{aligned}\]</span> Replace the sum over <span class="math inline">\(\vec{Z}\)</span> with an integral for continuous latent variables.</p>
<p><span class="math inline">\(\{\vec{X},\vec{Z}\}\)</span> is called the <strong>complete</strong> data set,and <span class="math inline">\(\{\vec{X}\}\)</span> is <strong>incomplete</strong>.We consider complete-data log likelihood functions’ expected value under the posterior distribution of the latent variables,corresponding to the E step.In the subsequent M step,maximize this expectation.The expectation of complete-data log likelihood <span class="math display">\[\begin{aligned}
\mathcal{Q}(\vec{\theta},\vec{\theta}^{old}) &amp;=
\sum_{\vec{z}}p(\vec{Z}|\vec{X},\vec{\theta}^{old})\ln p(\vec{X},\vec{Z}|\vec{\theta})\end{aligned}\]</span> Maximize this <span class="math display">\[\begin{aligned}
\vec{\theta}^{new} &amp;=\arg\max\limits_{\vec{\theta}}\mathcal{Q}(\vec{\theta},\vec{\theta}^{old})\end{aligned}\]</span></p>
<p>[H] [General EM algorithm]</p>
<p>When finding MAP(maximum posterior) solutions,maximize the quantity <span class="math inline">\(\mathcal{Q}(\vec{\theta},\vec{\theta}^{old})+\ln p(\vec{\theta})\)</span> in M step.</p>
<h2 id="the-em-algorithm-in-general">The EM Algorithm in General</h2>
<p>The <strong>expectation maximization</strong> algorithm,or EM,is a general technique for finding maximum likelihood solutions for probabilistic models having latent variables.</p>
<p>Given the same setting as previous,our goal is to maximize the likelihood function <span class="math display">\[\begin{aligned}
p(\vec{X}|\vec{\theta}) &amp;=\sum_{\vec{Z}}p(\vec{X},\vec{Z}|\vec{\theta}) \end{aligned}\]</span> Replace the summation by integration as appropriate to apply to continuous latent variables.</p>
<p>Introduce a distribution <span class="math inline">\(q(\vec{Z})\)</span> over the latent variables,and the decompose the log complete-data likelihood function <span class="math display">\[\begin{aligned}
\because \ln p(\vec{X}|\vec{\theta}) &amp;=\ln p(\vec{X},\vec{Z}|\vec{\theta}) -\ln p(\vec{Z}|\vec{X},\vec{\theta}) \\\end{aligned}\]</span> take the expectation over values of <span class="math inline">\(\vec{Z}\)</span> by multiplying both sides by <span class="math inline">\(q(\vec{Z})\)</span> and summing(or integrting) over <span class="math inline">\(\vec{Z}\)</span> we get <span class="math display">\[\begin{aligned}
\ln p(\vec{X}|\vec{\theta}) 
&amp;=\sum_{\vec{Z}}q(\vec{Z})\ln p(\vec{X}|\vec{\theta}) \\
&amp;=\sum_{\vec{Z}}q(\vec{Z})\ln p(\vec{X},\vec{Z}|\vec{\theta})-\sum_{\vec{Z}}q(\vec{Z})\ln p(\vec{Z}|\vec{X}) \\
&amp;=\sum_{\vec{Z}}q(\vec{Z})(\ln \dfrac {p(\vec{X},\vec{Z})}{p(\vec{Z})}+\ln p(\vec{Z}))-\sum_{\vec{Z}}q(\vec{Z})\ln p(\vec{Z}|\vec{X}) \\
&amp;=Q+H \\
&amp;=\mathcal{L}(q,\vec{\theta})+KL(q\parallel p)\end{aligned}\]</span> where <span class="math display">\[\begin{aligned}
\label{eqn:complete-data log likelihood decomposition}
\mathcal{L}(q,\vec{\theta})
&amp;=\sum_{\vec{Z}}q(\vec{Z})\ln
\{\dfrac{p(\vec{X},\vec{Z}|\vec{\theta})}{q(\vec{Z})} \} \\
KL(q\parallel p)
&amp;=-\sum_{\vec{Z}}{q(\vec{Z})\ln
    \{\dfrac{p(\vec{Z}|\vec{X},\vec{\theta})}
         {q(\vec{Z})}\}}\end{aligned}\]</span> Note that <span class="math inline">\(\mathcal{L}(q,\vec{\theta})\)</span> is a functional of the distribution <span class="math inline">\(p(\vec{Z})\)</span> and KL is the Kullback-Leibler divergence between <span class="math inline">\(q(\vec{Z})\)</span> and the posterior <span class="math inline">\(p(\vec{Z}|\vec{X},\vec{\theta})\)</span>.KL divergence will be nonnegative according to Jenson’s inequality about convex function. <span class="math display">\[\begin{aligned}
\because KL(q\parallel q)&amp;\geq,\text{with equality if,and only if,}q(\vec{Z})=p(\vec{Z}|\vec{X},\vec{\theta})\\
\therefore
\mathcal{L} (q,\vec{\theta})&amp;\leq\ln p(\vec{X}|\vec{\theta})\end{aligned}\]</span> in other words that <span class="math inline">\(\mathcal{L}(q,\vec{\theta})\)</span> is a lower bound on <span class="math inline">\(\ln p(\vec{X}|\vec{\theta})\)</span>.</p>
<p>The EM is a two-stage <strong>iterative</strong> optimization technique for finding <strong>maximum likelihood</strong> solutions.With the decomposition [eqn:complete-data log likelihood decomposition], in the E step,the <strong>lower bound</strong> <span class="math inline">\(\mathcal{L}(q,\vec{\theta}^{old})\)</span> is maximized with respect to <span class="math inline">\(q(\vec{Z})\)</span> holding <span class="math inline">\(\vec{\theta}^{old}\)</span> fixed.</p>
<p>In the subsequent M step,the distribution <span class="math inline">\(q(\vec{Z})\)</span> is held fixed and the lower bound <span class="math inline">\(\mathcal{L}(q,\vec{\theta})\)</span> is maximized with respect to <span class="math inline">\(\vec{\theta}\)</span>.The nonzero KL divergence causes the log likelihood function increase more than the lower bound,as shown in figure</p>
<p><img src="prml/Figure9.13.jpg" alt="image" /></p>
<p>Substitute <span class="math inline">\(q(\vec{Z}) = p(\vec{Z}|\vec{X},\vec{\theta}^{old})\)</span> into the lower bound,then after the E step, <span class="math display">\[\begin{aligned}
\mathcal{L}(q,\vec{\theta}) 
&amp;= \sum_{\vec{Z}}p(\vec{Z}|\vec{X},\vec{\theta}^{old})\ln p(\vec{X},\vec{Z}|\vec{\theta})
    -\sum_{\vec{Z}}p(\vec{Z}|\vec{X},\vec{\theta}^{old})\ln p(\vec{Z}|\vec{X},\vec{\theta}^{old}) \\
&amp;= \mathcal{Q}(\vec{\theta},\vec{\theta}^{old})  + const\end{aligned}\]</span> where the constant is the negative entropy of the <span class="math inline">\(q\)</span> distribution.</p>
<p><img src="prml/Figure9.14.jpg" alt="image" /></p>
<p>For the particular case of independent,identically distributed data set,<span class="math inline">\(\vec{X}=(\vec{x}_1^T,\vec{x}_2^T,\cdots,\vec{x}_n^T)^T\)</span> and <span class="math inline">\(\vec{Z}=(\vec{z}_1^T,\vec{z}_2^T,\cdots,\vec{z}_n^T)^T\)</span>.Using the sum and product rules,the posterior in E step take the form <span class="math display">\[\begin{aligned}
p(\vec{Z}|\vec{X},\vec{\theta}) &amp;= \dfrac{p(\vec{X},\vec{Z}|\vec{\theta})}{\sum_{\vec{Z}}p(\vec{X},\vec{Z}|\vec{\theta})}
=\dfrac{\prod_{n=1}^{N}p(\vec{x}_n,\vec{z}_n|\vec{\theta})}{\sum_{\vec{Z}}\prod_{n=1}^{N}p(\vec{x}_n,\vec{z}_n|\vec{\theta})}
=\prod_{n=1}^{N}p(\vec{z}_n|\vec{x}_n,\vec{\theta})\end{aligned}\]</span> will also factorize with respect to <span class="math inline">\(n\)</span>.</p>
<h3 id="maximum-posterior">Maximum posterior</h3>
<p>Maximize the posterior distribution <span class="math inline">\(p(\vec{\theta}|\vec{X})\)</span> with EM for models with a prior <span class="math inline">\(p(\vec{\theta})\)</span> over parameters. <span class="math display">\[\begin{aligned}
\because p(\vec{\theta}|\vec{X})&amp;=p(\vec{\theta},\vec{X})/p(\vec{X}) \\
\therefore \ln p(\vec{\theta}|\vec{X})&amp;=\ln p(\vec{\theta},\vec{X})-\ln p(\vec{X})
    =\ln p(\vec{\theta})+\ln p(\vec{X}|\vec{\theta})-\ln p(\vec{X})\end{aligned}\]</span> decomposed as <span class="math display">\[\begin{aligned}
\ln p(\vec{\theta}|\vec{X}) &amp;= \mathcal{L}(q,\vec{\theta})+KL(q\parallel p) +\ln p(\vec{\theta}) -\ln p(\vec{X})\\
&amp;\geq \mathcal{L}(q,\vec{\theta}) +\ln p(\vec{\theta}) -\ln p(\vec{X})\end{aligned}\]</span> where <span class="math inline">\(\ln p(\vec{X})\)</span> is constant.We can optimize the right-hand side alternatively with respect to <span class="math inline">\(q\)</span> and <span class="math inline">\(\vec{\theta}\)</span> in E-step and M-step.</p>
<h3 id="generalized-em">generalized EM</h3>
<p>The <strong>generalized EM</strong>,or GEM,algorithm addresses the problem of an intractable M step.Instead of aiming to maximize <span class="math inline">\(\mathcal{L}(q,\vec{\theta})\)</span> with respect to <span class="math inline">\(\vec{\theta}\)</span>,it seeks instead to change the parameters in such a way as to increase its value.</p>
<ul>
<li><p>Nonlinear optimization. conjugate gradients algorithm in M step</p></li>
<li><p>expectation conditional maximization(ECM). Making several constrained optimizations within each M step.</p></li>
</ul>
<p>Generalize the E step by performing a <strong>partial</strong>,rather than complete, optimization of <span class="math inline">\(\mathcal{L}(q,\vec{\theta})\)</span> with respect to <span class="math inline">\(q(\vec{Z})\)</span>.</p>
</body>
</html>
