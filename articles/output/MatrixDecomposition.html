<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>
<h1 id="chapter:Matrix Decomposition">Matrix Decomposition</h1>
<h2 id="lu-decomposition">LU Decomposition</h2>
<h2 id="qr-decomposition">QR Decomposition</h2>
<h2 id="cholesky-decomposition">Cholesky Decomposition</h2>
<h2 id="eigen-value-decomposition">Eigen value Decomposition</h2>
<p>An <span class="math inline">\(\mathbf{eigenvector}\)</span> of an <span class="math inline">\(n\times n\)</span> matrix <span class="math inline">\(A\)</span> is a nonzero vector <span class="math inline">\(\vec{x}\)</span> such that <span class="math inline">\(A\vec{x}=\lambda\vec{x}\)</span> for some scalar <span class="math inline">\(\lambda\)</span>.A scalar <span class="math inline">\(\lambda\)</span> is called <span class="math inline">\(\mathbf{eigenvalue}\)</span> of <span class="math inline">\(A\)</span> if there is a nontrivial solution of <span class="math inline">\(A\vec{x}=\lambda\vec{x}\)</span>;such an is called an eigenvector corresponding to <span class="math inline">\(\lambda\)</span><a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>.</p>
<h2 id="singular-value-decomposition">Singular Value Decomposition</h2>
<h3 id="definition">Definition</h3>
<p>Any matrix can be decomposed as follows <span class="math display">\[\label{eqn:SVD}
    \underbrace{\vec{X}}_{N \times D}=\underbrace{\vec{U}}_{N \times N}\underbrace{\vec{\Sigma}}_{N \times D}\underbrace{\vec{V}^T}_{D \times D}\]</span> where <span class="math inline">\(\vec{U}\)</span> is an <span class="math inline">\(N \times N\)</span> matrix whose columns are orthornormal(so <span class="math inline">\(\vec{U}^T\vec{U}=\vec{I}\)</span>), <span class="math inline">\(\vec{V}\)</span> is <span class="math inline">\(D \times D\)</span> matrix whose rows and columns are orthonormal (so <span class="math inline">\(\vec{V}^T\vec{V}=\vec{V}\vec{V}^T=\vec{I}_D\)</span>), and <span class="math inline">\(\vec{\Sigma}\)</span> is a <span class="math inline">\(N \times D\)</span> matrix containing the <span class="math inline">\(r=\min(N,D)\)</span> singular values <span class="math inline">\(\sigma_i \geq 0\)</span> on the main diagonal, with 0s filling the rest of the matrix.</p>
<h3 id="proof">Proof</h3>
<p>Let <span class="math inline">\(A\)</span> be an <span class="math inline">\(m\times n\)</span> matrix.Then <span class="math inline">\(A^TA\)</span> is symmetric and can be orthogonally diagonalized with eigenvectors.The <strong>singular values</strong> of <span class="math inline">\(A\)</span> are the square root of the eigenvalues of <span class="math inline">\(A^TA\)</span>,denoted by <span class="math inline">\(\sigma_1,\sigma_2,...,\sigma_n\)</span>.That is <span class="math inline">\(\sigma_i = \sqrt{\lambda_i}\)</span> for <span class="math inline">\(1\leq i \leq n\)</span>. The eigenvalues are usually arranged so that <span class="math display">\[\lambda_1 \geq \lambda_2 \geq...\geq \lambda_n \geq 0\]</span></p>
<p>Suppose <span><span class="math inline">\(\vec{v}_1,\vec{v_2}...,\vec{v_n}\)</span></span> is an orthogonal basis of <span class="math inline">\(\mathbb{R}^n\)</span> consisting of eigenvector of <span class="math inline">\(A^TA\)</span>,arranged so that the corresponding eigenvalues of <span class="math inline">\(A^TA\)</span> satisfy <span class="math inline">\(\lambda_1 \geq \lambda_2 \geq...\geq \lambda_n \geq 0\)</span> and suppose <span class="math inline">\(A\)</span> has r nonzero singular values.Then <span><span class="math inline">\(A\vec{v_1},...,A\vec{v_r}\)</span></span> is an orthogonal basis for <span class="math inline">\(ColA\)</span>,and <span class="math inline">\(rankA=r\)</span>.</p>
<p>Because <span class="math inline">\(\mathbf{v_i}\)</span> and <span class="math inline">\(\vec{v_j}\)</span> are orthogonal for <span class="math inline">\(i\neq j\)</span>, <span class="math display">\[(A\vec{v_i})^T(A\vec{v_j}) = \vec{v_i}^TA^TA\vec{v_j} = \vec{v_i}^T(\lambda_j\vec{v_j})=0\]</span></p>
<p>We therefore have <span class="math display">\[A\vec{v_i} = \sigma_i\vec{u_i}\]</span> For a general vector <span class="math inline">\(\vec{x}\)</span>,since eigenvectors are orthogonal unit vectors,we have <span class="math display">\[\vec{x} = (\vec{v_1}\cdot\vec{x})\vec{v_1} + (\vec{v_2}\cdot\vec{x})\vec{v_2} +...+(\vec{v_n}\cdot\vec{x})\vec{v_n}\]</span> This means that <span class="math display">\[\begin{aligned}
&amp; M\vec{x} = (\vec{v_1}\cdot\vec{x})M\vec{v_1} + (\vec{v_2}\cdot\vec{x})M\vec{v_2} +...+(\vec{v_n}\cdot\vec{x})M\vec{v_n} \\
&amp; M\vec{x} = (\vec{v_1}\cdot\vec{x})\sigma_1\vec{u_1} + (\vec{v_2}\cdot\vec{x})\sigma_2\vec{u_2} +...+(\vec{v_n}\cdot\vec{x})\sigma_n\vec{u_n}\end{aligned}\]</span> Remember that dot product can be computed using the vector transpose <span class="math display">\[\vec{v}\cdot\vec{u} = \vec{v^T}\vec{u}\]</span> which leads to <span class="math display">\[\begin{aligned}
&amp; M\vec{x} = \vec{u_1}\sigma_1\vec{v_1^T}\vec{x}+\vec{u_2}\sigma_2\vec{v_2^T}\vec{x}+...+\vec{u_n}\sigma_n\vec{v_n^T}\vec{x} \\
&amp;  M = \vec{u_1}\sigma_1\vec{v_1^T}+\vec{u_2}\sigma_2\vec{v_2^T}+...+\vec{u_n}\sigma_n\vec{v_n^T}\end{aligned}\]</span> And this is usually expressed by writing <span class="math display">\[M = U\Sigma V^T\]</span> As for <span class="math inline">\(\vec{u_i}\)</span>,we have <span class="math display">\[\begin{aligned}
&amp;\begin{cases}
(A^TA)\vec{v_i} = \lambda_i\vec{v_i}    \\
A\vec{v_i}    = \sigma_i\vec{u_i}   \\
\end{cases} \\
&amp;\Rightarrow A^T\sigma_i\vec{u_i} = \lambda_i\vec{v_i} \\
&amp;\Rightarrow \sigma_iA^T\vec{u_i} = \lambda_i\vec{v_i}\\
&amp;\Rightarrow (AA^T)\vec{u_i}=\sigma_iA\vec{v_i} = \lambda_i\vec{u_i}\\\end{aligned}\]</span> So we can see that <span class="math inline">\(\vec{u_i}\)</span> is the eigenvector of symmetric matrix <span class="math inline">\(AA^T\)</span>,and <span class="math inline">\(\vec{v_i}\)</span> is the eigenvector of symmetric matrix <span class="math inline">\(A^TA\)</span>.In summary,<span class="math inline">\(\vec{u_i}\)</span> and are the <strong>left-eigenvector</strong> and <strong>right-eigenvectors</strong> of matrix <span class="math inline">\(A\)</span>.</p>
<h3 id="application">Application</h3>
<h4 id="principal-component-analysis">Principal Component Analysis</h4>
<p>The projection vectors for principal component projection are the left-eigenvectors</p>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>An eigenvalue may be zero<a href="#fnref1">â†©</a></p></li>
</ol>
</div>
</body>
</html>
