<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>
<h1 id="chap:Optimization-methods">Optimization methods</h1>
<h2 id="sec:Convexity">Convexity</h2>
<p>(<strong>Convex set</strong>) We say aset <span class="math inline">\(\mathcal{S}\)</span> is convex if for any <span class="math inline">\(\vec{x}_1, \vec{x}_2 \in \mathcal{S}\)</span>, we have <span class="math display">\[\lambda\vec{x}_1+(1-\lambda)\vec{x}_2 \in \mathcal{S}, \forall \lambda \in [0,1]\]</span></p>
<p>(<strong>Convex function</strong>) A function <span class="math inline">\(f(\vec{x})\)</span> is called convex if its <strong>epigraph</strong>(the set of points above the function) defines a convex set. Equivalently, a function <span class="math inline">\(f(\vec{x})\)</span> is called convex if it is defined on a convex set and if, for any <span class="math inline">\(\vec{x}_1, \vec{x}_2 \in \mathcal{S}\)</span>, and any <span class="math inline">\(\lambda \in [0,1]\)</span>, we have <span class="math display">\[f(\lambda\vec{x}_1+(1-\lambda)\vec{x}_2) \leq \lambda f(\vec{x}_1)+(1-\lambda)f(\vec{x}_2)\]</span></p>
<p>A function <span class="math inline">\(f(\vec{x})\)</span> is said to be <strong>strictly convex</strong> if the inequality is strict <span class="math display">\[f(\lambda \vec{x}_1 + (1 - \lambda)\vec{x}_2) &lt; \lambda f(\vec{x}_1) + (1 - \lambda)f(\vec{x}_2)\]</span></p>
<p>A function <span class="math inline">\(f(\vec{x})\)</span> is said to be (strictly) <strong>concave</strong> if <span class="math inline">\(-f(\vec{x})\)</span> is (strictly) convex.</p>
<p>If <span class="math inline">\(f(x)\)</span> is twice differentiable on <span class="math inline">\([a, b]\)</span> and <span class="math inline">\(f&#39;&#39;(x) \geq 0\)</span> on <span class="math inline">\([a, b]\)</span> then <span class="math inline">\(f(x)\)</span> is convex on <span class="math inline">\([a, b]\)</span>.</p>
<p><span class="math inline">\(\log(x)\)</span> is strictly convex on <span class="math inline">\((0, \infty)\)</span>.</p>
<p>Intuitively, a (strictly) convex function has a “bowl shape”, and hence has a unique global minimum <span class="math inline">\(x^*\)</span> corresponding to the bottom of the bowl. Hence its second derivative must be positive everywhere, <span class="math inline">\(\frac{\mathrm{d}^2}{\mathrm{d}x^2}f(x)&gt;0\)</span>. A twice-continuously differentiable, multivariate function <span class="math inline">\(f\)</span> is convex iff its Hessian is positive definite for all <span class="math inline">\(\vec{x}\)</span>. In the machine learning context, the function <span class="math inline">\(f\)</span> often corresponds to the NLL.</p>
<p>Models where the NLL is convex are desirable, since this means we can always find the globally optimal MLE. We will see many examples of this later in the book. However, many models of interest will not have concave likelihoods. In such cases, we will discuss ways to derive locally optimal parameter estimates.</p>
<h2 id="sec:Gradient-descent">Gradient descent</h2>
<h3 id="stochastic-gradient-descent">Stochastic gradient descent</h3>
<p>[htbp]</p>
<p><span class="math inline">\(\vec{w} \leftarrow 0;\; b \leftarrow 0;\; k \leftarrow 0\)</span></p>
<h3 id="batch-gradient-descent">Batch gradient descent</h3>
<h3 id="line-search">Line search</h3>
<p>The <strong>line search</strong><a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> approach first finds a descent direction along which the objective function <span class="math inline">\(f\)</span> will be reduced and then computes a step size that determines how far <span class="math inline">\(\vec{x}\)</span> should move along that direction. The descent direction can be computed by various methods, such as gradient descent(Section [sec:Gradient-descent]), Newton’s method(Section [sec:Newtons-method]) and Quasi-Newton method(Section [sec:Quasi-Newton-method]). The step size can be determined either exactly or inexactly.</p>
<h3 id="momentum-term">Momentum term</h3>
<h2 id="lagrange-duality">Lagrange duality</h2>
<h3 id="primal-form">Primal form</h3>
<p>Consider the following, which we’ll call the <strong>primal</strong> optimization problem: <span class="math display">\[\begin{aligned}
xyz\end{aligned}\]</span></p>
<h3 id="dual-form">Dual form</h3>
<h2 id="sec:Newtons-method">Newton’s method</h2>
<p><span class="math display">\[\begin{aligned}
f(\vec{x})&amp; \approx f(\vec{x}_k)+\vec{g}_k^T(\vec{x}-\vec{x}_k)+\dfrac{1}{2}(\vec{x}-\vec{x}_k)^T\vec{H}_k(\vec{x}-\vec{x}_k) \nonumber \\
\text{where } &amp; \vec{g}_k \triangleq \vec{g}(\vec{x}_k)=f&#39;(\vec{x}_k), \vec{H}_k \triangleq \vec{H}(\vec{x}_k), \nonumber \\
              &amp; \vec{H}(\vec{x}) \triangleq \left[\dfrac{\partial^2 f}{\partial x_i \partial x_j}\right]_{D \times D} \quad \text{(Hessian matrix)} \nonumber \\
f&#39;(\vec{x})&amp; = \vec{g}_k+\vec{H}_k(\vec{x}-\vec{x}_k)=0 \Rightarrow  \label{eqn:newton-stationary-point} \\
\vec{x}_{k+1}&amp; = \vec{x}_k-\vec{H}_k^{-1}\vec{g}_k\end{aligned}\]</span></p>
<p>[htbp] Initialize <span class="math inline">\(\vec{x}_0\)</span><br />
</p>
<h2 id="sec:Quasi-Newton-method">Quasi-Newton method</h2>
<p>From Equation [eqn:newton-stationary-point] we can infer out the <strong>quasi-Newton condition</strong> as follows: <span class="math display">\[\begin{aligned}
f&#39;(\vec{x})-\vec{g}_k &amp; = \vec{H}_k(\vec{x}-\vec{x}_k) \nonumber \\
\vec{g}_{k-1}-\vec{g}_k &amp; = \vec{H}_k(\vec{x}_{k-1}-\vec{x}_k) \Rightarrow \nonumber \\
\vec{g}_k- \vec{g}_{k-1} &amp; = \vec{H}_k(\vec{x}_k-\vec{x}_{k-1}) \nonumber \\
\vec{g}_{k+1}- \vec{g}_k &amp; = \vec{H}_{k+1}(\vec{x}_{k+1}-\vec{x}_k) \quad \text{(quasi-Newton condition)} \label{eqn:quasi-Newton-condition}\end{aligned}\]</span></p>
<p>The idea is to replace <span class="math inline">\(\vec{H}_k^{-1}\)</span> with a approximation <span class="math inline">\(\vec{B}_k\)</span>, which satisfies the following properties:</p>
<ol>
<li><p><span><span class="math inline">\(\vec{B}_k\)</span> must be symmetric</span></p></li>
<li><p><span><span class="math inline">\(\vec{B}_k\)</span> must satisfies the quasi-Newton condition, i.e., <span class="math inline">\(\vec{g}_{k+1} - \vec{g}_k= \vec{B}_{k+1}(\vec{x}_{k+1}-\vec{x}_k)\)</span>.<br />
Let <span class="math inline">\(\vec{y}_k=\vec{g}_{k+1}- \vec{g}_k\)</span>, <span class="math inline">\(\vec{\delta}_k=\vec{x}_{k+1}-\vec{x}_k\)</span>, then <span class="math display">\[\vec{B}_{k+1}\vec{y}_k=\vec{\delta}_k \label{eqn:secant-equation}\]</span> </span></p></li>
<li><p><span>Subject to the above, <span class="math inline">\(\vec{B}_k\)</span> should be as close as possible to <span class="math inline">\(\vec{B_{k-1}}\)</span>.</span></p></li>
</ol>
<p>Note that we did not require that <span class="math inline">\(\vec{B}_k\)</span> be positive definite. That is because we can show that it must be positive definite if <span class="math inline">\(\vec{B_{k-1}}\)</span> is. Therefore, as long as the initial Hessian approximation <span class="math inline">\(\vec{B}_0\)</span> is positive definite, all <span class="math inline">\(\vec{B}_k\)</span> are, by induction.</p>
<h3 id="dfp">DFP</h3>
<p>Updating rule: <span class="math display">\[\vec{B}_{k+1}=\vec{B}_k+\vec{P}_k+\vec{Q}_k\]</span></p>
<p>From Equation [eqn:secant-equation] we can get <span class="math display">\[\vec{B}_{k+1}\vec{y}_k=\vec{B}_k\vec{y}_k+\vec{P}_k\vec{y}_k+\vec{Q}_k\vec{y}_k=\vec{\delta}_k\]</span></p>
<p>To make the equation above establish, just let <span class="math display">\[\begin{aligned}
\vec{P}_k\vec{y}_k &amp; = \vec{\delta}_k \\
\vec{Q}_k\vec{y}_k &amp; = -\vec{B}_k\vec{y}_k\end{aligned}\]</span></p>
<p>In DFP algorithm, <span class="math inline">\(\vec{P}_k\)</span> and <span class="math inline">\(\vec{Q}_k\)</span> are <span class="math display">\[\begin{aligned}
\vec{P}_k &amp;= \dfrac{\vec{\delta}_k\vec{\delta}_k^T}{\vec{\delta}_k^T\vec{y}_k} \\
\vec{Q}_k &amp;= -\dfrac{\vec{B}_k\vec{y}_k\vec{y}_k^T\vec{B}_k}{\vec{y}_k^T\vec{B}_k\vec{y}_k}\end{aligned}\]</span></p>
<h3 id="bfgs">BFGS</h3>
<p>Use <span class="math inline">\(\vec{B}_k\)</span> as a approximation to <span class="math inline">\(\vec{H}_k\)</span>, then the quasi-Newton condition becomes <span class="math display">\[\vec{B}_{k+1}\vec{\delta}_k=\vec{y}_k\]</span></p>
<p>The updating rule is similar to DFP, but <span class="math inline">\(\vec{P}_k\)</span> and <span class="math inline">\(\vec{Q}_k\)</span> are different. Let <span class="math display">\[\begin{aligned}
\vec{P}_k\vec{\delta}_k &amp; = \vec{y}_k \\
\vec{Q}_k\vec{\delta}_k &amp; = -\vec{B}_k\vec{\delta}_k\end{aligned}\]</span></p>
<p>Then <span class="math display">\[\begin{aligned}
\vec{P}_k &amp;= \dfrac{\vec{y}_k\vec{y}_k^T}{\vec{y}_k^T\vec{\delta}_k} \\
\vec{Q}_k &amp;= -\dfrac{\vec{B}_k\vec{\delta}_k\vec{\delta}_k^T\vec{B}_k}{\vec{\delta}_k^T\vec{B}_k\vec{\delta}_k}\end{aligned}\]</span></p>
<h3 id="broyden">Broyden</h3>
<p>Broyden’s algorithm is a linear combination of DFP and BFGS.</p>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p><a href="http://en.wikipedia.org/wiki/Line_search" class="uri">http://en.wikipedia.org/wiki/Line_search</a><a href="#fnref1">↩</a></p></li>
</ol>
</div>
</body>
</html>
