<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>
<h1 id="adaptive-basis-function-models">Adaptive basis function models</h1>
<h2 id="adaboost">AdaBoost</h2>
<h3 id="representation">Representation</h3>
<p><span class="math display">\[y=\text{sign}(f(\vec{x}))=\text{sign}\left(\sum\limits_{i=1}^m \alpha_mG_m(\vec{x})\right)\]</span></p>
<p>where <span class="math inline">\(G_m(\vec{x})\)</span> are sub classifiers.</p>
<h3 id="evaluation">Evaluation</h3>
<p><span class="math display">\[\nonumber
L(y,f(\vec{x}))=\exp[-yf(\vec{x})] \text{  i.e., exponential loss function}\]</span></p>
<p><span class="math display">\[(\alpha_m,G_m(x))= \arg\min_{\alpha,G} \sum_{i=1}^N \exp{[-y_i(f_{m-1}(\vec{x}_i)+\alpha G(\vec{x}_i))]}\]</span></p>
<p>Define <span class="math inline">\(\bar{w}_{mi}=\exp{[-y_i(f_{m-1}(\vec{x}_i)]}\)</span>, which is constant w.r.t. <span class="math inline">\(\alpha, G\)</span> <span class="math display">\[(\alpha_m,G_m(x))= \arg\min_{\alpha,G} \sum_{i=1}^N {\bar{w}}_{mi} \exp{(-y_i \alpha G(x_i))}\]</span></p>
<h3 id="optimization">Optimization</h3>
<h4 id="input">Input</h4>
<p><span class="math display">\[\begin{aligned}
&amp; \mathcal{D}=\{(\vec{x}_1,y_1),(\vec{x}_2,y2),\dots,(\vec{x}_N,y_N)\} \\
&amp; \quad \text{where } \vec{x}_i \in \mathbb{R}^D,\ y_i \in \{-1,+1\} \\
&amp; \text{ Weak classifiers } \{G_1,G_2,\dots,G_m\}\end{aligned}\]</span></p>
<h4 id="output">Output</h4>
<p>Final classifier: <span class="math inline">\(G(x)\)</span></p>
<h4 id="algorithm">Algorithm</h4>
<ol>
<li><p>Initialize the weights’ distribution of training data(when <span class="math inline">\(m=1\)</span>) <span class="math display">\[\nonumber
\mathcal{D}_0=(w_{11},w_{12},\cdots,w_{1n})=(\frac{1}{N},\frac{1}{N},\cdots,\frac{1}{N})\]</span></p></li>
<li><p>Iterate over <span class="math inline">\(m=1,2,\dotsc,M\)</span> (a) Use training data with current weights’ distribution <span class="math inline">\(\mathcal{D}_m\)</span> to get a classifier <span class="math inline">\(G_m(\vec{x})\)</span> (b) Compute the error rate of <span class="math inline">\(G_m(\vec{x})\)</span> over the training data<br />
<span class="math display">\[e_m=P(G_m(\vec{x}_i)\neq y_i)=\sum_{i=1}^N {w_{mi}\mathbb{I}(G_m(\vec{x}_i) \neq y_i)}\]</span></p>
<p>(c) Compute the coefficient of classifier <span class="math inline">\(G_m(x)\)</span> <span class="math display">\[\alpha_m = \frac{1}{2}\log{\frac{1-e_m}{e_m}}\]</span> (d) Update the weights’ distribution of training data <span class="math display">\[w_{m+1,i}=\frac{w_{mi}}{Z_m}\exp(-\alpha_m y_i G_m(\vec{x}_i))\]</span> where <span class="math inline">\(Z_m\)</span> is the normalizing constant <span class="math display">\[Z_m=\sum_{i=1}^N w_{mi}\exp(-\alpha_m y_i G_m(\vec{x}_i))\]</span></p></li>
<li><p>Ensemble <span class="math inline">\(M\)</span> weak classifiers <span class="math display">\[G(x)=\text{sign}f(\vec{x})=\text{sign}\left[\sum_{m=1}^M \alpha_m G_m(\vec{x})\right]\]</span></p></li>
</ol>
<h3 id="the-upper-bound-of-the-training-error-of-adaboost">The upper bound of the training error of AdaBoost</h3>
<p>The upper bound of the training error of AdaBoost is <span class="math display">\[\frac{1}{N} \sum_{i=1}^N \mathbb{I}(G(\vec{x}_i)\neq y_i) \leq \frac{1}{N} \sum_{i=1}^N \exp(-y_i f(\vec{x}_i))=\prod_{m=1}^M Z_m\]</span></p>
<p>Note: the following equation would help proof this theorem <span class="math display">\[w_{mi}\exp(-\alpha_m y_i G_m(\vec{x}_i))=Z_m w_{m+1,i}\]</span></p>
</body>
</html>
