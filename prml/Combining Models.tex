\chapter{Combining Models}
\section{Introduction}
\begin{description}
	\item[\textbf{committees}] Train $L$ different models and then make predictions using the average of the predictions made by each model.
	\item[\textbf{boosting}] Train multiple models in sequence in which the error function used to train a particular model depends on the performance of the previous models.
	\item[\textbf{decision trees}] Different models are responsible for making predictions in different regions of input space.
	\item[\textbf{mixtures of experts}] Models are viewed as mixture distributions in which the component densities,as well as the mixing coefficients,are conditioned on the input variables.
	\begin{align}
	p(t|\vec{x})=\sum\limits_{k=1}^{K}\pi_k(\vec{x})p(t|\vec{x},k)
	\end{align}
	in which $\pi_k(\vec{x})=p(k|\vec{x})$ represents the input-dependent mixing coefficients,and $k$ indexes the model.
\end{description}

\section{Bayesian Model Averaging}
In Bayesian model averaging the whole data set is generated by a single model.By contrast,when we combine multiple models,we see that different data points within the data set can potentially be generated from different values of the latent variable $\vec{z}$ and hence by different components.

\section{Committees}
The simplest way to construct a committee is to average the predictions of a set of individual models,to cancel the contribution arising from variance and bias.

\textbf{Bootstrap} data to introduce variability between the different models within the committee.Suppose we generate $M$ bootstrap data sets
\begin{align}
y_{COM}(\vec{x})=\dfrac{1}{M}\sum\limits_{m=1}^{M}y_m(\vec{x})
\end{align}
where $m=1,...,M$.This procedure is known as \textbf{bootstrap aggregation} or \textbf{bagging}.

\section{Boosting}
Here we describe the most widely used form of boosting algorithm:\textbf{AdaBoost},short for 'adaptive boosting'.The base classifiers are known as \textbf{weak learners} and are trained in \textbf{sequence} using a \textbf{weighted form of the data set} in which the weighting coefficient associated with each data point depends on the performance of the previous classifiers.

Consider a two-class classification problem,in which the training data comprises input vectors $\vec{x}_1,...,\vec{x}_N$ along with corresponding binary target variables $t_1,...,t_N$ where $t_n\in \{-1,1\}$.Each data point is given an associated weighting parameter $w_n$,initially set $1/N$ for all.A base classifier function $y(\vec{x})\in \{-1,1\}$
\begin{SCfigure*}
	\caption{Shematic illustration of boosting framework}
	\includegraphics{prml/Figure14.1}
\end{SCfigure*}

\begin{algorithm}[H]
	\caption{\color{red}{AdaBoost}}
	\label{algo:AdaBoost}
	\DontPrintSemicolon % Some LaTeX compilers require you to use \dontprintsemicolon instead 
	\KwIn{A set $\vec{X} = \{\vec{x}_1, \vec{x}_2, \ldots, \vec{x}_N\}$,$\{t_1,...,t_N\}$}
	\KwOut{$y(\vec{x})$,.}
	1. Initialize the data weighting coefficients $\{w_n\}$ by setting $w_n^{1}=1/N$ for $n=1,...,N$. \;
	2. \For{$m=1,...,N$}{
		(a) Fit a classifier $y_m(\vec{x})$ to the training data by minimizing the weighted error function
		\begin{align}
		J_m=\sum\limits_{n=1}^{N}w_n^{(m)}I(y_m(\vec{x}_n)\neq t_n)
		\end{align}
		where $I(y_m(\vec{x}_n)\neq t_n)$ is the indicator function and equals $1$ when $y_m(\vec{x}_n)= t_n$ and $0$ otherwise. \;
		(b) Evaluate  the quantities
		\begin{align}
		\epsilon_m=\dfrac{\sum_{n=1}^{N}w_n^{(m)}I(y_m(\vec{x}_n)\neq t_n)}{\sum_{n=1}^{N}w_n^{(m)}}
		\end{align}
		and then use these to evaluate
		\begin{align}
		\alpha_m=\ln\{\dfrac{1-\epsilon_m}{\epsilon_m}\}.
		\end{align}
		\;
		(c) Update the data weighting coefficients
		\begin{align}
		w_n^{(m+1)}=w_n^{(m)}\exp\{\alpha_m I(y_m(\vec{x}_m)\neq t_n)\}
		\end{align}
	}
	3. Make predictions using the final model,which is given by
	\begin{align}
	Y_M(\vec{x})=sign(\sum_{m=1}^{M}\alpha_m y_m(\vec{x}))
	\end{align}
		
\end{algorithm}

\subsection{Minimizing exponential error}

\subsection{Error functions for boosting}



\section{Tree-based Models}

\section{Conditional Mixture Models}




























